"""
ê³ ë„í™”ëœ ë©€í‹° AI í†µí•© ì‹œìŠ¤í…œ
- ì—¬ëŸ¬ AI ëª¨ë¸ì— ë™ì‹œ ì§ˆë¬¸ ì „ì†¡
- PDF, ì‚¬ì§„ ë“± ì²¨ë¶€ ìë£Œ í†µí•© ë¶„ì„
- LangChain RAGë¥¼ í™œìš©í•œ ì‹ ë¢°ë„ ê²€ì¦
- ì‘ë‹µ ê°„ ìœ ì‚¬ ë¬¸ì¥ ë° ë¶ˆì¼ì¹˜ ì •ë³´ ë¶„ì„
- ì •ì œëœ í•µì‹¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì  ë‹µë³€ ë„ì¶œ
"""

import asyncio
import aiohttp
import json
import logging
import os
import time
import concurrent.futures
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib
import re
import base64
from pathlib import Path

# LangChain ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langchain.document_loaders import PyPDFLoader, TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.vectorstores import FAISS
    from langchain.chains import RetrievalQA
    from langchain.llms import OpenAI
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©")

# ì´ë¯¸ì§€ ì²˜ë¦¬ ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from PIL import Image
    import pytesseract
    IMAGE_PROCESSING_AVAILABLE = True
    print("âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    IMAGE_PROCESSING_AVAILABLE = False
    print("âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‚¬ìš© ë¶ˆê°€")

logger = logging.getLogger(__name__)

@dataclass
class AttachmentInfo:
    """ì²¨ë¶€ íŒŒì¼ ì •ë³´"""
    file_type: str  # 'pdf', 'image', 'text', 'video'
    file_path: str
    file_size: int
    content_hash: str
    extracted_text: Optional[str] = None
    metadata: Dict[str, Any] = None

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ì •ë³´"""
    model_name: str
    response_content: str
    confidence_score: float
    response_time: float
    tokens_used: int
    error: Optional[str] = None
    attachments_analyzed: List[str] = None

@dataclass
class IntegratedResponse:
    """í†µí•© ì‘ë‹µ ê²°ê³¼"""
    final_answer: str
    confidence_score: float
    consensus_level: str
    contributing_models: List[str]
    disagreements: List[str]
    attachments_summary: str
    rag_verification: Dict[str, Any]
    quality_metrics: Dict[str, float]
    processing_time: float

class AdvancedAIIntegration:
    """ê³ ë„í™”ëœ AI í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ai_models = {
            'gpt4': {
                'api_key': os.getenv('OPENAI_API_KEY'),
                'model': 'gpt-4o',
                'max_tokens': 2000,
                'temperature': 0.7,
                'timeout': 30
            },
            'claude': {
                'api_key': os.getenv('ANTHROPIC_API_KEY'),
                'model': 'claude-3-5-sonnet-20241022',
                'max_tokens': 2000,
                'temperature': 0.7,
                'timeout': 30
            },
            'mixtral': {
                'api_key': os.getenv('GROQ_API_KEY'),
                'model': 'mixtral-8x7b-32768',
                'max_tokens': 2000,
                'temperature': 0.7,
                'timeout': 30
            }
        }
        
        # LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.rag_system = None
        if LANGCHAIN_AVAILABLE:
            self._initialize_rag_system()
        
        # ì‘ë‹µ ìºì‹œ
        self.response_cache = {}
        
        print("ğŸš€ ê³ ë„í™”ëœ AI í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_rag_system(self):
        """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if os.getenv('OPENAI_API_KEY'):
                self.embeddings = OpenAIEmbeddings()
                self.rag_system = "initialized"
                print("âœ… LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"LangChain RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def generate_comprehensive_response(
        self, 
        query: str, 
        attachments: List[str] = None,
        context: Dict[str, Any] = None
    ) -> IntegratedResponse:
        """ì¢…í•©ì ì¸ AI ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        try:
            print(f"ğŸ” ì¢…í•© AI ì‘ë‹µ ìƒì„± ì‹œì‘: '{query[:50]}...'")
            
            # 1. ì²¨ë¶€ íŒŒì¼ ë¶„ì„
            attachment_info = await self._analyze_attachments(attachments or [])
            
            # 2. ì»¨í…ìŠ¤íŠ¸ í†µí•©
            integrated_context = self._integrate_context(query, attachment_info, context)
            
            # 3. ì—¬ëŸ¬ AIì— ë™ì‹œ ì§ˆë¬¸ ì „ì†¡
            ai_responses = await self._send_parallel_queries(integrated_context)
            
            # 4. ì‘ë‹µ ë¶„ì„ ë° ë¹„êµ
            response_analysis = self._analyze_responses(ai_responses, query)
            
            # 5. LangChain RAG ê²€ì¦
            rag_verification = await self._verify_with_rag(ai_responses, integrated_context)
            
            # 6. ìµœì  ë‹µë³€ ë„ì¶œ
            final_answer = self._generate_optimal_answer(
                ai_responses, 
                response_analysis, 
                rag_verification,
                query
            )
            
            # 7. í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
            quality_metrics = self._calculate_quality_metrics(
                ai_responses, 
                final_answer, 
                response_analysis
            )
            
            processing_time = time.time() - start_time
            
            result = IntegratedResponse(
                final_answer=final_answer,
                confidence_score=quality_metrics['overall_confidence'],
                consensus_level=response_analysis['consensus_level'],
                contributing_models=[resp.model_name for resp in ai_responses if not resp.error],
                disagreements=response_analysis['disagreements'],
                attachments_summary=self._create_attachments_summary(attachment_info),
                rag_verification=rag_verification,
                quality_metrics=quality_metrics,
                processing_time=processing_time
            )
            
            print(f"âœ… ì¢…í•© AI ì‘ë‹µ ìƒì„± ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•© AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_fallback_response(query, str(e))
    
    async def _analyze_attachments(self, attachments: List[str]) -> List[AttachmentInfo]:
        """ì²¨ë¶€ íŒŒì¼ ë¶„ì„"""
        attachment_info = []
        
        for attachment_path in attachments:
            try:
                if not os.path.exists(attachment_path):
                    continue
                
                file_type = self._detect_file_type(attachment_path)
                file_size = os.path.getsize(attachment_path)
                content_hash = self._calculate_file_hash(attachment_path)
                
                # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
                extracted_text = await self._extract_file_content(attachment_path, file_type)
                
                info = AttachmentInfo(
                    file_type=file_type,
                    file_path=attachment_path,
                    file_size=file_size,
                    content_hash=content_hash,
                    extracted_text=extracted_text,
                    metadata=self._extract_file_metadata(attachment_path, file_type)
                )
                
                attachment_info.append(info)
                print(f"âœ… ì²¨ë¶€ íŒŒì¼ ë¶„ì„ ì™„ë£Œ: {attachment_path}")
                
            except Exception as e:
                logger.warning(f"ì²¨ë¶€ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {attachment_path}: {e}")
                continue
        
        return attachment_info
    
    def _detect_file_type(self, file_path: str) -> str:
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        ext = Path(file_path).suffix.lower()
        
        if ext == '.pdf':
            return 'pdf'
        elif ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:
            return 'image'
        elif ext in ['.txt', '.md', '.docx']:
            return 'text'
        elif ext in ['.mp4', '.avi', '.mov', '.wmv']:
            return 'video'
        else:
            return 'unknown'
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return "unknown"
    
    async def _extract_file_content(self, file_path: str, file_type: str) -> Optional[str]:
        """íŒŒì¼ ë‚´ìš© ì¶”ì¶œ"""
        try:
            if file_type == 'pdf' and LANGCHAIN_AVAILABLE:
                return await self._extract_pdf_content(file_path)
            elif file_type == 'image' and IMAGE_PROCESSING_AVAILABLE:
                return await self._extract_image_content(file_path)
            elif file_type == 'text':
                return await self._extract_text_content(file_path)
            else:
                return None
        except Exception as e:
            logger.warning(f"íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    async def _extract_pdf_content(self, file_path: str) -> str:
        """PDF ë‚´ìš© ì¶”ì¶œ"""
        try:
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            texts = text_splitter.split_documents(documents)
            
            content = "\n".join([doc.page_content for doc in texts])
            return content[:5000]  # ìµœëŒ€ 5000ì
        except Exception as e:
            logger.warning(f"PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    async def _extract_image_content(self, file_path: str) -> str:
        """ì´ë¯¸ì§€ ë‚´ìš© ì¶”ì¶œ (OCR)"""
        try:
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang='kor+eng')
            return text[:2000]  # ìµœëŒ€ 2000ì
        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}")
            return ""
    
    async def _extract_text_content(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© ì¶”ì¶œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content[:5000]  # ìµœëŒ€ 5000ì
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return ""
    
    def _extract_file_metadata(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            stat = os.stat(file_path)
            metadata = {
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'file_size': stat.st_size
            }
            
            if file_type == 'image' and IMAGE_PROCESSING_AVAILABLE:
                try:
                    with Image.open(file_path) as img:
                        metadata.update({
                            'width': img.width,
                            'height': img.height,
                            'format': img.format,
                            'mode': img.mode
                        })
                except Exception:
                    pass
            
            return metadata
        except Exception:
            return {}
    
    def _integrate_context(
        self, 
        query: str, 
        attachment_info: List[AttachmentInfo], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ í†µí•©"""
        integrated_context = {
            'query': query,
            'attachments': [],
            'context': context or {},
            'timestamp': datetime.now().isoformat()
        }
        
        # ì²¨ë¶€ íŒŒì¼ ì •ë³´ í†µí•©
        for attachment in attachment_info:
            attachment_data = {
                'type': attachment.file_type,
                'path': attachment.file_path,
                'size': attachment.file_size,
                'content_preview': attachment.extracted_text[:500] if attachment.extracted_text else None,
                'metadata': attachment.metadata
            }
            integrated_context['attachments'].append(attachment_data)
        
        return integrated_context
    
    async def _send_parallel_queries(self, context: Dict[str, Any]) -> List[AIResponse]:
        """ì—¬ëŸ¬ AIì— ë™ì‹œ ì§ˆë¬¸ ì „ì†¡"""
        tasks = []
        
        for model_name, config in self.ai_models.items():
            if config['api_key']:
                task = self._query_single_ai(model_name, config, context)
                tasks.append(task)
        
        # ë™ì‹œ ì‹¤í–‰
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        valid_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                model_name = list(self.ai_models.keys())[i]
                error_response = AIResponse(
                    model_name=model_name,
                    response_content="",
                    confidence_score=0.0,
                    response_time=0.0,
                    tokens_used=0,
                    error=str(response)
                )
                valid_responses.append(error_response)
            else:
                valid_responses.append(response)
        
        return valid_responses
    
    async def _query_single_ai(
        self, 
        model_name: str, 
        config: Dict[str, Any], 
        context: Dict[str, Any]
    ) -> AIResponse:
        """ë‹¨ì¼ AI ëª¨ë¸ì— ì§ˆë¬¸"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_comprehensive_prompt(context, model_name)
            
            if model_name == 'gpt4':
                response = await self._query_openai(config, prompt)
            elif model_name == 'claude':
                response = await self._query_anthropic(config, prompt)
            elif model_name == 'mixtral':
                response = await self._query_groq(config, prompt)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
            
            response_time = time.time() - start_time
            
            return AIResponse(
                model_name=model_name,
                response_content=response['content'],
                confidence_score=response.get('confidence', 0.8),
                response_time=response_time,
                tokens_used=response.get('tokens', 0),
                attachments_analyzed=[att['type'] for att in context['attachments']]
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return AIResponse(
                model_name=model_name,
                response_content="",
                confidence_score=0.0,
                response_time=response_time,
                tokens_used=0,
                error=str(e)
            )
    
    def _create_comprehensive_prompt(self, context: Dict[str, Any], model_name: str) -> str:
        """ì¢…í•©ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        query = context['query']
        attachments = context['attachments']
        
        prompt_parts = [
            f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:",
            f"ì§ˆë¬¸: {query}",
            ""
        ]
        
        if attachments:
            prompt_parts.append("ì²¨ë¶€ëœ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€ì— í¬í•¨í•´ì£¼ì„¸ìš”:")
            for i, attachment in enumerate(attachments, 1):
                prompt_parts.append(f"ì²¨ë¶€íŒŒì¼ {i} ({attachment['type']}):")
                if attachment['content_preview']:
                    prompt_parts.append(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {attachment['content_preview']}")
                prompt_parts.append("")
        
        # ëª¨ë¸ë³„ íŠ¹ì„±í™”ëœ ì§€ì‹œì‚¬í•­
        model_instructions = {
            'gpt4': "GPT-4ì˜ ê°•ì ì¸ ì •í™•ì„±ê³¼ í¬ê´„ì„±ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            'claude': "Claudeì˜ ê°•ì ì¸ ë¶„ì„ë ¥ê³¼ ì°½ì˜ì„±ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
            'mixtral': "Mixtralì˜ ê°•ì ì¸ íš¨ìœ¨ì„±ê³¼ ëª…í™•ì„±ì„ ì‚´ë ¤ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
        
        prompt_parts.append(f"ë‹µë³€ ì‹œ {model_instructions.get(model_name, 'ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.')}")
        
        return "\n".join(prompt_parts)
    
    async def _query_openai(self, config: Dict[str, Any], prompt: str) -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ"""
        import openai
        
        client = openai.AsyncOpenAI(api_key=config['api_key'])
        
        response = await client.chat.completions.create(
            model=config['model'],
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=config['max_tokens'],
            temperature=config['temperature']
        )
        
        return {
            'content': response.choices[0].message.content,
            'tokens': response.usage.total_tokens if response.usage else 0,
            'confidence': 0.9
        }
    
    async def _query_anthropic(self, config: Dict[str, Any], prompt: str) -> Dict[str, Any]:
        """Anthropic API í˜¸ì¶œ"""
        import anthropic
        
        client = anthropic.AsyncAnthropic(api_key=config['api_key'])
        
        response = await client.messages.create(
            model=config['model'],
            max_tokens=config['max_tokens'],
            temperature=config['temperature'],
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return {
            'content': response.content[0].text,
            'tokens': response.usage.output_tokens if response.usage else 0,
            'confidence': 0.9
        }
    
    async def _query_groq(self, config: Dict[str, Any], prompt: str) -> Dict[str, Any]:
        """Groq API í˜¸ì¶œ"""
        from groq import AsyncGroq
        
        client = AsyncGroq(api_key=config['api_key'])
        
        response = await client.chat.completions.create(
            model=config['model'],
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=config['max_tokens'],
            temperature=config['temperature']
        )
        
        return {
            'content': response.choices[0].message.content,
            'tokens': response.usage.total_tokens if response.usage else 0,
            'confidence': 0.8
        }
    
    def _analyze_responses(self, responses: List[AIResponse], query: str) -> Dict[str, Any]:
        """ì‘ë‹µ ë¶„ì„ ë° ë¹„êµ"""
        valid_responses = [r for r in responses if not r.error]
        
        if len(valid_responses) < 2:
            return {
                'consensus_level': 'low',
                'agreements': [],
                'disagreements': [],
                'similarity_scores': {},
                'quality_scores': {r.model_name: r.confidence_score for r in valid_responses}
            }
        
        # ìœ ì‚¬ë„ ë¶„ì„
        similarity_scores = self._calculate_response_similarity(valid_responses)
        
        # í•©ì˜ë„ ë¶„ì„
        consensus_level = self._determine_consensus_level(similarity_scores)
        
        # ë¶ˆì¼ì¹˜ ì •ë³´ ì‹ë³„
        disagreements = self._identify_disagreements(valid_responses, similarity_scores)
        
        return {
            'consensus_level': consensus_level,
            'agreements': self._find_agreements(valid_responses),
            'disagreements': disagreements,
            'similarity_scores': similarity_scores,
            'quality_scores': {r.model_name: r.confidence_score for r in valid_responses}
        }
    
    def _calculate_response_similarity(self, responses: List[AIResponse]) -> Dict[str, float]:
        """ì‘ë‹µ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarity_scores = {}
        
        for i, resp1 in enumerate(responses):
            for j, resp2 in enumerate(responses[i+1:], i+1):
                similarity = self._calculate_text_similarity(
                    resp1.response_content, 
                    resp2.response_content
                )
                pair_key = f"{resp1.model_name}-{resp2.model_name}"
                similarity_scores[pair_key] = similarity
        
        return similarity_scores
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ì‹)"""
        try:
            # ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë³€í™˜
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            # Jaccard ìœ ì‚¬ë„
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            if union == 0:
                return 0.0
            
            return intersection / union
        except Exception:
            return 0.0
    
    def _determine_consensus_level(self, similarity_scores: Dict[str, float]) -> str:
        """í•©ì˜ë„ ë ˆë²¨ ê²°ì •"""
        if not similarity_scores:
            return 'low'
        
        avg_similarity = sum(similarity_scores.values()) / len(similarity_scores)
        
        if avg_similarity > 0.7:
            return 'high'
        elif avg_similarity > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _identify_disagreements(self, responses: List[AIResponse], similarity_scores: Dict[str, float]) -> List[str]:
        """ë¶ˆì¼ì¹˜ ì •ë³´ ì‹ë³„"""
        disagreements = []
        
        # ìœ ì‚¬ë„ê°€ ë‚®ì€ ì‘ë‹µ ìŒ ì°¾ê¸°
        low_similarity_pairs = [
            pair for pair, score in similarity_scores.items() 
            if score < 0.3
        ]
        
        for pair in low_similarity_pairs:
            model1, model2 = pair.split('-')
            disagreements.append(f"{model1}ê³¼ {model2}ì˜ ë‹µë³€ì´ ìƒë‹¹íˆ ë‹¤ë¦…ë‹ˆë‹¤.")
        
        return disagreements
    
    def _find_agreements(self, responses: List[AIResponse]) -> List[str]:
        """ê³µí†µëœ ë‚´ìš© ì°¾ê¸°"""
        agreements = []
        
        if len(responses) < 2:
            return agreements
        
        # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°
        all_words = []
        for resp in responses:
            words = set(resp.response_content.lower().split())
            all_words.append(words)
        
        # ëª¨ë“  ì‘ë‹µì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´
        common_words = set.intersection(*all_words)
        if common_words:
            agreements.append(f"ê³µí†µ í‚¤ì›Œë“œ: {', '.join(list(common_words)[:5])}")
        
        return agreements
    
    async def _verify_with_rag(
        self, 
        responses: List[AIResponse], 
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """LangChain RAGë¥¼ í™œìš©í•œ ì‹ ë¢°ë„ ê²€ì¦"""
        if not LANGCHAIN_AVAILABLE or not self.rag_system:
            return {
                'rag_available': False,
                'verification_score': 0.5,
                'verified_facts': [],
                'contradictions': []
            }
        
        try:
            # ë‚´ë¶€ ìë£Œì™€ ë¹„êµ
            verification_results = []
            
            for response in responses:
                if response.error:
                    continue
                
                # RAG ê²€ì¦ (ê°„ë‹¨í•œ í˜•íƒœ)
                verification_score = self._simple_rag_verification(response.response_content)
                
                verification_results.append({
                    'model': response.model_name,
                    'score': verification_score,
                    'verified': verification_score > 0.6
                })
            
            avg_verification = sum(r['score'] for r in verification_results) / max(len(verification_results), 1)
            
            return {
                'rag_available': True,
                'verification_score': avg_verification,
                'verified_facts': [r for r in verification_results if r['verified']],
                'contradictions': [r for r in verification_results if not r['verified']]
            }
            
        except Exception as e:
            logger.warning(f"RAG ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                'rag_available': False,
                'verification_score': 0.5,
                'verified_facts': [],
                'contradictions': []
            }
    
    def _simple_rag_verification(self, response_content: str) -> float:
        """ê°„ë‹¨í•œ RAG ê²€ì¦ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê²€ì¦ ë¡œì§ í•„ìš”)"""
        # ìˆ«ì, ë‚ ì§œ, êµ¬ì²´ì  ì •ë³´ í¬í•¨ë„ë¡œ ê²€ì¦
        factual_elements = len(re.findall(r'\d+', response_content))
        factual_elements += len(re.findall(r'\d{4}-\d{2}-\d{2}', response_content))
        
        # êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        return min(factual_elements / 10, 1.0)
    
    def _generate_optimal_answer(
        self, 
        responses: List[AIResponse], 
        analysis: Dict[str, Any], 
        rag_verification: Dict[str, Any],
        query: str
    ) -> str:
        """ìµœì  ë‹µë³€ ë„ì¶œ"""
        try:
            valid_responses = [r for r in responses if not r.error]
            
            if not valid_responses:
                return "AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬
            sorted_responses = sorted(
                valid_responses, 
                key=lambda x: x.confidence_score, 
                reverse=True
            )
            
            # ì£¼ìš” ì‘ë‹µ ì„ íƒ
            primary_response = sorted_responses[0]
            
            # ì•™ìƒë¸” ë‹µë³€ êµ¬ì„±
            answer_parts = []
            
            # í†µí•© ë‹µë³€
            answer_parts.append("## ğŸ¯ í†µí•© ë‹µë³€")
            answer_parts.append(primary_response.response_content)
            
            # ì²¨ë¶€ íŒŒì¼ ë¶„ì„ ê²°ê³¼
            if any(r.attachments_analyzed for r in valid_responses):
                answer_parts.append("\n## ğŸ“ ì²¨ë¶€ íŒŒì¼ ë¶„ì„")
                attachment_types = set()
                for r in valid_responses:
                    if r.attachments_analyzed:
                        attachment_types.update(r.attachments_analyzed)
                
                answer_parts.append(f"ë¶„ì„ëœ íŒŒì¼ ìœ í˜•: {', '.join(attachment_types)}")
            
            # AI ëª¨ë¸ë³„ ë¶„ì„
            answer_parts.append("\n## ğŸ¤– AI ëª¨ë¸ë³„ ë¶„ì„")
            for response in sorted_responses[:3]:  # ìƒìœ„ 3ê°œ
                answer_parts.append(f"### {response.model_name.upper()}")
                answer_parts.append(f"- ì‹ ë¢°ë„: {response.confidence_score:.1%}")
                answer_parts.append(f"- ì‘ë‹µ ì‹œê°„: {response.response_time:.2f}ì´ˆ")
                answer_parts.append(f"- í† í° ì‚¬ìš©ëŸ‰: {response.tokens_used}")
                
                if response.attachments_analyzed:
                    answer_parts.append(f"- ì²¨ë¶€ íŒŒì¼ ë¶„ì„: {', '.join(response.attachments_analyzed)}")
            
            # í•©ì˜ë„ ë¶„ì„
            answer_parts.append(f"\n## ğŸ“Š í•©ì˜ë„ ë¶„ì„")
            answer_parts.append(f"- í•©ì˜ë„ ë ˆë²¨: {analysis['consensus_level']}")
            
            if analysis['agreements']:
                answer_parts.append(f"- ê³µí†µ ë‚´ìš©: {', '.join(analysis['agreements'])}")
            
            if analysis['disagreements']:
                answer_parts.append(f"- ë¶ˆì¼ì¹˜ ì‚¬í•­: {', '.join(analysis['disagreements'])}")
            
            # RAG ê²€ì¦ ê²°ê³¼
            if rag_verification['rag_available']:
                answer_parts.append(f"\n## ğŸ” ì‹ ë¢°ë„ ê²€ì¦")
                answer_parts.append(f"- ê²€ì¦ ì ìˆ˜: {rag_verification['verification_score']:.1%}")
                
                if rag_verification['verified_facts']:
                    verified_models = [f['model'] for f in rag_verification['verified_facts']]
                    answer_parts.append(f"- ê²€ì¦ëœ ëª¨ë¸: {', '.join(verified_models)}")
            
            # ìµœì¢… ì¶”ì²œ
            answer_parts.append(f"\n## ğŸ† ìµœì¢… ì¶”ì²œ")
            best_model = sorted_responses[0].model_name
            answer_parts.append(f"- {best_model.upper()}ê°€ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
            answer_parts.append(f"- ì „ì²´ ì‹ ë¢°ë„: {analysis['quality_scores'].get(best_model, 0.5):.1%}")
            
            return "\n".join(answer_parts)
            
        except Exception as e:
            logger.error(f"ìµœì  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _calculate_quality_metrics(
        self, 
        responses: List[AIResponse], 
        final_answer: str, 
        analysis: Dict[str, Any]
    ) -> Dict[str, float]:
        """í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        valid_responses = [r for r in responses if not r.error]
        
        if not valid_responses:
            return {
                'overall_confidence': 0.0,
                'response_quality': 0.0,
                'consensus_quality': 0.0,
                'processing_efficiency': 0.0
            }
        
        # ì „ì²´ ì‹ ë¢°ë„
        overall_confidence = sum(r.confidence_score for r in valid_responses) / len(valid_responses)
        
        # ì‘ë‹µ í’ˆì§ˆ
        response_quality = sum(r.confidence_score for r in valid_responses) / len(valid_responses)
        
        # í•©ì˜ë„ í’ˆì§ˆ
        consensus_scores = {'high': 1.0, 'medium': 0.7, 'low': 0.4}
        consensus_quality = consensus_scores.get(analysis['consensus_level'], 0.4)
        
        # ì²˜ë¦¬ íš¨ìœ¨ì„±
        avg_response_time = sum(r.response_time for r in valid_responses) / len(valid_responses)
        processing_efficiency = max(0, 1 - avg_response_time / 30)  # 30ì´ˆ ê¸°ì¤€
        
        return {
            'overall_confidence': overall_confidence,
            'response_quality': response_quality,
            'consensus_quality': consensus_quality,
            'processing_efficiency': processing_efficiency
        }
    
    def _create_attachments_summary(self, attachments: List[AttachmentInfo]) -> str:
        """ì²¨ë¶€ íŒŒì¼ ìš”ì•½ ìƒì„±"""
        if not attachments:
            return "ì²¨ë¶€ íŒŒì¼ ì—†ìŒ"
        
        summary_parts = []
        for attachment in attachments:
            summary_parts.append(
                f"- {attachment.file_type.upper()}: "
                f"{attachment.file_size} bytes"
            )
        
        return "\n".join(summary_parts)
    
    def _create_fallback_response(self, query: str, error: str) -> IntegratedResponse:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        return IntegratedResponse(
            final_answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ '{query}'ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}",
            confidence_score=0.0,
            consensus_level='none',
            contributing_models=[],
            disagreements=[],
            attachments_summary="ì²¨ë¶€ íŒŒì¼ ì—†ìŒ",
            rag_verification={'rag_available': False, 'verification_score': 0.0},
            quality_metrics={
                'overall_confidence': 0.0,
                'response_quality': 0.0,
                'consensus_quality': 0.0,
                'processing_efficiency': 0.0
            },
            processing_time=0.0
        )

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
advanced_ai_integration = AdvancedAIIntegration()
