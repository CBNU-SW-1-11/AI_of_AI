"""
AI ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""
import openai
import ollama
from ..config.ai_config import (
    KOREAN_LANGUAGE_INSTRUCTION,
    OPENAI_MODEL_COMPLETION_LIMITS,
    DEFAULT_OPENAI_COMPLETION_LIMIT
)


def enforce_korean_instruction(text: str) -> str:
    """Ensure that the given system prompt explicitly enforces Korean responses."""
    if not text:
        return text
    if KOREAN_LANGUAGE_INSTRUCTION in text:
        return text
    return text + KOREAN_LANGUAGE_INSTRUCTION


def get_openai_completion_limit(model_name: str) -> int:
    """ëª¨ë¸ëª…ì— ë”°ë¼ ì•ˆì „í•œ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë°˜í™˜"""
    if not model_name:
        return DEFAULT_OPENAI_COMPLETION_LIMIT
    normalized_name = model_name.lower()
    for key, limit in OPENAI_MODEL_COMPLETION_LIMITS:
        if key in normalized_name:
            return limit
    return DEFAULT_OPENAI_COMPLETION_LIMIT


def generate_optimal_response_with_ollama(ai_responses, user_question):
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± (ë¹„ìš© ì ˆì•½ + í’ˆì§ˆ í–¥ìƒ)"""
    try:
        # AI ì‘ë‹µë“¤ì„ ì •ë¦¬
        responses_text = ""
        model_names = []
        for model_name, response in ai_responses.items():
            responses_text += f"### {model_name.upper()}:\n{response}\n\n"
            model_names.append(model_name.upper())
        
        # AI ë¶„ì„ ì„¹ì…˜ ìƒì„±
        analysis_sections = ""
        for name in model_names:
            analysis_sections += f"### {name}\n- ì¥ì : [ì£¼ìš” ì¥ì ]\n- ë‹¨ì : [ì£¼ìš” ë‹¨ì ]\n- íŠ¹ì§•: [íŠ¹ë³„í•œ íŠ¹ì§•]\n"
        
        # ë¹„ìš© ì ˆì•½ì„ ìœ„í•œ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""AI ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

í˜•ì‹:
## í†µí•© ë‹µë³€
[ëª¨ë“  AIì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì  ë‹µë³€]

## ê° AI ë¶„ì„
{analysis_sections}
## ë¶„ì„ ê·¼ê±°
[í†µí•© ë‹µë³€ì„ ë§Œë“  êµ¬ì²´ì  ì´ìœ ]

## ìµœì¢… ì¶”ì²œ
[ìƒí™©ë³„ AI ì„ íƒ ê°€ì´ë“œ]

ì§ˆë¬¸: {user_question}

AI ë‹µë³€ë“¤:
{responses_text}

ìœ„ ë‹µë³€ë“¤ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

âš ï¸ ì§€ì‹œì‚¬í•­: ì§ˆë¬¸ ì–¸ì–´ë‚˜ ë‚´ìš©ì— ìƒê´€ì—†ì´ ìµœì¢… í†µí•© ë‹µë³€ê³¼ ëª¨ë“  ì„¤ëª…ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê³  ìœ ì°½í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
        
        response = ollama.chat(
                   model='llama3.2:latest',
            messages=[
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            options={
                'temperature': 0.7,
                'num_predict': 2500
            }
        )
        
        return response['message']['content']
    except Exception as e:
        return f"Ollama ìµœì  ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def generate_optimal_response(ai_responses, user_question, api_key=None):
    """AIë“¤ì˜ ì‘ë‹µì„ í†µí•©í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„± (Ollama ì‚¬ìš©)"""
    try:
        # Ollamaë¡œ ìµœì  ë‹µë³€ ìƒì„± (ë¹„ìš© ì ˆì•½)
        if not api_key:
            return generate_optimal_response_with_ollama(ai_responses, user_question)
        
        client = openai.OpenAI(api_key=api_key)
        
        # AI ì‘ë‹µë“¤ì„ ì •ë¦¬
        responses_text = ""
        model_names = []
        for model_name, response in ai_responses.items():
            responses_text += f"### {model_name.upper()}:\n{response}\n\n"
            model_names.append(model_name.upper())
        
        # ëª¨ë¸ë³„ ë¶„ì„ ì„¹ì…˜ ë™ì  ìƒì„±
        analysis_sections = ""
        for model_name in model_names:
            analysis_sections += f"""
### {model_name}
- ì¥ì : [ì£¼ìš” ì¥ì ]
- ë‹¨ì : [ì£¼ìš” ë‹¨ì ]
- íŠ¹ì§•: [íŠ¹ë³„í•œ íŠ¹ì§•]
"""
        
        system_prompt = f"""ë‹¹ì‹ ì€ AI ì‘ë‹µ ë¶„ì„ ë° ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ AIì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì™„ì „í•˜ê³  ì •í™•í•œ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

## ğŸ¯ í†µí•© ë‹µë³€
[ê°€ì¥ ì™„ì „í•˜ê³  ì •í™•í•œ í†µí•© ë‹µë³€ - ëª¨ë“  AIì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì ì˜ ë‹µë³€]

## ğŸ“Š ê° AI ë¶„ì„
{analysis_sections}

## ğŸ” ë¶„ì„ ê·¼ê±°
[ê° AIì˜ ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì¡°í•©í•˜ì—¬ í†µí•© ë‹µë³€ì„ ë§Œë“¤ì—ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]

## ğŸ† ìµœì¢… ì¶”ì²œ
[ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë‹µë³€ê³¼ ê·¸ ì´ìœ  - ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ AIë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ í¬í•¨]

## ğŸ’¡ ì¶”ê°€ ì¸ì‚¬ì´íŠ¸
[ì§ˆë¬¸ì— ëŒ€í•œ ë” ê¹Šì€ ì´í•´ë‚˜ ì¶”ê°€ ê³ ë ¤ì‚¬í•­]"""
        system_prompt = enforce_korean_instruction(system_prompt)

        user_prompt = f"ì§ˆë¬¸: {user_question}\n\në‹¤ìŒì€ ì—¬ëŸ¬ AIì˜ ë‹µë³€ì…ë‹ˆë‹¤:\n\n{responses_text}\nìœ„ ë‹µë³€ë“¤ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í†µí•© ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\nâš ï¸ ì§€ì‹œì‚¬í•­: ì§ˆë¬¸ ì–¸ì–´ë‚˜ ë‚´ìš©ì— ìƒê´€ì—†ì´ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ½ê³  ìœ ì°½í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=2500
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"ìµœì í™”ëœ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

