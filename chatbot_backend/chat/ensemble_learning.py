"""
ì•™ìƒë¸” í•™ìŠµ ê¸°ë²•ì„ í™œìš©í•œ AI í†µí•© ë‹µë³€ ìµœì í™” ì‹œìŠ¤í…œ
í”„ë¡œì íŠ¸ ëª©í‘œ: AI í†µí•© ê¸°ë°˜ ë‹µë³€ ìµœì í™” í”Œë«í¼
"""

import json
import logging
import numpy as np
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import re

logger = logging.getLogger(__name__)

# scikit-learn ì˜ì¡´ì„±ì„ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
    print("âœ… scikit-learn ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learn ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©")

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„° êµ¬ì¡°"""
    model_name: str
    content: str
    confidence: float
    quality_score: float
    helpfulness_score: float
    clarity_score: float
    factual_accuracy: float
    completeness_score: float
    ensemble_contribution: float

@dataclass
class EnsembleResult:
    """ì•™ìƒë¸” í•™ìŠµ ê²°ê³¼"""
    final_answer: str
    confidence_score: float
    consensus_level: str
    contributing_ais: List[str]
    disagreements: List[str]
    reasoning: str
    query_type: str
    ensemble_weights: Dict[str, float]
    quality_metrics: Dict[str, float]

class EnsembleLearningOptimizer:
    """ì•™ìƒë¸” í•™ìŠµì„ í™œìš©í•œ AI ì‘ë‹µ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        if SKLEARN_AVAILABLE:
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2)
            )
        else:
            self.vectorizer = None
        print("ğŸ” ì•™ìƒë¸” í•™ìŠµ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def optimize_responses(self, responses: Dict[str, str], query: str, file_context: str = None) -> Dict[str, Any]:
        """ì•™ìƒë¸” í•™ìŠµì„ í™œìš©í•œ ì‘ë‹µ ìµœì í™”"""
        try:
            print(f"ğŸ” ì•™ìƒë¸” í•™ìŠµ ì‹œì‘: {len(responses)}ê°œ ì‘ë‹µ")
            
            # ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ ìƒì„± (ì•ˆì „í•œ ë°©ì‹)
            simple_result = self._create_simple_ensemble_response(responses, query)
            if simple_result:
                print("âœ… ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ ìƒì„± ì„±ê³µ")
                return simple_result
            
            # ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹œë„
            try:
                # 1. ì‘ë‹µ ë¶„ì„ ë° êµ¬ì¡°í™”
                ai_responses = self._analyze_and_structure_responses(responses, query)
                
                # 2. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
                query_type = self._classify_query_type(query)
                
                # 3. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°
                ensemble_weights = self._calculate_ensemble_weights(ai_responses, query_type)
                
                # 4. í•©ì˜ë„ ë¶„ì„
                consensus_analysis = self._analyze_consensus(ai_responses)
                
                # 5. ìµœì¢… ë‹µë³€ ìƒì„± (ì•™ìƒë¸” í•™ìŠµ)
                final_answer = self._generate_ensemble_answer(ai_responses, ensemble_weights, query, query_type)
                
                # 6. í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
                quality_metrics = self._calculate_quality_metrics(ai_responses, final_answer)
                
                # 7. ê²°ê³¼ êµ¬ì„±
                result = EnsembleResult(
                    final_answer=final_answer,
                    confidence_score=quality_metrics['overall_confidence'],
                    consensus_level=consensus_analysis['consensus_level'],
                    contributing_ais=[resp.model_name for resp in ai_responses if resp.ensemble_contribution > 0.3],
                    disagreements=consensus_analysis['disagreements'],
                    reasoning=f"ì•™ìƒë¸” í•™ìŠµ ê¸°ë²• ì ìš©. ì§ˆë¬¸ ìœ í˜•: {query_type}",
                    query_type=query_type,
                    ensemble_weights=ensemble_weights,
                    quality_metrics=quality_metrics
                )
                
                print(f"âœ… ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ: ì‹ ë¢°ë„ {quality_metrics['overall_confidence']:.2f}")
                return self._convert_to_dict(result)
                
            except Exception as advanced_e:
                print(f"âŒ ê³ ê¸‰ ì•™ìƒë¸” í•™ìŠµ ì‹¤íŒ¨: {advanced_e}")
                # ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ìœ¼ë¡œ í´ë°±
                return self._create_simple_ensemble_response(responses, query)
            
        except Exception as e:
            logger.error(f"âŒ ì•™ìƒë¸” í•™ìŠµ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì•™ìƒë¸” í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return self._create_fallback_result(responses)
    
    def _create_simple_ensemble_response(self, responses: Dict[str, str], query: str) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ ìƒì„± (ì•ˆì „í•œ ë°©ì‹)"""
        try:
            print(f"ğŸ” ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ ìƒì„±: {len(responses)}ê°œ ì‘ë‹µ")
            
            if not responses:
                return None
            
            # ì§ˆë¬¸ ìœ í˜• ê°„ë‹¨ ë¶„ë¥˜
            query_type = self._classify_query_type(query)
            
            # ëª¨ë¸ë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜
            model_weights = {
                'technical': {'gpt': 0.4, 'claude': 0.3, 'mixtral': 0.3},
                'creative': {'claude': 0.4, 'gpt': 0.3, 'mixtral': 0.3},
                'factual': {'gpt': 0.35, 'claude': 0.35, 'mixtral': 0.3},
                'general': {'gpt': 0.35, 'claude': 0.35, 'mixtral': 0.3}
            }
            
            weights = model_weights.get(query_type, model_weights['general'])
            
            # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_responses = sorted(
                responses.items(), 
                key=lambda x: weights.get(x[0], 0.3), 
                reverse=True
            )
            
            # ì£¼ìš” ì‘ë‹µ ì„ íƒ
            primary_response = sorted_responses[0][1]
            
            # ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ êµ¬ì„±
            ensemble_parts = []
            
            # í†µí•© ë‹µë³€
            ensemble_parts.append("## ğŸ¯ í†µí•© ë‹µë³€")
            ensemble_parts.append(primary_response)
            
            # ê° AI ë¶„ì„
            ensemble_parts.append("\n## ğŸ“Š ê° AI ë¶„ì„")
            for ai_name, response in sorted_responses[:2]:  # ìƒìœ„ 2ê°œë§Œ
                weight = weights.get(ai_name, 0.3)
                ensemble_parts.append(f"### {ai_name.upper()}")
                ensemble_parts.append(f"- ì‹ ë¢°ë„: {weight*100:.1f}%")
                ensemble_parts.append(f"- ì•™ìƒë¸” ê¸°ì—¬ë„: {weight*100:.1f}%")
                
                # ê°„ë‹¨í•œ ì¥ì  ë¶„ì„
                if ai_name == 'gpt':
                    ensemble_parts.append("- ì¥ì : ê¸°ìˆ ì  ë¬¸ì œ í•´ê²°ì— ë›°ì–´ë‚¨")
                elif ai_name == 'claude':
                    ensemble_parts.append("- ì¥ì : ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€")
                elif ai_name == 'mixtral':
                    ensemble_parts.append("- ì¥ì : ë¹ ë¥´ê³  ê°„ê²°í•œ ë‹µë³€")
            
            # ë¶„ì„ ê·¼ê±°
            ensemble_parts.append("\n## ğŸ” ë¶„ì„ ê·¼ê±°")
            ensemble_parts.append(f"- ì§ˆë¬¸ ìœ í˜•: {query_type}")
            ensemble_parts.append(f"- ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {dict(sorted(weights.items(), key=lambda x: x[1], reverse=True))}")
            ensemble_parts.append(f"- ì£¼ìš” ê¸°ì—¬ ëª¨ë¸: {sorted_responses[0][0].upper()}")
            
            # ìµœì¢… ì¶”ì²œ
            ensemble_parts.append("\n## ğŸ† ìµœì¢… ì¶”ì²œ")
            best_model = sorted_responses[0][0]
            ensemble_parts.append(f"- {query_type} ìœ í˜• ì§ˆë¬¸ì—ëŠ” {best_model.upper()}ê°€ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤.")
            ensemble_parts.append(f"- ì „ì²´ ì‹ ë¢°ë„: {max(weights.values())*100:.1f}%")
            
            final_answer = "\n".join(ensemble_parts)
            
            return {
                'final_answer': final_answer,
                'confidence_score': max(weights.values()),
                'consensus_level': 'medium',
                'contributing_ais': [sorted_responses[0][0]],
                'disagreements': [],
                'reasoning': f"ê°„ë‹¨í•œ ì•™ìƒë¸” í•™ìŠµ ì ìš©. ì§ˆë¬¸ ìœ í˜•: {query_type}",
                'query_type': query_type,
                'ensemble_weights': weights,
                'quality_metrics': {
                    'overall_confidence': max(weights.values()),
                    'quality_score': 0.7,
                    'helpfulness_score': 0.7,
                    'clarity_score': 0.7,
                    'factual_accuracy': 0.7
                }
            }
            
        except Exception as e:
            print(f"âŒ ê°„ë‹¨í•œ ì•™ìƒë¸” ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_and_structure_responses(self, responses: Dict[str, str], query: str) -> List[AIResponse]:
        """ì‘ë‹µ ë¶„ì„ ë° êµ¬ì¡°í™”"""
        ai_responses = []
        
        for model_name, content in responses.items():
            try:
                # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
                quality_scores = self._calculate_basic_quality_scores(content, query)
                
                ai_response = AIResponse(
                    model_name=model_name,
                    content=content,
                    confidence=quality_scores['confidence'],
                    quality_score=quality_scores['quality'],
                    helpfulness_score=quality_scores['helpfulness'],
                    clarity_score=quality_scores['clarity'],
                    factual_accuracy=quality_scores['factual_accuracy'],
                    completeness_score=quality_scores['completeness'],
                    ensemble_contribution=0.0  # ë‚˜ì¤‘ì— ê³„ì‚°
                )
                
                ai_responses.append(ai_response)
                print(f"âœ… {model_name} ì‘ë‹µ ë¶„ì„ ì™„ë£Œ: í’ˆì§ˆ {quality_scores['quality']:.2f}")
                
            except Exception as e:
                logger.warning(f"{model_name} ì‘ë‹µ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
        
        return ai_responses
    
    def _calculate_basic_quality_scores(self, content: str, query: str) -> Dict[str, float]:
        """ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        try:
            # ê¸¸ì´ ê¸°ë°˜ ì™„ì„±ë„
            completeness = min(len(content) / max(len(query) * 3, 100), 1.0)
            
            # ëª…í™•ì„± (ë¬¸ì¥ êµ¬ì¡° ë¶„ì„)
            sentences = re.split(r'[.!?]+', content)
            avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
            clarity = max(0, 1 - abs(avg_sentence_length - 15) / 15)  # 15ë‹¨ì–´ê°€ ì´ìƒì 
            
            # ìœ ìš©ì„± (ì§ˆë¬¸ í‚¤ì›Œë“œ í¬í•¨ë„)
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())
            helpfulness = len(query_words.intersection(content_words)) / max(len(query_words), 1)
            
            # ì‚¬ì‹¤ ì •í™•ì„± (ìˆ«ì, ë‚ ì§œ ë“± êµ¬ì²´ì  ì •ë³´ í¬í•¨ë„)
            factual_elements = len(re.findall(r'\d+', content)) + len(re.findall(r'\d{4}', content))
            factual_accuracy = min(factual_elements / 5, 1.0)  # ìµœëŒ€ 5ê°œ ìš”ì†Œ
            
            # ì „ì²´ í’ˆì§ˆ (ê°€ì¤‘ í‰ê· )
            quality = (completeness * 0.3 + clarity * 0.25 + helpfulness * 0.25 + factual_accuracy * 0.2)
            
            # ì‹ ë¢°ë„ (í’ˆì§ˆì˜ ì œê³±ê·¼ìœ¼ë¡œ ë³´ì •)
            confidence = quality ** 0.5
            
            return {
                'completeness': completeness,
                'clarity': clarity,
                'helpfulness': helpfulness,
                'factual_accuracy': factual_accuracy,
                'quality': quality,
                'confidence': confidence
            }
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'completeness': 0.5,
                'clarity': 0.5,
                'helpfulness': 0.5,
                'factual_accuracy': 0.5,
                'quality': 0.5,
                'confidence': 0.5
            }
    
    def _classify_query_type(self, query: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        query_lower = query.lower()
        
        # ê¸°ìˆ ì  ì§ˆë¬¸
        technical_keywords = ['ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°', 'ì•Œê³ ë¦¬ì¦˜', 'ê°œë°œ', 'ê¸°ìˆ ', 'ì‹œìŠ¤í…œ', 'ë°ì´í„°ë² ì´ìŠ¤']
        if any(keyword in query_lower for keyword in technical_keywords):
            return 'technical'
        
        # ì°½ì˜ì  ì§ˆë¬¸
        creative_keywords = ['ì•„ì´ë””ì–´', 'ì°½ì˜', 'ë””ìì¸', 'ê¸€ì“°ê¸°', 'ìŠ¤í† ë¦¬', 'ìƒìƒ', 'í˜ì‹ ']
        if any(keyword in query_lower for keyword in creative_keywords):
            return 'creative'
        
        # ì‚¬ì‹¤ì  ì§ˆë¬¸
        factual_keywords = ['ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ë¬´ì—‡ì„', 'ì–¼ë§ˆë‚˜', 'ì •ì˜', 'ì˜ë¯¸']
        if any(keyword in query_lower for keyword in factual_keywords):
            return 'factual'
        
        # ë¶„ì„ì  ì§ˆë¬¸
        analytical_keywords = ['ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'ì¥ë‹¨ì ', 'ì°¨ì´ì ', 'ê´€ê³„', 'ì›ì¸']
        if any(keyword in query_lower for keyword in analytical_keywords):
            return 'analytical'
        
        return 'general'
    
    def _calculate_ensemble_weights(self, ai_responses: List[AIResponse], query_type: str) -> Dict[str, float]:
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        weights = {}
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ëª¨ë¸ ì„ í˜¸ë„
        model_preferences = {
            'technical': {'gpt': 0.4, 'claude': 0.3, 'mixtral': 0.3},
            'creative': {'claude': 0.4, 'gpt': 0.3, 'mixtral': 0.3},
            'factual': {'gpt': 0.35, 'claude': 0.35, 'mixtral': 0.3},
            'analytical': {'claude': 0.4, 'gpt': 0.35, 'mixtral': 0.25},
            'general': {'gpt': 0.35, 'claude': 0.35, 'mixtral': 0.3}
        }
        
        base_preferences = model_preferences.get(query_type, model_preferences['general'])
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        total_quality = sum(resp.quality_score for resp in ai_responses)
        
        for resp in ai_responses:
            # ê¸°ë³¸ ì„ í˜¸ë„
            base_weight = base_preferences.get(resp.model_name, 0.3)
            
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì¡°ì •
            quality_factor = resp.quality_score / max(total_quality, 0.1)
            
            # ìµœì¢… ê°€ì¤‘ì¹˜
            weights[resp.model_name] = base_weight * quality_factor
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        
        print(f"ğŸ” ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {weights}")
        return weights
    
    def _analyze_consensus(self, ai_responses: List[AIResponse]) -> Dict[str, Any]:
        """í•©ì˜ë„ ë¶„ì„"""
        if len(ai_responses) < 2:
            return {
                'consensus_level': 'low',
                'agreements': [],
                'disagreements': [],
                'agreement_ratio': 0.0
            }
        
        try:
            # TF-IDF ë²¡í„°í™”
            contents = [resp.content for resp in ai_responses]
            tfidf_matrix = self.vectorizer.fit_transform(contents)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])
            
            # í•©ì˜ë„ ë ˆë²¨ ê²°ì •
            if avg_similarity > 0.7:
                consensus_level = 'high'
            elif avg_similarity > 0.4:
                consensus_level = 'medium'
            else:
                consensus_level = 'low'
            
            # ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
            agreements = self._extract_common_keywords(contents)
            
            return {
                'consensus_level': consensus_level,
                'agreements': agreements,
                'disagreements': [],
                'agreement_ratio': avg_similarity
            }
            
        except Exception as e:
            logger.warning(f"í•©ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'consensus_level': 'low',
                'agreements': [],
                'disagreements': [],
                'agreement_ratio': 0.0
            }
    
    def _extract_common_keywords(self, contents: List[str]) -> List[str]:
        """ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
            all_text = ' '.join(contents)
            
            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            words = re.findall(r'\b\w+\b', all_text.lower())
            word_freq = {}
            
            for word in words:
                if len(word) > 2:  # 2ê¸€ì ì´ìƒë§Œ
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # ê³µí†µ í‚¤ì›Œë“œ (ëª¨ë“  í…ìŠ¤íŠ¸ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´)
            common_keywords = []
            for word, freq in word_freq.items():
                if freq >= len(contents):  # ëª¨ë“  ì‘ë‹µì— ë‚˜íƒ€ë‚¨
                    common_keywords.append(word)
            
            return common_keywords[:10]  # ìƒìœ„ 10ê°œ
            
        except Exception as e:
            logger.warning(f"ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_ensemble_answer(self, ai_responses: List[AIResponse], weights: Dict[str, float], query: str, query_type: str) -> str:
        """ì•™ìƒë¸” í•™ìŠµì„ í™œìš©í•œ ìµœì¢… ë‹µë³€ ìƒì„±"""
        try:
            # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_responses = sorted(ai_responses, key=lambda x: weights.get(x.model_name, 0), reverse=True)
            
            # ì£¼ìš” ì‘ë‹µ ì„ íƒ (ê°€ì¤‘ì¹˜ ìƒìœ„ 2ê°œ)
            primary_responses = sorted_responses[:2]
            
            # ì•™ìƒë¸” ë‹µë³€ êµ¬ì„±
            ensemble_answer = self._construct_ensemble_answer(primary_responses, weights, query, query_type)
            
            return ensemble_answer
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê°€ì¥ í’ˆì§ˆì´ ë†’ì€ ì‘ë‹µ ì‚¬ìš©
            best_response = max(ai_responses, key=lambda x: x.quality_score)
            return best_response.content
    
    def _construct_ensemble_answer(self, primary_responses: List[AIResponse], weights: Dict[str, float], query: str, query_type: str) -> str:
        """ì•™ìƒë¸” ë‹µë³€ êµ¬ì„±"""
        try:
            # ê¸°ë³¸ êµ¬ì¡°
            answer_parts = []
            
            # í†µí•© ë‹µë³€ ì„¹ì…˜
            main_content = primary_responses[0].content
            if len(primary_responses) > 1:
                # ë‘ ë²ˆì§¸ ì‘ë‹µì˜ ë³´ì™„ ì •ë³´ ì¶”ê°€
                secondary_content = primary_responses[1].content
                main_content = self._merge_responses(main_content, secondary_content)
            
            answer_parts.append(f"## ğŸ¯ í†µí•© ë‹µë³€\n{main_content}")
            
            # ê° AI ë¶„ì„ ì„¹ì…˜
            analysis_parts = []
            for resp in primary_responses:
                weight = weights.get(resp.model_name, 0)
                analysis = f"### {resp.model_name.upper()}\n"
                analysis += f"- ì‹ ë¢°ë„: {resp.confidence:.1%}\n"
                analysis += f"- í’ˆì§ˆ ì ìˆ˜: {resp.quality_score:.1%}\n"
                analysis += f"- ì•™ìƒë¸” ê¸°ì—¬ë„: {weight:.1%}\n"
                analysis += f"- ì¥ì : {self._get_model_strengths(resp.model_name, query_type)}\n"
                analysis_parts.append(analysis)
            
            answer_parts.append(f"## ğŸ“Š ê° AI ë¶„ì„\n" + "\n".join(analysis_parts))
            
            # ë¶„ì„ ê·¼ê±° ì„¹ì…˜
            reasoning = f"## ğŸ” ë¶„ì„ ê·¼ê±°\n"
            reasoning += f"- ì§ˆë¬¸ ìœ í˜•: {query_type}\n"
            reasoning += f"- ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {dict(sorted(weights.items(), key=lambda x: x[1], reverse=True))}\n"
            reasoning += f"- ì£¼ìš” ê¸°ì—¬ ëª¨ë¸: {primary_responses[0].model_name.upper()}\n"
            
            answer_parts.append(reasoning)
            
            # ìµœì¢… ì¶”ì²œ ì„¹ì…˜
            recommendation = f"## ğŸ† ìµœì¢… ì¶”ì²œ\n"
            recommendation += f"- {query_type} ìœ í˜• ì§ˆë¬¸ì—ëŠ” {primary_responses[0].model_name.upper()}ê°€ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤.\n"
            recommendation += f"- ì „ì²´ ì‹ ë¢°ë„: {sum(resp.confidence * weights.get(resp.model_name, 0) for resp in primary_responses):.1%}\n"
            
            answer_parts.append(recommendation)
            
            return "\n\n".join(answer_parts)
            
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ë‹µë³€ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return primary_responses[0].content if primary_responses else "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _merge_responses(self, primary: str, secondary: str) -> str:
        """ë‘ ì‘ë‹µì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©"""
        try:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            primary_sentences = re.split(r'[.!?]+', primary)
            secondary_sentences = re.split(r'[.!?]+', secondary)
            
            # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
            merged_sentences = []
            used_sentences = set()
            
            # ì£¼ìš” ì‘ë‹µ ìš°ì„ 
            for sentence in primary_sentences:
                sentence = sentence.strip()
                if sentence and sentence not in used_sentences:
                    merged_sentences.append(sentence)
                    used_sentences.add(sentence)
            
            # ë³´ì¡° ì‘ë‹µì—ì„œ ë³´ì™„ ì •ë³´ ì¶”ê°€
            for sentence in secondary_sentences:
                sentence = sentence.strip()
                if sentence and sentence not in used_sentences and len(sentence) > 20:
                    merged_sentences.append(sentence)
                    used_sentences.add(sentence)
            
            return '. '.join(merged_sentences) + '.'
            
        except Exception as e:
            logger.warning(f"ì‘ë‹µ ë³‘í•© ì‹¤íŒ¨: {e}")
            return primary
    
    def _get_model_strengths(self, model_name: str, query_type: str) -> str:
        """ëª¨ë¸ë³„ ê°•ì  ë°˜í™˜"""
        strengths = {
            'gpt': {
                'technical': 'ì½”ë”©ê³¼ ê¸°ìˆ ì  ë¬¸ì œ í•´ê²°ì— ë›°ì–´ë‚¨',
                'factual': 'ì‚¬ì‹¤ ì •ë³´ ì œê³µì— ì •í™•í•¨',
                'analytical': 'ë…¼ë¦¬ì  ë¶„ì„ê³¼ ì¶”ë¡ ì´ ê°•í•¨',
                'creative': 'ì°½ì˜ì  ì•„ì´ë””ì–´ ìƒì„± ê°€ëŠ¥',
                'general': 'ë‹¤ì–‘í•œ ì£¼ì œì— ê· í˜•ì¡íŒ ë‹µë³€'
            },
            'claude': {
                'technical': 'ë³µì¡í•œ ê¸°ìˆ  ë¬¸ì œ ì´í•´ë„ê°€ ë†’ìŒ',
                'factual': 'ì •í™•í•œ ì •ë³´ ê²€ì¦ê³¼ ì œê³µ',
                'analytical': 'ì‹¬ì¸µ ë¶„ì„ê³¼ í†µì°°ë ¥ ì œê³µ',
                'creative': 'ì°½ì˜ì  ì‚¬ê³ ì™€ ê¸€ì“°ê¸° ì „ë¬¸',
                'general': 'ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€'
            },
            'mixtral': {
                'technical': 'ë¹ ë¥¸ ê¸°ìˆ ì  ì‘ë‹µ ì œê³µ',
                'factual': 'ê°„ê²°í•œ ì‚¬ì‹¤ ì •ë³´ ì „ë‹¬',
                'analytical': 'íš¨ìœ¨ì ì¸ ë¶„ì„ê³¼ ìš”ì•½',
                'creative': 'ì‹ ì†í•œ ì°½ì˜ì  ì•„ì´ë””ì–´',
                'general': 'ë¹ ë¥´ê³  ê°„ê²°í•œ ë‹µë³€'
            }
        }
        
        return strengths.get(model_name, {}).get(query_type, 'ê· í˜•ì¡íŒ ë‹µë³€ ì œê³µ')
    
    def _calculate_quality_metrics(self, ai_responses: List[AIResponse], final_answer: str) -> Dict[str, float]:
        """í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        try:
            # ì „ì²´ ì‹ ë¢°ë„
            overall_confidence = np.mean([resp.confidence for resp in ai_responses])
            
            # í’ˆì§ˆ ì ìˆ˜
            quality_score = np.mean([resp.quality_score for resp in ai_responses])
            
            # ìœ ìš©ì„± ì ìˆ˜
            helpfulness_score = np.mean([resp.helpfulness_score for resp in ai_responses])
            
            # ëª…í™•ì„± ì ìˆ˜
            clarity_score = np.mean([resp.clarity_score for resp in ai_responses])
            
            # ì‚¬ì‹¤ ì •í™•ì„±
            factual_accuracy = np.mean([resp.factual_accuracy for resp in ai_responses])
            
            return {
                'overall_confidence': overall_confidence,
                'quality_score': quality_score,
                'helpfulness_score': helpfulness_score,
                'clarity_score': clarity_score,
                'factual_accuracy': factual_accuracy
            }
            
        except Exception as e:
            logger.warning(f"í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'overall_confidence': 0.5,
                'quality_score': 0.5,
                'helpfulness_score': 0.5,
                'clarity_score': 0.5,
                'factual_accuracy': 0.5
            }
    
    def _convert_to_dict(self, result: EnsembleResult) -> Dict[str, Any]:
        """EnsembleResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'final_answer': result.final_answer,
            'confidence_score': result.confidence_score,
            'consensus_level': result.consensus_level,
            'contributing_ais': result.contributing_ais,
            'disagreements': result.disagreements,
            'reasoning': result.reasoning,
            'query_type': result.query_type,
            'ensemble_weights': result.ensemble_weights,
            'quality_metrics': result.quality_metrics
        }
    
    def _create_fallback_result(self, responses: Dict[str, str]) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        if responses:
            first_response = list(responses.values())[0]
            return {
                'final_answer': first_response,
                'confidence_score': 0.5,
                'consensus_level': 'low',
                'contributing_ais': list(responses.keys())[:1],
                'disagreements': [],
                'reasoning': "ì•™ìƒë¸” í•™ìŠµ ì‹¤íŒ¨ë¡œ í´ë°± ëª¨ë“œ ì‚¬ìš©",
                'query_type': 'general',
                'ensemble_weights': {list(responses.keys())[0]: 1.0},
                'quality_metrics': {
                    'overall_confidence': 0.5,
                    'quality_score': 0.5,
                    'helpfulness_score': 0.5,
                    'clarity_score': 0.5,
                    'factual_accuracy': 0.5
                }
            }
        else:
            return {
                'final_answer': "AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'confidence_score': 0.0,
                'consensus_level': 'none',
                'contributing_ais': [],
                'disagreements': [],
                'reasoning': "ëª¨ë“  AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨",
                'query_type': 'general',
                'ensemble_weights': {},
                'quality_metrics': {
                    'overall_confidence': 0.0,
                    'quality_score': 0.0,
                    'helpfulness_score': 0.0,
                    'clarity_score': 0.0,
                    'factual_accuracy': 0.0
                }
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ensemble_learning_optimizer = EnsembleLearningOptimizer()
