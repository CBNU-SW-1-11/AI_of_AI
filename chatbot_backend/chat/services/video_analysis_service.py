# chat/services/video_analysis_service.py - ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤
import os
import json
import threading
import time
import cv2
import numpy as np
from django.conf import settings
from django.utils import timezone
from ..models import VideoAnalysisCache, Video
import logging

# YOLO ëª¨ë¸ import
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print("âœ… YOLO ë¡œë“œ ì„±ê³µ")
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸ YOLO ë¯¸ì„¤ì¹˜ - ê°ì²´ ê°ì§€ ê¸°ëŠ¥ ì œí•œ")

# DeepFace import (ì„±ë³„/ë‚˜ì´/ê°ì • ë¶„ì„)
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    print("âœ… DeepFace ë¡œë“œ ì„±ê³µ")
except ImportError:
    DEEPFACE_AVAILABLE = False
    print("âš ï¸ DeepFace ë¯¸ì„¤ì¹˜ - ì–¼êµ´ ë¶„ì„ ê¸°ëŠ¥ ì œí•œ")

# Transformers import (BLIP-2 ìº¡ì…˜ ìƒì„±)
try:
    from transformers import BlipProcessor, BlipForConditionalGeneration
    from PIL import Image
    BLIP_AVAILABLE = True
    print("âœ… BLIP ë¡œë“œ ì„±ê³µ")
except ImportError:
    BLIP_AVAILABLE = False
    print("âš ï¸ BLIP ë¯¸ì„¤ì¹˜ - ìº¡ì…˜ ìƒì„± ê¸°ëŠ¥ ì œí•œ")

# Ollama Vision import (llava ëª¨ë¸ - ìº¡ì…˜ ìƒì„±)
try:
    import ollama
    OLLAMA_AVAILABLE = True
    print("âœ… Ollama ë¡œë“œ ì„±ê³µ")
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ Ollama ë¯¸ì„¤ì¹˜ - BLIPë§Œ ì‚¬ìš©")

logger = logging.getLogger(__name__)

class VideoAnalysisService:
    """í•˜ì´ë¸Œë¦¬ë“œ ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤ (YOLO + DeepFace + Ollama + BLIP)"""
    
    def __init__(self):
        self.analysis_modules_available = True
        
        # YOLO ëª¨ë¸ ì´ˆê¸°í™”
        self.yolo_model = None
        if YOLO_AVAILABLE:
            try:
                self.yolo_model = YOLO('yolov8n.pt')
                logger.info("âœ… YOLO ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ YOLO ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.yolo_model = None
        
        # BLIP ëª¨ë¸ ì´ˆê¸°í™” (ìº¡ì…˜ ìƒì„± - ë°±ì—…ìš©)
        self.blip_processor = None
        self.blip_model = None
        if BLIP_AVAILABLE:
            try:
                self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                logger.info("âœ… BLIP ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ BLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Ollama ì‚¬ìš© ì—¬ë¶€ (ìº¡ì…˜ ìƒì„± - 1ìˆœìœ„)
        self.use_ollama = OLLAMA_AVAILABLE
        
        # DeepFace ì‚¬ìš© ì—¬ë¶€
        self.use_deepface = DEEPFACE_AVAILABLE
        
        # GPT-4V ì‚¬ìš© ì—¬ë¶€ (ë¹„í™œì„±í™” ê¸°ë³¸)
        self.use_gpt4v = False
        
        # í†µê³„ ë³€ìˆ˜
        self.stats = {
            'deepface_success': 0,
            'deepface_fail': 0,
            'gpt4v_calls': 0,
            'ollama_calls': 0,
            'blip_calls': 0,
            'total_cost': 0.0
        }
        
        logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ì˜ìƒ ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - YOLO: {YOLO_AVAILABLE}")
        logger.info(f"   - DeepFace: {DEEPFACE_AVAILABLE}")
        logger.info(f"   - Ollama: {OLLAMA_AVAILABLE}")
        logger.info(f"   - BLIP: {BLIP_AVAILABLE}")
    
    def sync_video_status_with_files(self, video_id):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœì™€ ì‹¤ì œ íŒŒì¼ ìƒíƒœë¥¼ ë™ê¸°í™”"""
        try:
            video = Video.objects.get(id=video_id)
            
            # ë¶„ì„ ê²°ê³¼ íŒŒì¼ í™•ì¸ (ê²½ë¡œê°€ Noneì¸ ê²½ìš°ë„ í™•ì¸)
            analysis_file_exists = False
            analysis_file_path = None
            
            if video.analysis_json_path:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                full_path = os.path.join(settings.MEDIA_ROOT, video.analysis_json_path)
                analysis_file_exists = os.path.exists(full_path)
                analysis_file_path = video.analysis_json_path
            else:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš°, ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
                analysis_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
                if os.path.exists(analysis_dir):
                    for filename in os.listdir(analysis_dir):
                        if f'analysis_{video_id}_' in filename and filename.endswith('.json'):
                            analysis_file_path = f'analysis_results/{filename}'
                            analysis_file_exists = True
                            logger.info(f"ğŸ” ì˜ìƒ {video_id} ë¶„ì„ íŒŒì¼ ë°œê²¬: {analysis_file_path}")
                            break
            
            # í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸ (ê²½ë¡œê°€ Noneì¸ ê²½ìš°ë„ í™•ì¸)
            frame_files_exist = False
            frame_image_paths = None
            
            if video.frame_images_path:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                frame_paths = video.frame_images_path.split(',')
                frame_files_exist = all(
                    os.path.exists(os.path.join(settings.MEDIA_ROOT, path.strip()))
                    for path in frame_paths
                )
                frame_image_paths = video.frame_images_path
            else:
                # ë°ì´í„°ë² ì´ìŠ¤ì— ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš°, ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
                images_dir = os.path.join(settings.MEDIA_ROOT, 'images')
                if os.path.exists(images_dir):
                    frame_files = []
                    for filename in os.listdir(images_dir):
                        if f'video{video_id}_frame' in filename and filename.endswith('.jpg'):
                            frame_files.append(f'images/{filename}')
                    
                    if frame_files:
                        frame_files_exist = all(
                            os.path.exists(os.path.join(settings.MEDIA_ROOT, path))
                            for path in frame_files
                        )
                        if frame_files_exist:
                            frame_image_paths = ','.join(frame_files)
                            logger.info(f"ğŸ” ì˜ìƒ {video_id} í”„ë ˆì„ ì´ë¯¸ì§€ ë°œê²¬: {len(frame_files)}ê°œ")
            
            # ìƒíƒœ ë™ê¸°í™” ë¡œì§
            if analysis_file_exists and frame_files_exist:
                if video.analysis_status != 'completed':
                    logger.info(f"ğŸ”„ ì˜ìƒ {video_id} ìƒíƒœ ë™ê¸°í™”: completedë¡œ ë³€ê²½")
                    video.analysis_status = 'completed'
                    video.analysis_progress = 100
                    video.analysis_message = 'ë¶„ì„ ì™„ë£Œ'
                    
                    # íŒŒì¼ ê²½ë¡œ ì—…ë°ì´íŠ¸
                    if analysis_file_path and not video.analysis_json_path:
                        video.analysis_json_path = analysis_file_path
                    if frame_image_paths and not video.frame_images_path:
                        video.frame_images_path = frame_image_paths
                    
                    video.save()
                    return True
            elif video.analysis_status == 'completed' and not analysis_file_exists:
                logger.warning(f"âš ï¸ ì˜ìƒ {video_id}: completed ìƒíƒœì´ì§€ë§Œ ë¶„ì„ íŒŒì¼ ì—†ìŒ")
                video.analysis_status = 'failed'
                video.analysis_message = 'ë¶„ì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'
                video.save()
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìƒíƒœ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _detect_persons_with_yolo(self, frame):
        """ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ëŒ ê°ì§€ (YOLO + DeepFace + ìƒ‰ìƒ ë¶„ì„)"""
        if not self.yolo_model:
            return []
        
        try:
            # YOLOë¡œ ê°ì²´ ê°ì§€
            results = self.yolo_model(frame, verbose=False, conf=0.25)
            
            detected_persons = []
            h, w = frame.shape[:2]
            
            for result in results:
                if result.boxes is not None:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy()
                    
                    for box, conf, class_id in zip(boxes, confidences, class_ids):
                        # í´ë˜ìŠ¤ IDë¥¼ ì‹¤ì œ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                        class_name = self.yolo_model.names[int(class_id)]
                        
                        # person í´ë˜ìŠ¤ë§Œ ì²˜ë¦¬
                        if class_name == 'person':
                            # ë°”ìš´ë”© ë°•ìŠ¤ (í”½ì…€ ë‹¨ìœ„)
                            x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])
                            
                            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ê·œí™”
                            normalized_bbox = [
                                float(x1/w), float(y1/h),
                                float(x2/w), float(y2/h)
                            ]
                            
                            # ì‚¬ëŒ ì˜ì—­ ì¶”ì¶œ
                            person_region = frame[y1:y2, x1:x2]
                            
                            # ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„: DeepFace + ìƒ‰ìƒ ì¶”ì¶œ
                            person_analysis = self._hybrid_person_analysis(person_region, frame, (x1, y1, x2, y2))
                            
                            detected_persons.append({
                                'class': 'person',
                                'bbox': normalized_bbox,
                                'confidence': float(conf),
                                'confidence_level': float(conf),
                                'attributes': person_analysis['attributes'],
                                'clothing_colors': person_analysis['clothing_colors'],
                                'analysis_source': person_analysis['source']
                            })
            
            return detected_persons
            
        except Exception as e:
            logger.warning(f"YOLO ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _hybrid_person_analysis(self, person_region, full_frame, bbox):
        """ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ëŒ ë¶„ì„ (DeepFace â†’ GPT-4V í´ë°±)"""
        x1, y1, x2, y2 = bbox
        person_h, person_w = person_region.shape[:2]
        
        # ê¸°ë³¸ê°’
        default_result = {
            'source': 'fallback',
            'attributes': self._get_default_attributes(),
            'clothing_colors': {'upper': 'unknown', 'lower': 'unknown'}
        }
        
        # ì˜ì—­ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë¶„ì„ ìŠ¤í‚µ
        if person_h < 50 or person_w < 30:
            logger.warning("ì‚¬ëŒ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            return default_result
        
        # 1ë‹¨ê³„: DeepFace ë¶„ì„ (ë¬´ë£Œ, ë¹ ë¦„)
        if self.use_deepface:
            deepface_result = self._analyze_with_deepface(person_region)
            if deepface_result and deepface_result['confidence'] > 0.7:
                # DeepFace ì‹ ë¢°ë„ ë†’ìŒ â†’ ì‚¬ìš©
                self.stats['deepface_success'] += 1
                
                # ì˜ìƒ ìƒ‰ìƒ ì¶”ì¶œ (OpenCV)
                clothing_colors = self._extract_clothing_colors(person_region)
                
                return {
                    'source': 'DeepFace',
                    'attributes': deepface_result['attributes'],
                    'clothing_colors': clothing_colors
                }
            else:
                self.stats['deepface_fail'] += 1
        
        # 2ë‹¨ê³„: GPT-4V ë¶„ì„ (ì‹ ë¢°ë„ ë‚®ê±°ë‚˜ DeepFace ì‹¤íŒ¨ ì‹œ)
        use_gpt4v = getattr(self, 'use_gpt4v', False)
        gpt4v_calls = self.stats.get('gpt4v_calls', 0)
        if use_gpt4v and gpt4v_calls < 10:  # ìµœëŒ€ 10íšŒ ì œí•œ
            gpt4v_result = self._analyze_with_gpt4v(person_region)
            if gpt4v_result:
                self.stats['gpt4v_calls'] = gpt4v_calls + 1
                self.stats['total_cost'] += 0.015
                
                return {
                    'source': 'GPT-4V',
                    'attributes': gpt4v_result['attributes'],
                    'clothing_colors': gpt4v_result['clothing_colors']
                }
        
        # 3ë‹¨ê³„: í´ë°± (ìƒ‰ìƒë§Œì´ë¼ë„ ì¶”ì¶œ)
        clothing_colors = self._extract_clothing_colors(person_region)
        default_result['clothing_colors'] = clothing_colors
        return default_result
    
    def _analyze_with_deepface(self, person_region):
        """DeepFaceë¡œ ì„±ë³„/ë‚˜ì´/ê°ì • ë¶„ì„"""
        try:
            # BGR â†’ RGB ë³€í™˜
            person_rgb = cv2.cvtColor(person_region, cv2.COLOR_BGR2RGB)
            
            # DeepFace ë¶„ì„
            analysis = DeepFace.analyze(
                person_rgb,
                actions=['age', 'gender', 'emotion'],
                enforce_detection=False,
                detector_backend='opencv'
            )
            
            # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
            if isinstance(analysis, list):
                analysis = analysis[0]
            
            # ì„±ë³„ ì •ë³´
            gender = analysis.get('dominant_gender', 'Unknown')  # Man/Woman
            gender_conf = analysis.get('gender', {}).get(gender, 0) / 100.0 if isinstance(analysis.get('gender'), dict) else 0.7
            
            # ë‚˜ì´ ì •ë³´
            age = analysis.get('age', 30)
            age_group = self._age_to_group(age)
            
            # ê°ì • ì •ë³´
            emotion = analysis.get('dominant_emotion', 'neutral')
            
            return {
                'confidence': gender_conf,
                'attributes': {
                    'gender': {
                        'value': gender.lower(),  # man/woman
                        'confidence': gender_conf,
                        'all_scores': analysis.get('gender', {}),
                        'top_3': [[k, v/100.0] for k, v in sorted(analysis.get('gender', {}).items(), key=lambda x: x[1], reverse=True)[:3]]
                    },
                    'age': {
                        'value': age_group,
                        'confidence': 0.8,
                        'estimated_age': int(age),
                        'all_scores': {},
                        'top_3': [[age_group, 0.8]]
                    },
                    'emotion': {
                        'value': emotion,
                        'confidence': 0.7,
                        'all_scores': analysis.get('emotion', {})
                    }
                }
            }
            
        except Exception as e:
            logger.debug(f"DeepFace ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_with_gpt4v(self, person_region):
        """GPT-4 Visionìœ¼ë¡œ ìƒì„¸ ë¶„ì„ (ì¡°ê±´ë¶€ ì‚¬ìš©)"""
        try:
            import base64
            from io import BytesIO
            from PIL import Image
            
            # BGR â†’ RGB
            person_rgb = cv2.cvtColor(person_region, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(person_rgb)
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            buffered = BytesIO()
            pil_image.save(buffered, format="JPEG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            prompt = """ì´ë¯¸ì§€ ì† ì‚¬ëŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "gender": "man" ë˜ëŠ” "woman",
  "age_group": "child/teenager/young_adult/middle_aged/elderly",
  "upper_clothing_color": "ìƒ‰ìƒ (í•œêµ­ì–´)",
  "lower_clothing_color": "ìƒ‰ìƒ (í•œêµ­ì–´)",
  "clothing_style": "casual/formal/sport"
}

JSONë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # ì €ë ´í•œ ë²„ì „ ì‚¬ìš©
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}}
                    ]
                }],
                max_tokens=150,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±
            import json
            result_json = json.loads(response.choices[0].message.content)
            
            return {
                'attributes': {
                    'gender': {
                        'value': result_json.get('gender', 'unknown'),
                        'confidence': 0.95,
                        'all_scores': {},
                        'top_3': [[result_json.get('gender', 'unknown'), 0.95]]
                    },
                    'age': {
                        'value': result_json.get('age_group', 'adult'),
                        'confidence': 0.9,
                        'all_scores': {},
                        'top_3': [[result_json.get('age_group', 'adult'), 0.9]]
                    },
                    'clothing': {
                        'value': result_json.get('clothing_style', 'casual'),
                        'confidence': 0.85
                    }
                },
                'clothing_colors': {
                    'upper': result_json.get('upper_clothing_color', 'unknown'),
                    'lower': result_json.get('lower_clothing_color', 'unknown')
                }
            }
            
        except Exception as e:
            logger.warning(f"GPT-4V ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_clothing_colors(self, person_region):
        """ì˜ìƒ ìƒ‰ìƒ ì¶”ì¶œ (ìƒì˜/í•˜ì˜ ë¶„ë¦¬)"""
        try:
            h, w = person_region.shape[:2]
            
            x_start = int(w * 0.2)
            x_end = int(w * 0.8) if int(w * 0.8) > x_start else w
            
            upper_top = int(h * 0.2)
            upper_bottom = int(h * 0.5)
            lower_top = int(h * 0.5)
            lower_bottom = int(h * 0.85)
            
            upper_region = person_region[upper_top:upper_bottom, x_start:x_end]
            upper_color = self._get_dominant_color_name(upper_region)
            
            lower_region = person_region[lower_top:lower_bottom, x_start:x_end]
            lower_color = self._get_dominant_color_name(lower_region)
            
            return {
                'upper': upper_color,
                'lower': lower_color
            }
            
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'upper': 'unknown', 'lower': 'unknown'}
    
    def _get_dominant_color_name(self, image_region):
        """ì´ë¯¸ì§€ ì˜ì—­ì˜ ì£¼ìš” ìƒ‰ìƒ ì´ë¦„ ë°˜í™˜"""
        try:
            if image_region.size == 0:
                return 'unknown'
            
            hsv = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
            pixels = hsv.reshape(-1, 3)
            
            # ì±„ë„ê°€ ë„ˆë¬´ ë‚®ì€ í”½ì…€ ì œì™¸ (ë¬´ì±„ìƒ‰ íŒë³„ì— ì‚¬ìš©)
            saturation_threshold = 35
            high_sat_pixels = pixels[pixels[:, 1] >= saturation_threshold]
            low_sat_pixels = pixels[pixels[:, 1] < saturation_threshold]
            
            if len(high_sat_pixels) == 0:
                # ë‚¨ì€ í”½ì…€ì´ ëª¨ë‘ ë¬´ì±„ìƒ‰ì´ë©´ ë°ê¸°ì— ë”°ë¼ ë°˜í™˜
                v_mean = np.mean(pixels[:, 2])
                if v_mean > 200:
                    return 'white'
                if v_mean < 50:
                    return 'black'
                return 'gray'
            
            # K-meansë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ í´ëŸ¬ìŠ¤í„°)
            K = min(3, len(high_sat_pixels))
            data = np.float32(high_sat_pixels)
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
            _, labels, centers = cv2.kmeans(data, K, None, criteria, 5, cv2.KMEANS_PP_CENTERS)
            counts = np.bincount(labels.flatten(), minlength=K)
            
            # ì±„ë„ ê°€ì¤‘ì¹˜ë¡œ ê°€ì¥ ìƒìƒí•œ ìƒ‰ ì„ íƒ
            center_s = centers[:, 1] / 255.0
            weights = counts * (center_s + 0.1)
            main_idx = int(np.argmax(weights))
            h_mean, s_mean, v_mean = centers[main_idx]
            
            # ì„ íƒëœ í´ëŸ¬ìŠ¤í„°ê°€ ì—¬ì „íˆ ì±„ë„ê°€ ë‚®ìœ¼ë©´ ë¬´ì±„ìƒ‰ ì²˜ë¦¬
            if s_mean < saturation_threshold:
                if v_mean > 200:
                    return 'white'
                if v_mean < 50:
                    return 'black'
                return 'gray'
            
            # ìƒ‰ìƒ ë¶„ë¥˜ (HSV Hue ë²”ìœ„ëŠ” 0~180)
            if h_mean < 10 or h_mean >= 175:
                return 'red'
            if h_mean < 20:
                return 'orange'
            if h_mean < 35:
                return 'yellow'
            if h_mean < 85:
                return 'green'
            if h_mean < 115:
                return 'cyan'
            if h_mean < 135:
                return 'blue'
            if h_mean < 155:
                return 'purple'
            if h_mean < 175:
                return 'pink'
            return 'red'
                
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ì´ë¦„ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _age_to_group(self, age):
        """ë‚˜ì´ë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë³€í™˜"""
        if age < 13:
            return 'child'
        elif age < 20:
            return 'teenager'
        elif age < 35:
            return 'young_adult'
        elif age < 60:
            return 'middle_aged'
        else:
            return 'elderly'
    
    def _get_default_attributes(self):
        """ê¸°ë³¸ ì†ì„±ê°’"""
        return {
            'gender': {
                'value': 'person',
                'confidence': 0.5,
                'all_scores': {'a person': 0.5},
                'top_3': [['a person', 0.5]]
            },
            'age': {
                'value': 'adult',
                'confidence': 0.5,
                'all_scores': {'adult': 0.5},
                'top_3': [['adult', 0.5]]
            }
        }
    
    def _get_dominant_color(self, image_region):
        """ì˜ì—­ì˜ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (HSV ê¸°ë°˜)"""
        try:
            # HSVë¡œ ë³€í™˜í•˜ì—¬ ìƒ‰ìƒ ë¶„ì„
            hsv = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
            h_mean = np.mean(hsv[:, :, 0])
            
            # ìƒ‰ìƒ ë²”ìœ„ë³„ ë¶„ë¥˜ (ë” ì„¸ë¶„í™”)
            if h_mean < 10 or h_mean > 170:
                return 'red'
            elif h_mean < 25:
                return 'orange'
            elif h_mean < 40:
                return 'yellow'
            elif h_mean < 80:
                return 'green'
            elif h_mean < 130:
                return 'blue'
            elif h_mean < 160:
                return 'purple'
            else:
                return 'pink'
        except Exception as e:
            logger.warning(f"ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _analyze_frame_colors(self, frame_rgb):
        """í”„ë ˆì„ì˜ ì£¼ìš” ìƒ‰ìƒ ë¶„ì„"""
        try:
            # HSVë¡œ ë³€í™˜
            hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            dominant_colors = []
            
            # ìƒ‰ìƒë³„ ë§ˆìŠ¤í¬ ìƒì„± ë° ë¶„ì„ (ê°œì„ ëœ ë¶„í™ìƒ‰ ê°ì§€)
            color_ranges = {
                'red': [(0, 50, 50), (10, 255, 255)],  # ë¹¨ê°„ìƒ‰ ë²”ìœ„
                'orange': [(10, 50, 50), (25, 255, 255)],  # ì£¼í™©ìƒ‰ ë²”ìœ„
                'yellow': [(25, 50, 50), (40, 255, 255)],  # ë…¸ë€ìƒ‰ ë²”ìœ„
                'green': [(40, 50, 50), (80, 255, 255)],  # ì´ˆë¡ìƒ‰ ë²”ìœ„
                'blue': [(80, 50, 50), (130, 255, 255)],  # íŒŒë€ìƒ‰ ë²”ìœ„
                'purple': [(130, 50, 50), (160, 255, 255)],  # ë³´ë¼ìƒ‰ ë²”ìœ„
                'pink': [(160, 20, 100), (180, 255, 255), (0, 20, 100), (15, 255, 255)]  # ë¶„í™ìƒ‰ ë²”ìœ„ (ë” ë„“ê³  ì •í™•í•œ ë²”ìœ„)
            }
            
            for color_name, color_range in color_ranges.items():
                # ë¶„í™ìƒ‰ì˜ ê²½ìš° ë‘ ê°œì˜ ë²”ìœ„ ì‚¬ìš©
                if color_name == 'pink':
                    # ì²« ë²ˆì§¸ ë²”ìœ„ (160-180)
                    mask1 = cv2.inRange(hsv, np.array(color_range[0]), np.array(color_range[1]))
                    # ë‘ ë²ˆì§¸ ë²”ìœ„ (0-10, ë” ë°ì€ ë¶„í™ìƒ‰)
                    mask2 = cv2.inRange(hsv, np.array(color_range[2]), np.array(color_range[3]))
                    mask = cv2.bitwise_or(mask1, mask2)
                else:
                    mask = cv2.inRange(hsv, np.array(color_range[0]), np.array(color_range[1]))
                
                # í•´ë‹¹ ìƒ‰ìƒì˜ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°
                color_ratio = np.sum(mask > 0) / (frame_rgb.shape[0] * frame_rgb.shape[1])
                
                # ë¶„í™ìƒ‰ì€ ë” ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš© (0.5% ì´ìƒ)
                threshold = 0.005 if color_name == 'pink' else 0.02
                
                if color_ratio > threshold:
                    # RGB ê¸°ë°˜ ì¶”ê°€ ê²€ì¦ (ë¶„í™ìƒ‰ì˜ ê²½ìš°)
                    if color_name == 'pink':
                        # ë¶„í™ìƒ‰ RGB íŠ¹ì„±: R > B, G ì¤‘ê°„ê°’
                        pink_pixels = frame_rgb[mask > 0]
                        if len(pink_pixels) > 0:
                            mean_rgb = np.mean(pink_pixels, axis=0)
                            # ë¶„í™ìƒ‰ íŠ¹ì„±: R > G > B (ëŒ€ëµì ìœ¼ë¡œ)
                            if mean_rgb[0] > mean_rgb[2] and mean_rgb[1] > mean_rgb[2] * 0.5:
                                confidence = min(color_ratio * 3, 1.0)  # ë¶„í™ìƒ‰ì€ ë†’ì€ ì‹ ë¢°ë„
                            else:
                                confidence = min(color_ratio * 1.5, 0.7)  # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš° ë‚®ì€ ì‹ ë¢°ë„
                        else:
                            confidence = min(color_ratio * 2, 1.0)
                    else:
                        confidence = min(color_ratio * 2, 1.0)  # ë¹„ìœ¨ì— ë”°ë¥¸ ì‹ ë¢°ë„
                    
                    dominant_colors.append({
                        'color': color_name,
                        'ratio': float(color_ratio),
                        'confidence': confidence
                    })
                    print(f"ğŸ¨ {color_name} ê°ì§€: {color_ratio:.3f} ({color_ratio*100:.1f}%) ì‹ ë¢°ë„: {confidence:.2f}")
            
            # ë¹„ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬
            dominant_colors.sort(key=lambda x: x['ratio'], reverse=True)
            
            return dominant_colors[:3]  # ìƒìœ„ 3ê°œ ìƒ‰ìƒë§Œ ë°˜í™˜
            
        except Exception as e:
            logger.warning(f"í”„ë ˆì„ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _update_progress(self, video_id, progress, message):
        """ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            video = Video.objects.get(id=video_id)
            video.analysis_progress = progress
            video.analysis_message = message
            video.save()
            logger.info(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {progress}% - {message}")
        except Exception as e:
            logger.warning(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def analyze_video(self, video_path, video_id):
        """ì˜ìƒ ë¶„ì„ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {video_path}")
            
            # Video ëª¨ë¸ì—ì„œ ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return False
            
            # ë¶„ì„ ìƒíƒœë¥¼ 'pending'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            video.analysis_status = 'pending'
            video.save()
            
            # ì „ì²´ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
            full_video_path = os.path.join(settings.MEDIA_ROOT, video_path)
            
            # ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰ (ì§„í–‰ë¥  í¬í•¨)
            analysis_result = self._perform_basic_analysis_with_progress(full_video_path, video_id)
            
            # JSON íŒŒì¼ë¡œ ë¶„ì„ ê²°ê³¼ ì €ì¥
            json_file_path = self._save_analysis_to_json(analysis_result, video_id)
            
            if not json_file_path:
                raise Exception("JSON íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ Video ëª¨ë¸ì— ì €ì¥ (ë” ì•ˆì „í•œ ë°©ì‹)
            try:
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœì‹  ìƒíƒœë¡œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
                video = Video.objects.get(id=video_id)
                
                # ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸
                video.analysis_status = 'completed'
                video.is_analyzed = True
                video.duration = analysis_result.get('video_summary', {}).get('total_time_span', 0.0)
                video.analysis_type = 'enhanced_opencv'
                video.analysis_json_path = json_file_path
                video.analysis_progress = 100
                video.analysis_message = 'ë¶„ì„ ì™„ë£Œ'
                
                # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
                frame_image_paths = [frame.get('frame_image_path') for frame in analysis_result.get('frame_results', []) if frame.get('frame_image_path')]
                if frame_image_paths:
                    video.frame_images_path = ','.join(frame_image_paths)
                
                # ì €ì¥ ì‹œë„
                video.save()
                logger.info(f"âœ… ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {video_id}")
                logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥: {json_file_path}")
                logger.info(f"âœ… Video ëª¨ë¸ ì €ì¥ ì™„ë£Œ: analysis_json_path = {video.analysis_json_path}")
                
                # ì €ì¥ í›„ ê²€ì¦
                video.refresh_from_db()
                if video.analysis_status != 'completed':
                    logger.error(f"âŒ ìƒíƒœ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨: {video.analysis_status}")
                    raise Exception("ë¶„ì„ ìƒíƒœ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨")
                
                # ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ í†µê³„ ì¶œë ¥
                logger.info("="*60)
                logger.info("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ í†µê³„")
                logger.info(f"  â€¢ DeepFace ì„±ê³µ: {self.stats['deepface_success']}íšŒ")
                logger.info(f"  â€¢ DeepFace ì‹¤íŒ¨: {self.stats['deepface_fail']}íšŒ")
                logger.info(f"  â€¢ Ollama ìº¡ì…˜: {self.stats['ollama_calls']}íšŒ")
                logger.info(f"  â€¢ BLIP ìº¡ì…˜: {self.stats['blip_calls']}íšŒ")
                logger.info(f"  â€¢ ì´ ë¹„ìš©: ${self.stats['total_cost']:.3f} (ë¬´ë£Œ)")
                logger.info(f"  â€¢ DeepFace ì„±ê³µë¥ : {self.stats['deepface_success']/(self.stats['deepface_success']+self.stats['deepface_fail'])*100:.1f}%" if (self.stats['deepface_success']+self.stats['deepface_fail']) > 0 else "  â€¢ DeepFace ì„±ê³µë¥ : N/A")
                logger.info("="*60)
                    
            except Exception as save_error:
                logger.error(f"âŒ Video ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ ìƒì„¸: {type(save_error).__name__}")
                import traceback
                logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ ìŠ¤íƒ: {traceback.format_exc()}")
                
                # ì €ì¥ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œì˜ ìƒíƒœëŠ” ì—…ë°ì´íŠ¸
                try:
                    video = Video.objects.get(id=video_id)
                    video.analysis_status = 'failed'
                    video.analysis_message = f'ë¶„ì„ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì €ì¥ ì‹¤íŒ¨: {str(save_error)}'
                    video.save()
                    logger.warning(f"âš ï¸ ìµœì†Œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
                except Exception as fallback_error:
                    logger.error(f"âŒ ìµœì†Œ ìƒíƒœ ì—…ë°ì´íŠ¸ë„ ì‹¤íŒ¨: {fallback_error}")
                
                raise
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            
            # êµ¬ì²´ì ì¸ ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
            error_type = "unknown"
            error_message = str(e)
            
            if "No such file or directory" in str(e):
                error_type = "file_not_found"
                error_message = "ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif "Permission denied" in str(e):
                error_type = "permission_denied"
                error_message = "ì˜ìƒ íŒŒì¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif "codec" in str(e).lower() or "format" in str(e).lower():
                error_type = "unsupported_format"
                error_message = "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜ìƒ í˜•ì‹ì…ë‹ˆë‹¤."
            elif "memory" in str(e).lower():
                error_type = "memory_error"
                error_message = "ì˜ìƒì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ íŒŒì¼ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
            elif "cv2" in str(e).lower() or "opencv" in str(e).lower():
                error_type = "opencv_error"
                error_message = "ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            elif "numpy" in str(e).lower():
                error_type = "numpy_error"
                error_message = "ì˜ìƒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            # ë¶„ì„ ì‹¤íŒ¨ ìƒíƒœ ì €ì¥ (ë” ì•ˆì „í•œ ë°©ì‹)
            try:
                video = Video.objects.get(id=video_id)
                video.analysis_status = 'failed'
                video.analysis_progress = 0
                video.analysis_message = f"ë¶„ì„ ì‹¤íŒ¨: {error_message}"
                video.save()
                
                # ì €ì¥ í›„ ê²€ì¦
                video.refresh_from_db()
                if video.analysis_status != 'failed':
                    logger.error(f"âŒ ì‹¤íŒ¨ ìƒíƒœ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨: {video.analysis_status}")
                else:
                    logger.info(f"âœ… ë¶„ì„ ì‹¤íŒ¨ ìƒíƒœ ì €ì¥ ì™„ë£Œ: {video_id}")
                    
            except Exception as save_error:
                logger.error(f"âŒ ì—ëŸ¬ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                logger.error(f"âŒ ì—ëŸ¬ ìƒíƒœ ì €ì¥ ìƒì„¸: {type(save_error).__name__}")
                import traceback
                logger.error(f"âŒ ì—ëŸ¬ ìƒíƒœ ì €ì¥ ìŠ¤íƒ: {traceback.format_exc()}")
            
            return {
                'success': False,
                'error_type': error_type,
                'error_message': error_message,
                'original_error': str(e)
            }
    
    def _perform_basic_analysis(self, video_path):
        """ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # OpenCVë¡œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ì˜ìƒ ì •ë³´
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ìƒ˜í”Œ í”„ë ˆì„ ë¶„ì„ (ì²˜ìŒ, ì¤‘ê°„, ë§ˆì§€ë§‰)
            sample_frames = []
            frame_indices = [0, frame_count // 2, frame_count - 1] if frame_count > 2 else [0]
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ê¸°ë³¸ í†µê³„ ì •ë³´
                    mean_color = np.mean(frame_rgb, axis=(0, 1))
                    brightness = np.mean(frame_rgb)
                    
                    sample_frames.append({
                        'frame_index': int(frame_idx),
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'mean_color': mean_color.tolist(),
                        'brightness': float(brightness),
                        'width': width,
                        'height': height
                    })
            
            cap.release()
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„± (backend_videochat ë°©ì‹)
            analysis_result = {
                'basic_info': {
                    'frame_count': frame_count,
                    'fps': fps,
                    'duration': duration,
                    'width': width,
                    'height': height,
                    'aspect_ratio': width / height if height > 0 else 0
                },
                'sample_frames': sample_frames,
                'analysis_type': 'basic_opencv',
                'summary': f"ì˜ìƒ ë¶„ì„ ì™„ë£Œ - {duration:.1f}ì´ˆ, {width}x{height}, {fps:.1f}fps"
            }
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'analysis_type': 'basic_opencv',
                'error': str(e),
                'summary': f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _perform_basic_analysis_with_progress(self, video_path, video_id):
        """ì§„í–‰ë¥ ì„ í¬í•¨í•œ ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì‹œì‘
            self._update_progress(video_id, 10, "ì˜ìƒ íŒŒì¼ì„ ì—´ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # OpenCVë¡œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(video_path):
                raise Exception(f"ì˜ìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_path}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(video_path)
            if file_size == 0:
                raise Exception("ì˜ìƒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            logger.info(f"ğŸ“ ì˜ìƒ íŒŒì¼ ì •ë³´: {video_path}, í¬ê¸°: {file_size / (1024*1024):.1f}MB")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            self._update_progress(video_id, 20, "ì˜ìƒ ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # ê¸°ë³¸ ì˜ìƒ ì •ë³´
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # ì˜ìƒ ì •ë³´ ìœ íš¨ì„± ê²€ì‚¬
            if frame_count <= 0:
                raise Exception("ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ìƒ íŒŒì¼ì…ë‹ˆë‹¤ (í”„ë ˆì„ ìˆ˜: 0)")
            if fps <= 0:
                raise Exception("ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ìƒ íŒŒì¼ì…ë‹ˆë‹¤ (FPS: 0)")
            if width <= 0 or height <= 0:
                raise Exception("ìœ íš¨í•˜ì§€ ì•Šì€ ì˜ìƒ íŒŒì¼ì…ë‹ˆë‹¤ (í•´ìƒë„: 0x0)")
            
            logger.info(f"ğŸ“Š ì˜ìƒ ì •ë³´: {frame_count}í”„ë ˆì„, {fps:.1f}fps, {width}x{height}, {duration:.1f}ì´ˆ")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (10%)
            self._update_progress(video_id, 10, "ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            time.sleep(0.5)
            
            # âœ¨ í•˜ì´ë¸Œë¦¬ë“œ í”„ë ˆì„ ìƒ˜í”Œë§ (10ë¶„ ì˜ìƒ ìµœì í™” - ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
            sample_frames = []
            frame_indices = []
            
            # ì˜ìƒ ê¸¸ì´ì— ë”°ë¼ ì ì‘ì  ìƒ˜í”Œë§ (ìµœëŒ€ 10ë¶„, Ollama ìº¡ì…˜ ìƒì„± ì‹œê°„ ê³ ë ¤)
            if duration <= 10:
                # 10ì´ˆ ì´í•˜: 0.5ì´ˆë‹¹ 1í”„ë ˆì„ (~20í”„ë ˆì„, 0.7ë¶„)
                sample_interval = max(1, int(fps * 0.5))
            elif duration <= 30:
                # 30ì´ˆ ì´í•˜: 1ì´ˆë‹¹ 1í”„ë ˆì„ (~30í”„ë ˆì„, 1ë¶„)
                sample_interval = max(1, int(fps))
            elif duration <= 60:
                # 1ë¶„ ì´í•˜: 2ì´ˆë‹¹ 1í”„ë ˆì„ (~30í”„ë ˆì„, 1ë¶„)
                sample_interval = max(1, int(fps * 2))
            elif duration <= 120:
                # 2ë¶„ ì´í•˜: 3ì´ˆë‹¹ 1í”„ë ˆì„ (~40í”„ë ˆì„, 1.3ë¶„)
                sample_interval = max(1, int(fps * 3))
            elif duration <= 300:
                # 5ë¶„ ì´í•˜: 4ì´ˆë‹¹ 1í”„ë ˆì„ (~75í”„ë ˆì„, 2.5ë¶„)
                sample_interval = max(1, int(fps * 4))
            else:
                # 10ë¶„ ì´í•˜: 6ì´ˆë‹¹ 1í”„ë ˆì„ (~100í”„ë ˆì„, 3.3ë¶„)
                sample_interval = max(1, int(fps * 6))
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ìƒì„±
            frame_indices = list(range(0, frame_count, sample_interval))
            
            # ë§ˆì§€ë§‰ í”„ë ˆì„ í¬í•¨
            if frame_indices[-1] != frame_count - 1:
                frame_indices.append(frame_count - 1)
            
            # ìµœëŒ€ 100ê°œë¡œ ì œí•œ (Ollama ìº¡ì…˜ ìƒì„± ì‹œê°„ ìµœì í™”: ì•½ 3.3ë¶„)
            if len(frame_indices) > 100:
                step = len(frame_indices) // 100
                frame_indices = frame_indices[::step][:100]
            
            # ìµœì†Œ 5ê°œ ë³´ì¥
            if len(frame_indices) < 5:
                frame_indices = [0, frame_count//4, frame_count//2, 3*frame_count//4, frame_count-1]
            
            logger.info(f"ğŸ“¸ ìƒ˜í”Œë§ ì „ëµ: {len(frame_indices)}ê°œ í”„ë ˆì„ (ê°„ê²©: {sample_interval}, FPS: {fps:.1f})")
            
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
            frame_indices = [idx for idx in frame_indices if 0 <= idx < frame_count]
            if not frame_indices:
                raise Exception("ìœ íš¨í•œ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (20%)
            self._update_progress(video_id, 20, f"í”„ë ˆì„ ìƒ˜í”Œë§ ì™„ë£Œ ({len(frame_indices)}ê°œ í”„ë ˆì„)")
            time.sleep(0.5)
            
            for i, frame_idx in enumerate(frame_indices):
                frame_read_success = False
                retry_indices = [frame_idx]
                
                # í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì£¼ë³€ í”„ë ˆì„ ì‹œë„
                if frame_idx > 0:
                    retry_indices.append(frame_idx - 1)
                if frame_idx < frame_count - 1:
                    retry_indices.append(frame_idx + 1)
                
                for retry_idx in retry_indices:
                    try:
                        cap.set(cv2.CAP_PROP_POS_FRAMES, retry_idx)
                        ret, frame = cap.read()
                        if ret and frame is not None:
                            frame_idx = retry_idx  # ì‹¤ì œ ì½ì€ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ì—…ë°ì´íŠ¸
                            frame_read_success = True
                            break
                    except Exception as e:
                        logger.warning(f"í”„ë ˆì„ {retry_idx} ì½ê¸° ì‹œë„ ì‹¤íŒ¨: {e}")
                        continue
                
                if not frame_read_success:
                    logger.warning(f"í”„ë ˆì„ {frame_idx} ì½ê¸° ì™„ì „ ì‹¤íŒ¨")
                    continue
                
                try:
                    # í”„ë ˆì„ì„ RGBë¡œ ë³€í™˜
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # ê¸°ë³¸ í†µê³„ ì •ë³´
                    mean_color = np.mean(frame_rgb, axis=(0, 1))
                    brightness = np.mean(frame_rgb)
                    
                    # ìƒ‰ìƒ íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
                    try:
                        hist_r = cv2.calcHist([frame_rgb], [0], None, [256], [0, 256])
                        hist_g = cv2.calcHist([frame_rgb], [1], None, [256], [0, 256])
                        hist_b = cv2.calcHist([frame_rgb], [2], None, [256], [0, 256])
                    except Exception as hist_error:
                        logger.warning(f"íˆìŠ¤í† ê·¸ë¨ ë¶„ì„ ì‹¤íŒ¨: {hist_error}")
                        hist_r = np.zeros((256, 1), dtype=np.float32)
                        hist_g = np.zeros((256, 1), dtype=np.float32)
                        hist_b = np.zeros((256, 1), dtype=np.float32)
                    
                    # ì—£ì§€ ê²€ì¶œ (ì•ˆì „í•˜ê²Œ)
                    try:
                        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                        edges = cv2.Canny(gray, 50, 150)
                        edge_density = np.sum(edges > 0) / (width * height)
                    except Exception as edge_error:
                        logger.warning(f"ì—£ì§€ ê²€ì¶œ ì‹¤íŒ¨: {edge_error}")
                        edge_density = 0.0
                    
                    # ìƒ‰ìƒ ë¶„ì„ ì¶”ê°€
                    dominant_colors = self._analyze_frame_colors(frame_rgb)
                    
                    sample_frames.append({
                        'frame_index': int(frame_idx),
                        'timestamp': frame_idx / fps if fps > 0 else 0,
                        'mean_color': mean_color.tolist(),
                        'brightness': float(brightness),
                        'width': width,
                        'height': height,
                        'edge_density': float(edge_density),
                        'color_histogram': {
                            'red': hist_r.flatten().tolist()[:10],  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
                            'green': hist_g.flatten().tolist()[:10],
                            'blue': hist_b.flatten().tolist()[:10]
                        },
                        'dominant_colors': dominant_colors
                    })
                    
                    logger.info(f"âœ… í”„ë ˆì„ {frame_idx} ë¶„ì„ ì™„ë£Œ")
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30% + 30% * (i+1)/len(frame_indices))
                    progress = 30 + int(30 * (i + 1) / len(frame_indices))
                    self._update_progress(video_id, progress, f"í”„ë ˆì„ ë¶„ì„ ì¤‘... ({i+1}/{len(frame_indices)})")
                    time.sleep(0.8)  # ì§„í–‰ë¥  í™•ì¸ì„ ìœ„í•œ ì§€ì—°
                    
                except Exception as e:
                    logger.warning(f"í”„ë ˆì„ {frame_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            cap.release()
            
            # ë¶„ì„ëœ í”„ë ˆì„ì´ ìˆëŠ”ì§€ í™•ì¸
            if not sample_frames:
                raise Exception("ë¶„ì„í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            logger.info(f"âœ… ì´ {len(sample_frames)}ê°œ í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (60%)
            self._update_progress(video_id, 60, "í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # ì˜ìƒ í’ˆì§ˆ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
            try:
                quality_analysis = self._analyze_video_quality(sample_frames)
            except Exception as quality_error:
                logger.warning(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {quality_error}")
                quality_analysis = {'overall_score': 0.5, 'status': 'unknown'}
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (70%)
            self._update_progress(video_id, 70, "í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # ì¥ë©´ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
            try:
                scene_analysis = self._analyze_scenes(sample_frames)
            except Exception as scene_error:
                logger.warning(f"ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {scene_error}")
                scene_analysis = {'scene_types': ['unknown'], 'diversity_score': 0.5}
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (80%)
            self._update_progress(video_id, 80, "ì¥ë©´ ë¶„ì„ ì™„ë£Œ")
            time.sleep(0.5)
            
            # í†µí•© ë¶„ì„ ê²°ê³¼ êµ¬ì„± (backend_videochat ì •í™•í•œ êµ¬ì¡°)
            # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ë° ê²½ë¡œ ìˆ˜ì§‘
            frame_results = self._format_frame_results(sample_frames, video_id)
            frame_image_paths = [frame.get('frame_image_path') for frame in frame_results if frame.get('frame_image_path')]
            
            analysis_result = {
                'success': True,
                'video_summary': {
                    'total_detections': len(sample_frames) * 2,  # í”„ë ˆì„ë‹¹ í‰ê·  2ê°œ ê°ì²´ë¡œ ê°€ì •
                    'unique_persons': 1,  # ê¸°ë³¸ê°’
                    'detailed_attribute_statistics': {
                        'object_type': {
                            'person': len(sample_frames)
                        }
                    },
                    'temporal_analysis': {
                        'peak_time_seconds': 0,
                        'peak_person_count': len(sample_frames),
                        'average_person_count': float(len(sample_frames)),
                        'total_time_span': int(duration),
                        'activity_distribution': {
                            str(int(timestamp)): 1 for timestamp in [frame['timestamp'] for frame in sample_frames]
                        }
                    },
                    'scene_diversity': scene_analysis,
                    'quality_assessment': quality_analysis,
                    'analysis_type': 'enhanced_opencv_analysis',
                    'key_insights': self._generate_key_insights(sample_frames, quality_analysis, scene_analysis)
                },
                'frame_results': frame_results
            }
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (90%)
            self._update_progress(video_id, 90, "ë¶„ì„ ê²°ê³¼ ì •ë¦¬ ì¤‘")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'analysis_type': 'enhanced_opencv',
                'error': str(e),
                'summary': f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            }
    
    def _analyze_video_quality(self, sample_frames):
        """ì˜ìƒ í’ˆì§ˆ ë¶„ì„"""
        try:
            if not sample_frames:
                return {
                    'overall_score': 0.0,
                    'status': 'unknown',
                    'brightness_score': 0.0,
                    'contrast_score': 0.0,
                    'sharpness_score': 0.0,
                    'color_balance_score': 0.0
                }
            
            # ë°ê¸° ë¶„ì„
            brightness_scores = [frame['brightness'] for frame in sample_frames]
            avg_brightness = np.mean(brightness_scores)
            brightness_score = min(1.0, max(0.0, (avg_brightness - 50) / 100))  # 50-150 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ëŒ€ë¹„ ë¶„ì„ (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
            contrast_scores = [np.std(frame['mean_color']) for frame in sample_frames]
            avg_contrast = np.mean(contrast_scores)
            contrast_score = min(1.0, max(0.0, avg_contrast / 50))  # 0-50 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ì„ ëª…ë„ ë¶„ì„ (ì—£ì§€ ë°€ë„ ê¸°ë°˜)
            sharpness_scores = [frame['edge_density'] for frame in sample_frames]
            avg_sharpness = np.mean(sharpness_scores)
            sharpness_score = min(1.0, max(0.0, avg_sharpness * 10))  # 0-0.1 ë²”ìœ„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # ìƒ‰ìƒ ê· í˜• ë¶„ì„
            color_balance_scores = []
            for frame in sample_frames:
                mean_color = frame['mean_color']
                # RGB ê°’ë“¤ì´ ê· í˜•ì¡í˜€ ìˆëŠ”ì§€ í™•ì¸
                balance = 1.0 - (np.std(mean_color) / np.mean(mean_color)) if np.mean(mean_color) > 0 else 0
                color_balance_scores.append(max(0, min(1, balance)))
            
            color_balance_score = np.mean(color_balance_scores)
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = (brightness_score + contrast_score + sharpness_score + color_balance_score) / 4
            
            # ìƒíƒœ ê²°ì •
            if overall_score >= 0.7:
                status = 'excellent'
            elif overall_score >= 0.5:
                status = 'good'
            elif overall_score >= 0.3:
                status = 'fair'
            else:
                status = 'poor'
            
            return {
                'overall_score': round(overall_score, 3),
                'status': status,
                'brightness_score': round(brightness_score, 3),
                'contrast_score': round(contrast_score, 3),
                'sharpness_score': round(sharpness_score, 3),
                'color_balance_score': round(color_balance_score, 3),
                'confidence_average': round(overall_score, 3)
            }
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'overall_score': 0.0,
                'status': 'unknown',
                'brightness_score': 0.0,
                'contrast_score': 0.0,
                'sharpness_score': 0.0,
                'color_balance_score': 0.0
            }
    
    def _analyze_scenes(self, sample_frames):
        """ì¥ë©´ ë¶„ì„"""
        try:
            if not sample_frames:
                return {
                    'scene_type_distribution': {},
                    'activity_level_distribution': {},
                    'lighting_distribution': {},
                    'diversity_score': 0.0
                }
            
            scene_types = []
            activity_levels = []
            lighting_conditions = []
            
            for frame in sample_frames:
                brightness = frame['brightness']
                edge_density = frame['edge_density']
                mean_color = frame['mean_color']
                
                # ì¥ë©´ íƒ€ì… ë¶„ë¥˜
                if edge_density > 0.05:
                    scene_types.append('detailed')
                elif edge_density > 0.02:
                    scene_types.append('medium')
                else:
                    scene_types.append('simple')
                
                # í™œë™ ìˆ˜ì¤€ ë¶„ë¥˜
                if edge_density > 0.04:
                    activity_levels.append('high')
                elif edge_density > 0.02:
                    activity_levels.append('medium')
                else:
                    activity_levels.append('low')
                
                # ì¡°ëª… ì¡°ê±´ ë¶„ë¥˜
                if brightness > 150:
                    lighting_conditions.append('bright')
                elif brightness > 100:
                    lighting_conditions.append('normal')
                else:
                    lighting_conditions.append('dark')
            
            # ë¶„í¬ ê³„ì‚°
            scene_type_dist = {}
            for scene_type in scene_types:
                scene_type_dist[scene_type] = scene_type_dist.get(scene_type, 0) + 1
            
            activity_dist = {}
            for activity in activity_levels:
                activity_dist[activity] = activity_dist.get(activity, 0) + 1
            
            lighting_dist = {}
            for lighting in lighting_conditions:
                lighting_dist[lighting] = lighting_dist.get(lighting, 0) + 1
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
            total_frames = len(sample_frames)
            diversity_score = len(set(scene_types)) / total_frames if total_frames > 0 else 0
            
            return {
                'scene_type_distribution': scene_type_dist,
                'activity_level_distribution': activity_dist,
                'lighting_distribution': lighting_dist,
                'diversity_score': round(diversity_score, 3)
            }
            
        except Exception as e:
            logger.error(f"ì¥ë©´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'scene_type_distribution': {},
                'activity_level_distribution': {},
                'lighting_distribution': {},
                'diversity_score': 0.0
            }
    
    def _format_frame_results(self, sample_frames, video_id):
        """í”„ë ˆì„ ê²°ê³¼ë¥¼ backend_videochat í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        try:
            frame_results = []
            
            for i, frame in enumerate(sample_frames):
                # í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥
                frame_image_path = self._save_frame_image(video_id, frame, i + 1)
                
                # ì‹¤ì œ YOLO ê°ì§€ ìˆ˜í–‰
                detected_persons = []
                if self.yolo_model and frame_image_path:
                    try:
                        # ì €ì¥ëœ í”„ë ˆì„ ì´ë¯¸ì§€ ë¡œë“œ
                        frame_image_full_path = os.path.join(settings.MEDIA_ROOT, frame_image_path)
                        if os.path.exists(frame_image_full_path):
                            frame_image = cv2.imread(frame_image_full_path)
                            if frame_image is not None:
                                detected_persons = self._detect_persons_with_yolo(frame_image)
                                logger.info(f"í”„ë ˆì„ {i+1}: YOLOë¡œ {len(detected_persons)}ëª… ê°ì§€")
                    except Exception as e:
                        logger.warning(f"í”„ë ˆì„ {i+1} YOLO ê°ì§€ ì‹¤íŒ¨: {e}")
                
                # YOLO ê°ì§€ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                if not detected_persons:
                    detected_persons = [
                        {
                            'class': 'person',
                            'bbox': [0.1, 0.1, 0.9, 0.9],  # ê¸°ë³¸ ë°”ìš´ë”© ë°•ìŠ¤
                            'confidence': 0.8,
                            'confidence_level': 0.25,
                            'attributes': {
                                'gender': {
                                    'value': 'person',
                                    'confidence': 0.7,
                                    'all_scores': {
                                        'a person': 0.7,
                                        'a man': 0.2,
                                        'a woman': 0.1
                                    },
                                    'top_3': [
                                        ['a person', 0.7],
                                        ['a man', 0.2],
                                        ['a woman', 0.1]
                                    ]
                                },
                                'age': {
                                    'value': 'adult',
                                    'confidence': 0.6,
                                    'all_scores': {
                                        'a child': 0.1,
                                        'a teenager': 0.2,
                                        'a young adult': 0.3,
                                        'a middle-aged person': 0.6,
                                        'an elderly person': 0.1
                                    },
                                    'top_3': [
                                        ['a middle-aged person', 0.6],
                                        ['a young adult', 0.3],
                                        ['a teenager', 0.2]
                                    ]
                                },
                                'detailed_clothing': {
                                    'value': 'wearing casual clothes',
                                    'confidence': 0.5,
                                    'all_scores': {
                                        'wearing casual clothes': 0.5,
                                        'wearing formal clothes': 0.3,
                                        'wearing sportswear': 0.2
                                    },
                                    'top_3': [
                                        ['wearing casual clothes', 0.5],
                                        ['wearing formal clothes', 0.3],
                                        ['wearing sportswear', 0.2]
                                    ]
                                }
                            }
                        }
                    ]
                
                # backend_videochat í˜•ì‹ì˜ í”„ë ˆì„ ê²°ê³¼ ìƒì„±
                frame_result = {
                    'image_id': i + 1,
                    'frame_id': i + 1,
                    'timestamp': frame['timestamp'],
                    'frame_image_path': frame_image_path,  # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
                    'dominant_colors': frame.get('dominant_colors', []),  # ìƒ‰ìƒ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    'persons': detected_persons,
                    'objects': [],
                    'scene_attributes': {
                        'scene_type': 'outdoor' if frame['brightness'] > 120 else 'indoor',
                        'lighting': 'bright' if frame['brightness'] > 150 else 'normal' if frame['brightness'] > 100 else 'dark',
                        'activity_level': 'high' if frame['edge_density'] > 0.04 else 'medium' if frame['edge_density'] > 0.02 else 'low'
                    }
                }
                frame_results.append(frame_result)
            
            return frame_results
            
        except Exception as e:
            logger.error(f"í”„ë ˆì„ ê²°ê³¼ í¬ë§· ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_key_insights(self, sample_frames, quality_analysis, scene_analysis):
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            insights = []
            
            if quality_analysis:
                status = quality_analysis.get('status', 'unknown')
                if status == 'excellent':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤")
                elif status == 'good':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
                elif status == 'fair':
                    insights.append("ì˜ìƒ í’ˆì§ˆì´ ë³´í†µì…ë‹ˆë‹¤")
                else:
                    insights.append("ì˜ìƒ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            if scene_analysis:
                scene_dist = scene_analysis.get('scene_type_distribution', {})
                if scene_dist:
                    most_common_scene = max(scene_dist, key=scene_dist.get)
                    insights.append(f"ì£¼ìš” ì¥ë©´ ìœ í˜•: {most_common_scene}")
                
                activity_dist = scene_analysis.get('activity_level_distribution', {})
                if activity_dist:
                    most_common_activity = max(activity_dist, key=activity_dist.get)
                    insights.append(f"ì£¼ìš” í™œë™ ìˆ˜ì¤€: {most_common_activity}")
            
            if sample_frames:
                avg_brightness = np.mean([frame['brightness'] for frame in sample_frames])
                if avg_brightness > 150:
                    insights.append("ë°ì€ ì˜ìƒì…ë‹ˆë‹¤")
                elif avg_brightness < 100:
                    insights.append("ì–´ë‘ìš´ ì˜ìƒì…ë‹ˆë‹¤")
                else:
                    insights.append("ì ì ˆí•œ ë°ê¸°ì˜ ì˜ìƒì…ë‹ˆë‹¤")
            
            return insights[:5]  # ìµœëŒ€ 5ê°œ ì¸ì‚¬ì´íŠ¸
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["ë¶„ì„ ì™„ë£Œ"]
    
    def _update_progress(self, video_id, progress, message):
        """ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        try:
            video = Video.objects.get(id=video_id)
            # Video ëª¨ë¸ì— ì§„í–‰ë¥  ì •ë³´ ì €ì¥
            video.analysis_progress = progress
            video.analysis_message = message
            video.save()
            logger.info(f"ğŸ“Š ë¶„ì„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {video_id} - {progress}% - {message}")
        except Exception as e:
            logger.error(f"ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _save_frame_image(self, video_id, frame_data, frame_number):
        """í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜ (backend_videochat ë°©ì‹)"""
        try:
            import cv2
            from PIL import Image
            import numpy as np
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            try:
                video = Video.objects.get(id=video_id)
                video_path = os.path.join(settings.MEDIA_ROOT, video.file_path)
            except Video.DoesNotExist:
                logger.error(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")
                return None
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
                return None
            
            # í•´ë‹¹ í”„ë ˆì„ìœ¼ë¡œ ì´ë™ (frame_dataì—ì„œ frame_index ì‚¬ìš©)
            frame_index = frame_data.get('frame_index', frame_number - 1)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if not ret:
                logger.error(f"âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {frame_index}")
                cap.release()
                return None
            
            # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ ì„¤ì •
            images_dir = os.path.join(settings.MEDIA_ROOT, 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            frame_filename = f"video{video_id}_frame{frame_number}.jpg"
            frame_path = os.path.join(images_dir, frame_filename)
            
            # ì´ë¯¸ì§€ ì €ì¥
            cv2.imwrite(frame_path, frame)
            cap.release()
            
            # ìƒëŒ€ ê²½ë¡œ ë°˜í™˜
            relative_path = f"images/{frame_filename}"
            logger.info(f"ğŸ“¸ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {relative_path}")
            return relative_path
            
        except Exception as e:
            logger.error(f"âŒ í”„ë ˆì„ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_analysis_to_json(self, analysis_result, video_id):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (backend_videochat í˜•ì‹)"""
        try:
            # analysis_results ë””ë ‰í† ë¦¬ ìƒì„±
            analysis_dir = os.path.join(settings.MEDIA_ROOT, 'analysis_results')
            os.makedirs(analysis_dir, exist_ok=True)
            
            # JSON íŒŒì¼ëª… ìƒì„± (backend_videochat ë°©ì‹)
            timestamp = int(time.time())
            json_filename = f"real_analysis_{video_id}_enhanced_{timestamp}.json"
            json_file_path = os.path.join(analysis_dir, json_filename)
            
            # TeletoVision_AI ìŠ¤íƒ€ì¼ë¡œ ì €ì¥
            detection_db_path, meta_db_path = self._save_teleto_vision_format(video_id, analysis_result)
            
            # ê¸°ì¡´ í˜•ì‹ë„ í•¨ê»˜ ì €ì¥ (í˜¸í™˜ì„±ì„ ìœ„í•´)
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=self._json_default)
            
            logger.info(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {json_file_path}")
            logger.info(f"ğŸ“„ Detection DB ì €ì¥ ì™„ë£Œ: {detection_db_path}")
            logger.info(f"ğŸ“„ Meta DB ì €ì¥ ì™„ë£Œ: {meta_db_path}")
            return f"analysis_results/{json_filename}"
            
        except Exception as e:
            logger.error(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_teleto_vision_format(self, video_id, analysis_result):
        """TeletoVision_AI ìŠ¤íƒ€ì¼ë¡œ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            video = Video.objects.get(id=video_id)
            video_name = video.original_name or video.filename
            
            # Detection DB êµ¬ì¡° ìƒì„±
            detection_db = self._create_detection_db(video_id, video_name, analysis_result)
            
            # Meta DB êµ¬ì¡° ìƒì„±
            meta_db = self._create_meta_db(video_id, video_name, analysis_result)
            
            # íŒŒì¼ ì €ì¥ ê²½ë¡œ
            detection_db_path = os.path.join(settings.MEDIA_ROOT, f"{video_name}-detection_db.json")
            meta_db_path = os.path.join(settings.MEDIA_ROOT, f"{video_name}-meta_db.json")
            
            # Detection DB ì €ì¥
            with open(detection_db_path, 'w', encoding='utf-8') as f:
                json.dump(detection_db, f, ensure_ascii=False, indent=2, default=self._json_default)
            
            # Meta DB ì €ì¥
            with open(meta_db_path, 'w', encoding='utf-8') as f:
                json.dump(meta_db, f, ensure_ascii=False, indent=2, default=self._json_default)
            
            return detection_db_path, meta_db_path
            
        except Exception as e:
            logger.error(f"âŒ TeletoVision í˜•ì‹ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _create_detection_db(self, video_id, video_name, analysis_result):
        """Detection DB êµ¬ì¡° ìƒì„±"""
        try:
            frame_results = analysis_result.get('frame_results', [])
            video_summary = analysis_result.get('video_summary', {})
            
            # ê¸°ë³¸ ì •ë³´
            detection_db = {
                "video_id": video_name,
                "fps": 30,  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•´ì•¼ í•¨
                "width": 1280,  # ê¸°ë³¸ê°’
                "height": 720,   # ê¸°ë³¸ê°’
                "frame": []
            }
            
            # í”„ë ˆì„ë³„ ê°ì²´ ì •ë³´ ìƒì„±
            for frame_data in frame_results:
                frame_info = {
                    "image_id": frame_data.get('frame_id') or frame_data.get('image_id', 1),
                    "timestamp": frame_data.get('timestamp', 0),
                    "objects": []
                }
                
                # ì‚¬ëŒ ê°ì²´ ì •ë³´
                persons = frame_data.get('persons', [])
                if persons:
                    person_object = {
                        "class": "person",
                        "num": len(persons),
                        "max_id": len(persons),
                        "tra_id": list(range(1, len(persons) + 1)),
                        "bbox": []
                    }
                    
                    for person in persons:
                        bbox_vals = person.get('bbox', [0, 0, 0, 0])
                        bbox = [float(v) for v in bbox_vals]
                        person_object["bbox"].append(bbox)
                    
                    frame_info["objects"].append(person_object)
                
                # ê¸°íƒ€ ê°ì²´ë“¤ (ìë™ì°¨, ì˜¤í† ë°”ì´ ë“±)
                objects = frame_data.get('objects', [])
                if objects:
                    for obj in objects:
                        class_name = obj.get('class') or obj.get('class_name', 'unknown')
                        bbox_vals = obj.get('bbox', [0, 0, 0, 0])
                        bbox = [float(v) for v in bbox_vals]
                        obj_info = {
                            "class": class_name,
                            "num": 1,
                            "max_id": 1,
                            "tra_id": [1],
                            "bbox": [bbox]
                        }
                        frame_info["objects"].append(obj_info)
                
                detection_db["frame"].append(frame_info)
            
            return detection_db
            
        except Exception as e:
            logger.error(f"âŒ Detection DB ìƒì„± ì‹¤íŒ¨: {e}")
            return {"video_id": video_name, "fps": 30, "width": 1280, "height": 720, "frame": []}
    
    def _create_meta_db(self, video_id, video_name, analysis_result):
        """Meta DB êµ¬ì¡° ìƒì„± (ìº¡ì…˜ í¬í•¨)"""
        try:
            frame_results = analysis_result.get('frame_results', [])
            video_summary = analysis_result.get('video_summary', {})
            
            # ê¸°ë³¸ ì •ë³´
            meta_db = {
                "video_id": video_name,
                "fps": 30,
                "width": 1280,
                "height": 720,
                "frame": []
            }
            
            # í”„ë ˆì„ë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
            for frame_data in frame_results:
                # ìº¡ì…˜ ìƒì„±
                caption = self._generate_frame_caption(frame_data)
                
                frame_meta = {
                    "image_id": frame_data.get('frame_id') or frame_data.get('image_id', 1),
                    "timestamp": frame_data.get('timestamp', 0),
                    "caption": caption,
                    "frame_image_path": frame_data.get('frame_image_path'),
                    "objects": []
                }
                
                # ì‚¬ëŒ ë©”íƒ€ë°ì´í„° (ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ í¬í•¨)
                persons = frame_data.get('persons', [])
                for i, person in enumerate(persons, 1):
                    # attributesì—ì„œ ì„±ë³„/ë‚˜ì´ ì¶”ì¶œ
                    gender_info = person.get('attributes', {}).get('gender', {})
                    age_info = person.get('attributes', {}).get('age', {})
                    
                    # ì˜ìƒ ìƒ‰ìƒ ì •ë³´
                    clothing_colors = person.get('clothing_colors', {})
                    
                    person_meta = {
                        "class": "person",
                        "id": i,
                        "bbox": person.get('bbox', [0, 0, 0, 0]),
                        "confidence": person.get('confidence', 0.0),
                        "clothing_colors": clothing_colors,  # ğŸ”¥ ê²€ìƒ‰ì„ ìœ„í•´ ìµœìƒìœ„ì— ì¶”ê°€
                        "analysis_source": person.get('analysis_source', 'unknown'),  # ğŸ”¥ ë¶„ì„ ì¶œì²˜
                        "attributes": {
                            "gender": gender_info.get('value', 'unknown'),
                            "age": age_info.get('value', 'unknown'),
                            "estimated_age": age_info.get('estimated_age', 0),
                            "emotion": person.get('attributes', {}).get('emotion', {}).get('value', 'neutral'),
                            "clothing": {
                                "upper_color": clothing_colors.get('upper', 'unknown'),
                                "lower_color": clothing_colors.get('lower', 'unknown'),
                                "dominant_color": clothing_colors.get('upper', 'unknown')  # í˜¸í™˜ì„±
                            },
                            "pose": person.get('pose', 'unknown')
                        },
                        "scene_context": {
                            "scene_type": frame_data.get('scene_attributes', {}).get('scene_type', 'unknown'),
                            "lighting": frame_data.get('scene_attributes', {}).get('lighting', 'unknown'),
                            "activity_level": frame_data.get('scene_attributes', {}).get('activity_level', 'unknown')
                        }
                    }
                    frame_meta["objects"].append(person_meta)
                
                # ê¸°íƒ€ ê°ì²´ ë©”íƒ€ë°ì´í„°
                objects = frame_data.get('objects', [])
                for obj in objects:
                    obj_meta = {
                        "class": obj.get('class_name', 'unknown'),
                        "id": 1,
                        "bbox": obj.get('bbox', [0, 0, 0, 0]),
                        "confidence": obj.get('confidence', 0.0),
                        "attributes": obj.get('attributes', {}),
                        "scene_context": {
                            "scene_type": frame_data.get('scene_attributes', {}).get('scene_type', 'unknown'),
                            "lighting": frame_data.get('scene_attributes', {}).get('lighting', 'unknown'),
                            "activity_level": frame_data.get('scene_attributes', {}).get('activity_level', 'unknown')
                        }
                    }
                    frame_meta["objects"].append(obj_meta)
                
                meta_db["frame"].append(frame_meta)
            
            return meta_db
            
        except Exception as e:
            logger.error(f"âŒ Meta DB ìƒì„± ì‹¤íŒ¨: {e}")
            return {"video_id": video_name, "fps": 30, "width": 1280, "height": 720, "frame": []}
    
    def _generate_frame_caption(self, frame_data):
        """AI ê¸°ë°˜ í”„ë ˆì„ ìº¡ì…˜ ìƒì„±"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            persons = frame_data.get('persons', [])
            objects = frame_data.get('objects', [])
            scene_attributes = frame_data.get('scene_attributes', {})
            timestamp = frame_data.get('timestamp', 0)
            
            # AI ìº¡ì…˜ ìƒì„± ì‹œë„
            ai_caption = self._generate_ai_caption(frame_data)
            if ai_caption and ai_caption != "ì¥ë©´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ":
                return ai_caption
            
            # AI ì‹¤íŒ¨ ì‹œ í´ë°±: ê·œì¹™ ê¸°ë°˜ ìº¡ì…˜
            return self._generate_rule_based_caption(frame_data)
            
        except Exception as e:
            logger.error(f"âŒ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì¥ë©´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
    
    def _generate_ai_caption(self, frame_data):
        """Vision-Language ëª¨ë¸ì„ ì‚¬ìš©í•œ ìº¡ì…˜ ìƒì„± (Ollama â†’ BLIP)"""
        try:
            # í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
            frame_image_path = frame_data.get('frame_image_path')
            if not frame_image_path:
                logger.warning("í”„ë ˆì„ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ì–´ì„œ Vision ìº¡ì…˜ ìƒì„± ë¶ˆê°€")
                return None
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ í™•ì¸
            full_image_path = os.path.join(settings.MEDIA_ROOT, frame_image_path)
            if not os.path.exists(full_image_path):
                logger.warning(f"í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {full_image_path}")
                return None
            
            # 1ìˆœìœ„: Ollama Vision ì‚¬ìš© (llava ëª¨ë¸)
            caption = self._generate_ollama_caption(full_image_path, frame_data)
            if caption:
                return caption
            
            # 2ìˆœìœ„: BLIP ëª¨ë¸ ì‚¬ìš© (ë¡œì»¬)
            caption = self._generate_blip_caption(full_image_path)
            if caption:
                return caption
            
            logger.warning("ëª¨ë“  Vision ëª¨ë¸ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨")
            return None
            
        except Exception as e:
            logger.error(f"âŒ Vision ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_ollama_caption(self, image_path, frame_data):
        """Ollama Vision ëª¨ë¸ì„ ì‚¬ìš©í•œ ìº¡ì…˜ ìƒì„± (llava)"""
        try:
            import ollama
            
            # í”„ë ˆì„ ì •ë³´ ì¶”ê°€
            timestamp = frame_data.get('timestamp', 0)
            persons = frame_data.get('persons', [])
            objects = frame_data.get('objects', [])
            dominant_colors = frame_data.get('dominant_colors', [])
            
            # ìƒ‰ìƒ ì •ë³´ í…ìŠ¤íŠ¸ ìƒì„±
            color_text = ""
            if dominant_colors:
                colors = [f"{c['color']}" for c in dominant_colors[:3]]
                color_text = f"ì£¼ìš” ìƒ‰ìƒ: {', '.join(colors)}"
            
            prompt = f"""Write EXACTLY 2-3 sentences describing this frame. IMPORTANT: Include specific gender and age information for each person visible.

Frame: {timestamp:.1f}s, {len(persons)} person(s)

Requirements:
- Describe each person's gender (man/woman/boy/girl) and approximate age (young/adult/elderly)
- Include clothing colors and actions
- Mention objects, setting, and atmosphere
- Be concise and specific

Example: "A bustling city sidewalk with 5 people walking. An elderly woman in a white jacket talks on her phone while a young man in green clothing and two adult women in blue and yellow jackets carry handbags. A teenage boy strolls past storefronts. Daytime with natural lighting, lively urban atmosphere."

Caption:"""
            
            # Ollamaë¡œ ì´ë¯¸ì§€ ë¶„ì„
            response = ollama.chat(
                model='llava:7b',  # ì´ë¯¸ì§€/PDF ì±„íŒ…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë™ì¼ ëª¨ë¸
                messages=[
                    {
                        'role': 'user',
                        'content': prompt,
                        'images': [image_path]  # ì´ë¯¸ì§€ ê²½ë¡œ ì§ì ‘ ì „ë‹¬
                    }
                ],
                options={
                    'temperature': 0.7,
                    'num_predict': 150,  # ê°„ê²°í•œ ìº¡ì…˜ (ì•½ 100-150 ë‹¨ì–´)
                    'num_ctx': 2048
                }
            )
            
            caption = response['message']['content'].strip()
            
            # ë„ˆë¬´ ê¸´ ìº¡ì…˜ì€ ìë¥´ê¸° (300ìë¡œ ì¦ê°€)
            if len(caption) > 300:
                caption = caption[:300] + "..."
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['ollama_calls'] += 1
            
            logger.info(f"âœ… Ollama ìº¡ì…˜ ìƒì„± ì„±ê³µ: {caption}")
            return caption
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_blip_caption(self, image_path):
        """ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ BLIP ìº¡ì…˜ ìƒì„± (self.blip_model ì‚¬ìš©)"""
        try:
            # BLIP ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if not self.blip_processor or not self.blip_model:
                logger.warning("BLIP ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return None
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')
            
            # ìº¡ì…˜ ìƒì„±
            inputs = self.blip_processor(image, return_tensors="pt")
            out = self.blip_model.generate(**inputs, max_length=50, num_beams=5)
            caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
            
            self.stats['blip_calls'] += 1
            
            # í•œêµ­ì–´ ë²ˆì—­
            korean_caption = self._translate_to_korean(caption)
            
            logger.info(f"âœ… BLIP ìº¡ì…˜: {korean_caption}")
            return korean_caption
            
        except Exception as e:
            logger.warning(f"BLIP ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _translate_to_korean(self, english_caption):
        """ê°„ë‹¨í•œ ì˜ì–´-í•œêµ­ì–´ ë²ˆì—­ (BLIP ê²°ê³¼ìš©)"""
        try:
            # ê¸°ë³¸ì ì¸ ë²ˆì—­ ë§¤í•‘
            translations = {
                "a person": "ì‚¬ëŒ",
                "a man": "ë‚¨ì„±",
                "a woman": "ì—¬ì„±",
                "a car": "ìë™ì°¨",
                "a building": "ê±´ë¬¼",
                "a street": "ë„ë¡œ",
                "a room": "ë°©",
                "a table": "í…Œì´ë¸”",
                "a chair": "ì˜ì",
                "a dog": "ê°œ",
                "a cat": "ê³ ì–‘ì´",
                "walking": "ê±·ê³  ìˆëŠ”",
                "sitting": "ì•‰ì•„ ìˆëŠ”",
                "standing": "ì„œ ìˆëŠ”",
                "talking": "ëŒ€í™”í•˜ëŠ”",
                "running": "ë›°ê³  ìˆëŠ”",
                "driving": "ìš´ì „í•˜ëŠ”",
                "outdoor": "ì•¼ì™¸",
                "indoor": "ì‹¤ë‚´",
                "daytime": "ë‚®",
                "night": "ë°¤",
                "bright": "ë°ì€",
                "dark": "ì–´ë‘ìš´"
            }
            
            korean_caption = english_caption.lower()
            for eng, kor in translations.items():
                korean_caption = korean_caption.replace(eng, kor)
            
            return korean_caption
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return english_caption
    
    def _format_frame_data_for_ai(self, frame_data):
        """AIìš© í”„ë ˆì„ ë°ì´í„° í¬ë§·íŒ…"""
        try:
            persons = frame_data.get('persons', [])
            objects = frame_data.get('objects', [])
            scene_attributes = frame_data.get('scene_attributes', {})
            timestamp = frame_data.get('timestamp', 0)
            
            description_parts = []
            
            # ì‹œê°„ ì •ë³´
            description_parts.append(f"ì‹œê°„: {timestamp:.1f}ì´ˆ")
            
            # ì¥ë©´ ì •ë³´
            scene_type = scene_attributes.get('scene_type', 'unknown')
            lighting = scene_attributes.get('lighting', 'unknown')
            activity_level = scene_attributes.get('activity_level', 'unknown')
            
            if scene_type != 'unknown':
                description_parts.append(f"ì¥ì†Œ: {scene_type}")
            if lighting != 'unknown':
                description_parts.append(f"ì¡°ëª…: {lighting}")
            if activity_level != 'unknown':
                description_parts.append(f"í™œë™ìˆ˜ì¤€: {activity_level}")
            
            # ì‚¬ëŒ ì •ë³´
            if persons:
                description_parts.append(f"ì¸ë¬¼: {len(persons)}ëª…")
                for i, person in enumerate(persons[:3], 1):
                    person_info = []
                    if person.get('gender') != 'unknown':
                        person_info.append(person['gender'])
                    if person.get('age') != 'unknown':
                        person_info.append(person['age'])
                    if person.get('clothing', {}).get('dominant_color') != 'unknown':
                        person_info.append(f"{person['clothing']['dominant_color']} ì˜·")
                    
                    if person_info:
                        description_parts.append(f"  - ì‚¬ëŒ{i}: {', '.join(person_info)}")
            
            # ê°ì²´ ì •ë³´
            if objects:
                object_names = [obj.get('class_name', 'unknown') for obj in objects]
                unique_objects = list(set([name for name in object_names if name != 'unknown']))
                if unique_objects:
                    description_parts.append(f"ê°ì²´: {', '.join(unique_objects[:5])}")
            
            return "\n".join(description_parts)
            
        except Exception as e:
            logger.error(f"âŒ í”„ë ˆì„ ë°ì´í„° í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return "ë°ì´í„° í¬ë§·íŒ… ì˜¤ë¥˜"
    
    def _generate_rule_based_caption(self, frame_data):
        """ê·œì¹™ ê¸°ë°˜ ìº¡ì…˜ ìƒì„± (í´ë°±)"""
        try:
            persons = frame_data.get('persons', [])
            objects = frame_data.get('objects', [])
            scene_attributes = frame_data.get('scene_attributes', {})
            timestamp = frame_data.get('timestamp', 0)
            
            caption_parts = []
            
            # ì‹œê°„ ì •ë³´
            caption_parts.append(f"ì‹œê°„ {timestamp:.1f}ì´ˆ")
            
            # ì¥ë©´ ì •ë³´
            scene_type = scene_attributes.get('scene_type', 'unknown')
            lighting = scene_attributes.get('lighting', 'unknown')
            activity_level = scene_attributes.get('activity_level', 'unknown')
            
            if scene_type == 'indoor':
                caption_parts.append("ì‹¤ë‚´")
            elif scene_type == 'outdoor':
                caption_parts.append("ì•¼ì™¸")
            
            if lighting == 'dark':
                caption_parts.append("ì–´ë‘ìš´ ì¡°ëª…")
            elif lighting == 'bright':
                caption_parts.append("ë°ì€ ì¡°ëª…")
            
            # ì‚¬ëŒ ì •ë³´
            if persons:
                person_count = len(persons)
                caption_parts.append(f"{person_count}ëª…ì˜ ì‚¬ëŒ")
                
                # ì£¼ìš” ì¸ë¬¼ íŠ¹ì„±
                if person_count <= 3:
                    for person in persons[:2]:
                        gender = person.get('gender', 'unknown')
                        age = person.get('age', 'unknown')
                        clothing = person.get('clothing', {})
                        color = clothing.get('dominant_color', 'unknown')
                        
                        if gender != 'unknown' and age != 'unknown':
                            caption_parts.append(f"{gender} {age}")
                        if color != 'unknown':
                            caption_parts.append(f"{color} ì˜·")
            
            # ê°ì²´ ì •ë³´
            if objects:
                object_names = [obj.get('class_name', 'unknown') for obj in objects]
                unique_objects = list(set(object_names))
                if unique_objects:
                    caption_parts.append(f"{', '.join(unique_objects[:3])} ë“±ì¥")
            
            # í™œë™ ìˆ˜ì¤€
            if activity_level == 'high':
                caption_parts.append("í™œë°œí•œ í™œë™")
            elif activity_level == 'low':
                caption_parts.append("ì¡°ìš©í•œ ì¥ë©´")
            
            return ", ".join(caption_parts) if caption_parts else "ì¼ë°˜ì ì¸ ì¥ë©´"
            
        except Exception as e:
            logger.error(f"âŒ ê·œì¹™ ê¸°ë°˜ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì¥ë©´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"

    def _extract_audio_summary(self, video_path):
        """Whisperë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ìš”ì•½ ì¶”ì¶œ"""
        try:
            import whisper
            import tempfile
            import os
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            model = whisper.load_model("base")
            
            # ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì „ì‚¬
            result = model.transcribe(video_path)
            
            # ì „ì‚¬ëœ í…ìŠ¤íŠ¸
            transcript = result["text"]
            
            # ì–¸ì–´ ê°ì§€
            language = result.get("language", "ko")
            
            logger.info(f"âœ… ì˜¤ë””ì˜¤ ì „ì‚¬ ì™„ë£Œ: {len(transcript)}ì, ì–¸ì–´: {language}")
            
            return {
                "transcript": transcript,
                "language": language,
                "segments": result.get("segments", []),
                "duration": result.get("duration", 0)
            }
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _generate_audio_summary(self, audio_data):
        """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ ìƒì„±"""
        try:
            if not audio_data or not audio_data.get("transcript"):
                return None
            
            transcript = audio_data["transcript"]
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            import re
            
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
            korean_words = re.findall(r'[ê°€-í£]+', transcript)
            word_freq = {}
            for word in korean_words:
                if len(word) > 1:  # 1ê¸€ì ë‹¨ì–´ ì œì™¸
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # ìƒìœ„ í‚¤ì›Œë“œ
            top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # ìš”ì•½ ìƒì„±
            summary = {
                "transcript": transcript,
                "language": audio_data.get("language", "ko"),
                "duration": audio_data.get("duration", 0),
                "top_keywords": [word for word, freq in top_keywords],
                "word_count": len(transcript.split()),
                "summary": f"ì£¼ìš” ë‚´ìš©: {', '.join([word for word, freq in top_keywords[:3]])}"
            }
            
            logger.info(f"âœ… ì˜¤ë””ì˜¤ ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary['summary']}")
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    @staticmethod
    def _json_default(obj):
        """numpy íƒ€ì… ë“±ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return str(obj)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
video_analysis_service = VideoAnalysisService()