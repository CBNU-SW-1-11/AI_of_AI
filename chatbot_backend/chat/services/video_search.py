"""
ì›¹ ê²€ì¦ ë° ê²€ìƒ‰ ì„œë¹„ìŠ¤
Wikipedia APIë¥¼ í†µí•œ ì‚¬ì‹¤ ê²€ì¦
"""
import re
import requests
from collections import Counter


def quick_web_verify(conflict_type, conflict_values, question):
    """ê°œì„ ëœ ì›¹ ê²€ì¦ (Wikipedia + Google Search) - ë²”ìš©ì """
    import requests
    import time
    import re
    
    try:
        print(f"ğŸŒ ì›¹ ê²€ì¦ ì‹œì‘: '{question}'")
        
        # 1ì°¨: Wikipedia API ê²€ìƒ‰ (ì§ˆë¬¸ ê¸°ë°˜)
        print("ğŸ” Wikipedia ê²€ìƒ‰ ì‹œë„...")
        wiki_result = search_wikipedia(question, [])
        if wiki_result.get("verified"):
            print(f"âœ… Wikipedia ê²€ì¦ ì„±ê³µ")
            return wiki_result
        
        # 2ì°¨: Google Search (ê°„ë‹¨í•œ ë°©ë²•)
        print("ğŸ” Google ê²€ìƒ‰ ì‹œë„...")
        google_result = search_google_simple(question, [])
        if google_result.get("verified"):
            print(f"âœ… Google ê²€ì¦ ì„±ê³µ")
            return google_result
        
        # ëª¨ë“  ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        print("âš ï¸ ëª¨ë“  ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨")
        return {"verified": False, "error": "ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ ì‹¤íŒ¨"}
                
    except Exception as e:
        print(f"âš ï¸ ì›¹ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {"verified": False, "error": str(e)}



def search_wikipedia(question, keywords):
    """Wikipedia APIë¥¼ í†µí•œ ìë™ ê²€ì¦ (í•˜ë“œì½”ë”© ì—†ìŒ)"""
    import requests
    import re
    
    try:
        # 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        search_terms = extract_search_terms_from_question(question)
        
        if not search_terms:
            print("âš ï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            return {"verified": False, "error": "ê²€ìƒ‰ í‚¤ì›Œë“œ ì—†ìŒ"}
        
        print(f"ğŸ” Wikipedia ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_terms}")
        
        # 2ë‹¨ê³„: ê° ê²€ìƒ‰ì–´ë¡œ Wikipedia ê²€ìƒ‰ ì‹œë„
        for term in search_terms[:3]:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ ì‹œë„
            # í•œê¸€ Wikipedia ê²€ìƒ‰
            wiki_results = search_wikipedia_api(term, 'ko')
            
            if wiki_results.get("verified"):
                return wiki_results
            
            # ì‹¤íŒ¨ ì‹œ ì˜ì–´ Wikipedia ê²€ìƒ‰
            wiki_results_en = search_wikipedia_api(term, 'en')
            
            if wiki_results_en.get("verified"):
                return wiki_results_en
        
        print("âš ï¸ ëª¨ë“  Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨")
        return {"verified": False, "error": "Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨"}
        
    except Exception as e:
        print(f"âš ï¸ Wikipedia ê²€ì¦ ì˜¤ë¥˜: {e}")
        return {"verified": False, "error": f"Wikipedia ì˜¤ë¥˜: {e}"}



def extract_search_terms_from_question(question):
    """ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (ë²”ìš©ì )"""
    import re
    
    keywords = []
    
    # 1. ì¼ë°˜ì ì¸ ëª…ì‚¬ íŒ¨í„´ (í•˜ë“œì½”ë”© ì—†ì´)
    # í•œêµ­ì–´ ëª…ì‚¬ íŒ¨í„´ (2ê¸€ì ì´ìƒ)
    korean_nouns = re.findall(r'[ê°€-í£]{2,}', question)
    keywords.extend(korean_nouns)
    
    # ì˜ì–´ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ (ê³ ìœ ëª…ì‚¬)
    english_proper_nouns = re.findall(r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)*', question)
    keywords.extend(english_proper_nouns)
    
    # ìˆ«ìì™€ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ (ì—°ë„, ìˆ˜ì¹˜ ë“±)
    number_words = re.findall(r'\d{4}ë…„?|\d+ëª…?|\d+ê°œ?', question)
    keywords.extend(number_words)
    
    # íŠ¹ìˆ˜ íŒ¨í„´ë“¤ (ë²”ìš©ì )
    special_patterns = [
        r'([ê°€-í£]+ëŒ€í•™êµ?)',  # ëŒ€í•™êµ
        r'([ê°€-í£]+ëŒ€í•™?)',    # ëŒ€í•™
        r'([ê°€-í£]+íšŒì‚¬?)',    # íšŒì‚¬
        r'([ê°€-í£]+ì •ë¶€?)',    # ì •ë¶€
        r'([ê°€-í£]+ì‚¬ê±´?)',    # ì‚¬ê±´
        r'([ê°€-í£]+ì „ìŸ?)',    # ì „ìŸ
        r'([ê°€-í£]+í˜ëª…?)',    # í˜ëª…
        r'([ê°€-í£]+ì˜¬ë¦¼í”½?)',  # ì˜¬ë¦¼í”½
    ]
    
    for pattern in special_patterns:
        matches = re.findall(pattern, question)
        keywords.extend(matches)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_keywords = []
    for kw in keywords:
        if kw and kw not in unique_keywords and len(kw.strip()) > 1:
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ ì œì™¸
            common_words = ['ì„¤ëª…', 'ëŒ€í•´', 'ì•Œë ¤', 'ì¤˜', 'í•´ì¤˜', 'ì–´ë–¤', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””', 'ì™œ', 'ì–´ë–»ê²Œ']
            if kw.strip() not in common_words:
                unique_keywords.append(kw.strip())
    
    # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ë°˜í™˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ê²€ìƒ‰ì´ ë¹„íš¨ìœ¨ì )
    print(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {unique_keywords[:3]}")
    return unique_keywords[:3]



def search_wikipedia_api(search_term, lang='ko'):
    """Wikipedia API ì‹¤ì œ ê²€ìƒ‰"""
    import requests
    import re
    from collections import Counter
    
    try:
        # User-Agent í—¤ë” ì¶”ê°€ (Wikipedia API ìš”êµ¬ì‚¬í•­)
        headers = {
            'User-Agent': 'AI_of_AI_ChatBot/1.0 (Educational Project)'
        }
        
        # Wikipedia Search APIë¡œ í˜ì´ì§€ ì°¾ê¸°
        search_url = f"https://{lang}.wikipedia.org/w/api.php"
        search_params = {
            'action': 'opensearch',
            'search': search_term,
            'limit': 1,
            'namespace': 0,
            'format': 'json'
        }
        
        response = requests.get(search_url, params=search_params, headers=headers, timeout=5)
        
        if response.status_code != 200:
            return {"verified": False, "error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}"}
        
        search_results = response.json()
        
        if not search_results or len(search_results) < 2 or not search_results[1]:
            print(f"âš ï¸ '{search_term}' Wikipedia í˜ì´ì§€ ì—†ìŒ")
            return {"verified": False, "error": "í˜ì´ì§€ ì—†ìŒ"}
        
        page_title = search_results[1][0]
        print(f"ğŸ“„ Wikipedia í˜ì´ì§€ ë°œê²¬: {page_title}")
        
        # í˜ì´ì§€ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        summary_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{page_title}"
        summary_response = requests.get(summary_url, headers=headers, timeout=5)
        
        if summary_response.status_code == 200:
            data = summary_response.json()
            extract = data.get('extract', '')
            
            if extract and len(extract) > 20:
                print(f"âœ… Wikipedia ìš”ì•½: {extract[:100]}...")
                
                # ëª¨ë“  ì •ë³´ ì¶”ì¶œ (ì—°ë„, ìœ„ì¹˜, ê¸°íƒ€ ì •ë³´)
                extracted_info = {
                    "verified": True,
                    "source": f"Wikipedia ({lang})",
                    "abstract": extract[:400] + "..." if len(extract) > 400 else extract,
                    "full_text": extract,  # ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥
                    "confidence": 0.95,
                    "page_title": page_title
                }
                
                # ì—°ë„ íŒ¨í„´ ì¶”ì¶œ (ì„¤ë¦½, ì°½ë¦½, ê°œêµ ë“±)
                years = re.findall(r'(\d{4})', extract)
                valid_years = [year for year in years if 1900 <= int(year) <= 2024]
                
                if valid_years:
                    # ì„¤ë¦½/ê°œêµ ê´€ë ¨ ì—°ë„ ìš°ì„  ì¶”ì¶œ
                    founding_patterns = [
                        r'(\d{4})ë…„[^\d]*(?:ì„¤ë¦½|ì°½ë¦½|ê°œêµ|ëŒ€í•™.*ì„¤ë¦½|ëŒ€í•™êµ.*ì„¤ë¦½|ì„¤ë¦½.*ëŒ€í•™)',
                        r'(?:ì„¤ë¦½|ì°½ë¦½|ê°œêµ)[^\d]*(\d{4})ë…„',
                        r'(\d{4})ë…„.*(?:ì¶œë²”|íƒ„ìƒ|ìƒì„±)'
                    ]
                    
                    # ê° íŒ¨í„´ì—ì„œ ê°€ì¥ ë¨¼ì € ë§¤ì¹˜ë˜ëŠ” ì—°ë„ ì°¾ê¸° (ìœ„ì¹˜ ê¸°ì¤€)
                    first_matches = []
                    for pattern in founding_patterns:
                        match = re.search(pattern, extract, re.IGNORECASE)
                        if match:
                            matched_year = match.group(1)
                            if matched_year in valid_years:
                                position = match.start()
                                first_matches.append((position, matched_year))
                    
                    if first_matches:
                        # ìœ„ì¹˜ê°€ ê°€ì¥ ì•ì„  ì—°ë„ ì„ íƒ
                        first_matches.sort()
                        most_common_year = first_matches[0][1]
                    else:
                        # ì„¤ë¦½ ì—°ë„ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê°€ì¥ ìì£¼ ì–¸ê¸‰ëœ ì—°ë„
                        year_counts = Counter(valid_years)
                        most_common_year = year_counts.most_common(1)[0][0]
                    
                    extracted_info["extracted_year"] = most_common_year
                    print(f"ğŸ“… ì¶”ì¶œëœ ì—°ë„: {most_common_year}ë…„")
                
                # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (ì‹œ, ë„, êµ¬ ë“±)
                location_patterns = [
                    r'([ê°€-í£]+íŠ¹ë³„ì‹œ|[ê°€-í£]+ê´‘ì—­ì‹œ|[ê°€-í£]+ì‹œ)\s+([ê°€-í£]+êµ¬|[ê°€-í£]+êµ°)',
                    r'([ê°€-í£]+íŠ¹ë³„ì‹œ|[ê°€-í£]+ê´‘ì—­ì‹œ|[ê°€-í£]+ì‹œ)',
                    r'([ê°€-í£]+ë„)\s+([ê°€-í£]+ì‹œ)',
                ]
                
                for pattern in location_patterns:
                    location_matches = re.findall(pattern, extract)
                    if location_matches:
                        if isinstance(location_matches[0], tuple):
                            location = ' '.join(location_matches[0])
                        else:
                            location = location_matches[0]
                        extracted_info["location"] = location
                        print(f"ğŸ“ ì¶”ì¶œëœ ìœ„ì¹˜: {location}")
                        break
                
                # êµ­ë¦½/ì‚¬ë¦½/ê³µë¦½ ì •ë³´ ì¶”ì¶œ
                if 'êµ­ë¦½' in extract:
                    extracted_info["type"] = "êµ­ë¦½"
                    print(f"ğŸ›ï¸ ìœ í˜•: êµ­ë¦½")
                elif 'ì‚¬ë¦½' in extract:
                    extracted_info["type"] = "ì‚¬ë¦½"
                    print(f"ğŸ›ï¸ ìœ í˜•: ì‚¬ë¦½")
                
                # ì—°ë„ê°€ ì—†ìœ¼ë©´ ë³¸ë¬¸ì—ì„œ ì¶”ê°€ ê²€ìƒ‰
                if not extracted_info.get("extracted_year"):
                    print("âš ï¸ ìš”ì•½ì— ì—°ë„ ì—†ìŒ, ë³¸ë¬¸ APIë¡œ fallback...")
                    full_text_result = get_wikipedia_full_text(page_title, lang, headers)
                    if full_text_result.get("verified") and full_text_result.get("extracted_year"):
                        extracted_info["extracted_year"] = full_text_result["extracted_year"]
                        print(f"ğŸ“… ë³¸ë¬¸ì—ì„œ ì¶”ì¶œëœ ì„¤ë¦½ì—°ë„: {full_text_result['extracted_year']}ë…„")
                
                return extracted_info
        
        return {"verified": False, "error": "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"}
        
    except Exception as e:
        return {"verified": False, "error": f"API ì˜¤ë¥˜: {e}"}



def get_wikipedia_full_text(page_title, lang, headers):
    """Wikipedia ë³¸ë¬¸ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
    import requests
    import re
    from collections import Counter
    
    try:
        # Wikipedia Parse APIë¡œ ë³¸ë¬¸ ì¼ë¶€ ê°€ì ¸ì˜¤ê¸°
        parse_url = f"https://{lang}.wikipedia.org/w/api.php"
        parse_params = {
            'action': 'query',
            'prop': 'extracts',
            'exintro': True,  # ì„œë¡ ë§Œ ê°€ì ¸ì˜¤ê¸°
            'explaintext': True,  # ìˆœìˆ˜ í…ìŠ¤íŠ¸
            'titles': page_title,
            'format': 'json'
        }
        
        response = requests.get(parse_url, params=parse_params, headers=headers, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            
            if pages:
                page = list(pages.values())[0]
                full_text = page.get('extract', '')
                
                if full_text and len(full_text) > 50:
                    print(f"ğŸ“„ Wikipedia ë³¸ë¬¸: {full_text[:150]}...")
                    
                    # ì—°ë„ íŒ¨í„´ ì¶”ì¶œ (ì„¤ë¦½/ê°œêµ ê´€ë ¨ ì—°ë„ ìš°ì„ )
                    years = re.findall(r'(\d{4})', full_text)
                    valid_years = [year for year in years if 1900 <= int(year) <= 2024]
                    
                    if valid_years:
                        # ì„¤ë¦½/ê°œêµ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì¥ì—ì„œ ì—°ë„ ìš°ì„  ì¶”ì¶œ
                        founding_patterns = [
                            r'(\d{4})ë…„[^\d]*(?:ì„¤ë¦½|ì°½ë¦½|ê°œêµ|ëŒ€í•™.*ì„¤ë¦½|ëŒ€í•™êµ.*ì„¤ë¦½|ì„¤ë¦½.*ëŒ€í•™)',
                            r'(?:ì„¤ë¦½|ì°½ë¦½|ê°œêµ)[^\d]*(\d{4})ë…„',
                            r'(\d{4})ë…„.*(?:ì¶œë²”|íƒ„ìƒ|ìƒì„±)'
                        ]
                        # ê° íŒ¨í„´ì—ì„œ ê°€ì¥ ë¨¼ì € ë§¤ì¹˜ë˜ëŠ” ì—°ë„ ì°¾ê¸° (ìœ„ì¹˜ ê¸°ì¤€)
                        first_matches = []
                        for pattern in founding_patterns:
                            match = re.search(pattern, full_text, re.IGNORECASE)
                            if match:
                                matched_year = match.group(1)
                                if matched_year in valid_years:
                                    position = match.start()
                                    first_matches.append((position, matched_year))
                        
                        if first_matches:
                            # ìœ„ì¹˜ê°€ ê°€ì¥ ì•ì„  ì—°ë„ ì„ íƒ (ì›ë˜ ì„¤ë¦½ ì—°ë„ ìš°ì„ )
                            first_matches.sort()  # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
                            most_common_year = first_matches[0][1]
                        else:
                            # ì—†ìœ¼ë©´ ê°€ì¥ ìì£¼ ì–¸ê¸‰ëœ ì—°ë„ ì„ íƒ
                            year_counts = Counter(valid_years)
                            most_common_year = year_counts.most_common(1)[0][0]
                        
                        return {
                            "verified": True,
                            "source": f"Wikipedia Full Text ({lang})",
                            "extracted_year": most_common_year,
                            "abstract": full_text[:200] + "..." if len(full_text) > 200 else full_text,
                            "confidence": 0.85,
                            "page_title": page_title
                        }
        
        return {"verified": False, "error": "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"}
        
    except Exception as e:
        return {"verified": False, "error": f"ë³¸ë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"}



def search_google_simple(question, keywords):
    """ëŒ€ì²´ ê²€ìƒ‰ ë°©ë²• (Wikipedia ì‹¤íŒ¨ ì‹œ)"""
    # Wikipedia APIê°€ ì‹¤íŒ¨í•œ ê²½ìš° ë‹¤ë¥¸ ê³µê°œ API ì‹œë„ ê°€ëŠ¥
    # í˜„ì¬ëŠ” Wikipediaì—ë§Œ ì˜ì¡´
    return {"verified": False, "error": "Wikipedia ì™¸ ê²€ìƒ‰ ë¯¸êµ¬í˜„"}

