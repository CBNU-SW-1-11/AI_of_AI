"""
ìµœì  ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤
ì‹¬íŒ ëª¨ë¸ì„ í†µí•œ ë‹¤ì¤‘ LLM ì‘ë‹µ ê²€ì¦ ë° ìµœì  ë‹µë³€ ìƒì„±
"""
import os
import re
import json
import asyncio
import aiohttp
import openai
from collections import defaultdict
from difflib import SequenceMatcher

# ë¡œì»¬ imports
from ..utils.error_handlers import get_user_friendly_error_message
from ..utils.ai_utils import get_openai_completion_limit
from .verification_sources import get_best_verification_source


def detect_question_type_from_content(content):
    """ì§ˆë¬¸ ë‚´ìš©ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ìœ í˜• ê°ì§€: code, image, document, creative, general"""
    
    content_lower = content.lower()
    
    # ì½”ë“œ ê´€ë ¨ í‚¤ì›Œë“œ
    code_keywords = ['ì½”ë“œ', 'code', 'í•¨ìˆ˜', 'function', 'í”„ë¡œê·¸ë˜ë°', 'programming', 'ì•Œê³ ë¦¬ì¦˜', 'algorithm', 
                     'êµ¬í˜„', 'implement', 'ì‘ì„±', 'write', 'ê°œë°œ', 'develop', 'ìŠ¤í¬ë¦½íŠ¸', 'script',
                     'íŒŒì´ì¬', 'python', 'ìë°”', 'java', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'javascript', 'c++', 'c#']
    
    # ì´ë¯¸ì§€ ê´€ë ¨ í‚¤ì›Œë“œ
    image_keywords = ['ì´ë¯¸ì§€', 'image', 'ì‚¬ì§„', 'photo', 'ê·¸ë¦¼', 'picture', 'ì‹œê°', 'visual', 'í™”ë©´']
    
    # ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ
    document_keywords = ['ë¬¸ì„œ', 'document', 'pdf', 'íŒŒì¼', 'file', 'ìš”ì•½', 'summary', 'ë‚´ìš©', 'content']
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ í‚¤ì›Œë“œ
    creative_keywords = ['ê¸€ì“°ê¸°', 'writing', 'ì°½ì‘', 'creative', 'ì†Œì„¤', 'novel', 'ì‹œ', 'poem', 'ì—ì„¸ì´', 'essay',
                        'ì´ì•¼ê¸°', 'story', 'ë‚´ìš© ì‘ì„±', 'write content', 'ë¬¸ì¥', 'sentence']
    
    # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in code_keywords):
        code_patterns = [
            r'ì½”ë“œ.*ì‘ì„±|ì‘ì„±.*ì½”ë“œ',
            r'í•¨ìˆ˜.*ë§Œë“¤|ë§Œë“¤.*í•¨ìˆ˜',
            r'êµ¬í˜„.*í•´|í•´.*êµ¬í˜„',
            r'ì½”ë“œ.*ë³´ì—¬|ë³´ì—¬.*ì½”ë“œ',
            r'í”„ë¡œê·¸ë¨.*ì‘ì„±|ì‘ì„±.*í”„ë¡œê·¸ë¨',
            r'íŒŒì´ì¬.*ì½”ë“œ|ì½”ë“œ.*íŒŒì´ì¬',
            r'ì•Œê³ ë¦¬ì¦˜.*êµ¬í˜„|êµ¬í˜„.*ì•Œê³ ë¦¬ì¦˜'
        ]
        if any(re.search(pattern, content_lower) for pattern in code_patterns):
            return 'code'
    
    # ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in image_keywords):
        image_patterns = [
            r'ì´ë¯¸ì§€.*ë¶„ì„|ë¶„ì„.*ì´ë¯¸ì§€',
            r'ì‚¬ì§„.*ì„¤ëª…|ì„¤ëª….*ì‚¬ì§„',
            r'ê·¸ë¦¼.*ë­|ë­.*ê·¸ë¦¼',
            r'ì´ë¯¸ì§€.*ë­|ë­.*ì´ë¯¸ì§€'
        ]
        if any(re.search(pattern, content_lower) for pattern in image_patterns):
            return 'image'
    
    # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in document_keywords):
        document_patterns = [
            r'ë¬¸ì„œ.*ë¶„ì„|ë¶„ì„.*ë¬¸ì„œ',
            r'íŒŒì¼.*ë‚´ìš©|ë‚´ìš©.*íŒŒì¼',
            r'pdf.*ìš”ì•½|ìš”ì•½.*pdf',
            r'ë¬¸ì„œ.*ìš”ì•½|ìš”ì•½.*ë¬¸ì„œ'
        ]
        if any(re.search(pattern, content_lower) for pattern in document_patterns):
            return 'document'
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in creative_keywords):
        creative_patterns = [
            r'ê¸€.*ì“°|ì“°.*ê¸€',
            r'ì†Œì„¤.*ì‘ì„±|ì‘ì„±.*ì†Œì„¤',
            r'ì‹œ.*ì‘ì„±|ì‘ì„±.*ì‹œ',
            r'ì´ì•¼ê¸°.*ë§Œë“¤|ë§Œë“¤.*ì´ì•¼ê¸°',
            r'ì°½ì‘.*í•´|í•´.*ì°½ì‘',
            r'ì—ì„¸ì´.*ì‘ì„±|ì‘ì„±.*ì—ì„¸ì´'
        ]
        if any(re.search(pattern, content_lower) for pattern in creative_patterns):
            return 'creative'
    
    # ê¸°ë³¸ê°’: ì¼ë°˜ ì§ˆë¬¸
    return 'general'


def classify_question_type(question):
    """ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜ ë° ê²€ì¦ í‚¤ì›Œë“œ ì¶”ì¶œ: ì‚¬ì‹¤(Factual) vs ì˜ê²¬(Opinion)"""
    try:
        
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        classification_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , ì‚¬ì‹¤ì  ì§ˆë¬¸ì¸ ê²½ìš° ê²€ì¦ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë¶„ë¥˜ ê¸°ì¤€:
- ì‚¬ì‹¤ì  ì§ˆë¬¸: ê°ê´€ì  ì‚¬ì‹¤, ì •í™•í•œ ë‹µì´ ì¡´ì¬ (ì˜ˆ: ì„¤ë¦½ì—°ë„, ìœ„ì¹˜, ì—­ì‚¬ì  ì‚¬ì‹¤, ëŒ€í†µë ¹ ì´ë¦„)
- ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸: ì£¼ê´€ì  í‰ê°€, ì¶”ì²œ, ì„ í˜¸ë„ (ì˜ˆ: ë§›ì§‘ ì¶”ì²œ, ì¢‹ì€ ì¹´í˜, ìµœê³ ì˜ ì œí’ˆ)
- ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸: ì½”ë“œ ì‘ì„±, í”„ë¡œê·¸ë˜ë° ì˜ˆì œ, ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ìš”ì²­

ê²€ì¦ í‚¤ì›Œë“œ ì¶”ì¶œ ê·œì¹™ (ì‚¬ì‹¤ì  ì§ˆë¬¸ì¸ ê²½ìš°ë§Œ):
- ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª…ì‚¬ ì¶”ì¶œ
- ì„¤ëª… ìš”ì²­ í‘œí˜„("ì— ëŒ€í•´", "ì„¤ëª…í•´ì¤˜", "ì•Œë ¤ì¤˜" ë“±) ì œì™¸
- ì¡°ì‚¬("ì€", "ëŠ”", "ì´", "ê°€" ë“±) ì œì™¸
- ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ)

ì˜ˆì‹œ:
- "ì¶©ë¶ëŒ€ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜" â†’ keywords: ["ì¶©ë¶ëŒ€"]
- "ëŒ€í•œë¯¼êµ­ 11ëŒ€ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì•¼?" â†’ keywords: ["ëŒ€í•œë¯¼êµ­", "11ëŒ€", "ëŒ€í†µë ¹"]
- "ì„œìš¸ì˜ ì¸êµ¬ëŠ”?" â†’ keywords: ["ì„œìš¸", "ì¸êµ¬"]

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "type": "factual" ë˜ëŠ” "opinion" ë˜ëŠ” "code",
  "confidence": 0.0-1.0,
  "reason": "ë¶„ë¥˜ ì´ìœ ",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"] (ì‚¬ì‹¤ì  ì§ˆë¬¸ì¸ ê²½ìš°ë§Œ, ë¹ˆ ë°°ì—´ ê°€ëŠ¥)
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": classification_prompt}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        question_type = result.get('type', 'factual')
        keywords = result.get('keywords', [])
        
        print(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: {question_type} (ì‹ ë¢°ë„: {result.get('confidence', 0):.2f})")
        print(f"   ì´ìœ : {result.get('reason', '')}")
        if keywords:
            print(f"   ì¶”ì¶œëœ ê²€ì¦ í‚¤ì›Œë“œ: {keywords}")
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´)
        return {
            'type': question_type,
            'keywords': keywords,
            'confidence': result.get('confidence', 0.9),
            'reason': result.get('reason', '')
        }
        
    except Exception as e:
        print(f"âš ï¸ ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 'factual' ì‚¬ìš©")
        return {
            'type': 'factual',
            'keywords': [],
            'confidence': 0.5,
            'reason': 'ë¶„ë¥˜ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ê°’'
        }


def get_premium_models_to_call(currently_used_models):
    """ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì„ ì œì™¸í•œ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
    
    Args:
        currently_used_models: í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['GPT-4o', 'Gemini-2.0-Flash-Lite'])
    
    Returns:
        ì¶”ê°€ë¡œ í˜¸ì¶œí•  í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    """
    # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì •ì˜ (ìµœìƒìœ„ ëª¨ë¸ë“¤)
    premium_models = ['GPT-5', 'Gemini-2.5-Pro', 'Claude-3.7-Sonnet']
    
    # ëª¨ë¸ëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, í•˜ì´í”ˆ ë“± í†µì¼)
    def normalize_model_name(name):
        return name.lower().replace('-', '').replace('.', '').replace('_', '')
    
    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ê·œí™”
    used_normalized = {normalize_model_name(model) for model in currently_used_models}
    
    # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ í•„í„°ë§
    models_to_call = []
    for premium in premium_models:
        if normalize_model_name(premium) not in used_normalized:
            models_to_call.append(premium)
    
    print(f"ğŸ¯ ì¶”ê°€ í˜¸ì¶œí•  í”„ë¦¬ë¯¸ì—„ ëª¨ë¸: {models_to_call}")
    return models_to_call


async def call_additional_premium_models(user_message, premium_models, session_id=None):
    """í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ë“¤ì„ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œ
    
    Args:
        user_message: ì‚¬ìš©ì ì§ˆë¬¸
        premium_models: í˜¸ì¶œí•  í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        session_id: ì„¸ì…˜ ID
    
    Returns:
        {ëª¨ë¸ëª…: ì‘ë‹µ} ë”•ì…”ë„ˆë¦¬
    """
    
    # ì—”ë“œí¬ì¸íŠ¸ ë§¤í•‘
    all_llm_endpoints = {
        'GPT-5': 'http://localhost:8000/chat/gpt-5/',
        'GPT-5-Mini': 'http://localhost:8000/chat/gpt-5-mini/',
        'GPT-4.1': 'http://localhost:8000/chat/gpt-4.1/',
        'GPT-4.1-Mini': 'http://localhost:8000/chat/gpt-4.1-mini/',
        'GPT-4o': 'http://localhost:8000/chat/gpt-4o/',
        'GPT-4o-Mini': 'http://localhost:8000/chat/gpt-4o-mini/',
        'GPT-4-Turbo': 'http://localhost:8000/chat/gpt-4-turbo/',
        'GPT-3.5-Turbo': 'http://localhost:8000/chat/gpt-3.5-turbo/',
        'Gemini-2.5-Pro': 'http://localhost:8000/chat/gemini-2.5-pro/',
        'Gemini-2.5-Flash': 'http://localhost:8000/chat/gemini-2.5-flash/',
        'Gemini-2.0-Flash-Exp': 'http://localhost:8000/chat/gemini-2.0-flash-exp/',
        'Gemini-2.0-Flash-Lite': 'http://localhost:8000/chat/gemini-2.0-flash-lite/',
        'Claude-4-Opus': 'http://localhost:8000/chat/claude-4-opus/',
        'Claude-3.7-Sonnet': 'http://localhost:8000/chat/claude-3.7-sonnet/',
        'Claude-3.5-Sonnet': 'http://localhost:8000/chat/claude-3.5-sonnet/',
        'Claude-3.5-Haiku': 'http://localhost:8000/chat/claude-3.5-haiku/',
        'Claude-3-Opus': 'http://localhost:8000/chat/claude-3-opus/',
        'HCX-003': 'http://localhost:8000/chat/clova-hcx-003/',
        'HCX-DASH-001': 'http://localhost:8000/chat/clova-hcx-dash-001/',
    }
    
    responses = {}
    
    async def fetch_response(session, ai_name, endpoint):
        try:
            payload = {'message': user_message, 'user_id': session_id or 'system'}
            async with session.post(endpoint, json=payload, timeout=60) as response:
                if response.status == 200:
                    result = await response.json()
                    response_content = result.get('response', 'ì‘ë‹µ ì—†ìŒ')
                    print(f"âœ… [ì¶”ê°€] {ai_name} ì‘ë‹µ ìˆ˜ì‹ : {len(str(response_content))}ì")
                    return ai_name, response_content
                else:
                    error_text = await response.text()
                    friendly_msg = get_user_friendly_error_message(Exception(f"HTTP {response.status}"))
                    return ai_name, friendly_msg
        except Exception as e:
            friendly_msg = get_user_friendly_error_message(e)
            return ai_name, friendly_msg
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for model in premium_models:
            if model in all_llm_endpoints:
                endpoint = all_llm_endpoints[model]
                tasks.append(fetch_response(session, model, endpoint))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, tuple):
                ai_name, response = result
                responses[ai_name] = response
    
    # ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§
    error_patterns = [
        "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€", "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼", "ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œ",
        "API ì¸ì¦ì— ì‹¤íŒ¨", "ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼", "ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    ]
    
    valid_responses = {}
    for ai_name, response in responses.items():
        response_str = str(response)
        is_error = any(pattern in response_str for pattern in error_patterns)
        if len(response_str) < 50 and any(kw in response_str.lower() for kw in ["timeout", "error", "ì˜¤ë¥˜"]):
            is_error = True
        
        if not is_error:
            valid_responses[ai_name] = response
    
    print(f"ğŸ“Š [ì¶”ê°€] ìœ íš¨í•œ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‘ë‹µ: {len(valid_responses)}ê°œ")
    return valid_responses


def apply_voting_system(all_responses, user_question):
    """ë³´íŒ… ì‹œìŠ¤í…œ ì ìš©: ê°€ì¥ ë§ì€ ëª¨ë¸ì´ ë™ì˜í•˜ëŠ” ë‹µë³€ ì„ íƒ
    
    Args:
        all_responses: {ëª¨ë¸ëª…: ì‘ë‹µ} ë”•ì…”ë„ˆë¦¬
        user_question: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        ìµœì  ë‹µë³€ ê²°ê³¼
    """
    print(f"\nğŸ—³ï¸ ë³´íŒ… ì‹œìŠ¤í…œ ì ìš© ì‹œì‘")
    print(f"   ì°¸ì—¬ ëª¨ë¸: {list(all_responses.keys())}")
    
    # 1. ê° ì‘ë‹µì˜ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
    response_summaries = {}
    for model, response in all_responses.items():
        # ì²« ë¬¸ì¥ì´ë‚˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ê°„ë‹¨í•œ ìš”ì•½)
        sentences = extract_sentences_from_response(response)
        summary = sentences[0] if sentences else response[:200]
        response_summaries[model] = normalize_text(summary)
    
    # 2. ì‘ë‹µ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ë° ê·¸ë£¹í™”
    from collections import defaultdict
    similarity_groups = defaultdict(list)
    processed = set()
    
    models = list(all_responses.keys())
    for i, model1 in enumerate(models):
        if model1 in processed:
            continue
        
        group = [model1]
        summary1 = response_summaries[model1]
        
        for model2 in models[i+1:]:
            if model2 in processed:
                continue
            
            summary2 = response_summaries[model2]
            similarity = similarity_ratio(summary1, summary2)
            
            # ìœ ì‚¬ë„ 60% ì´ìƒì´ë©´ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ê°„ì£¼
            if similarity >= 0.6:
                group.append(model2)
                processed.add(model2)
        
        processed.add(model1)
        similarity_groups[model1] = group
    
    # 3. ê°€ì¥ ë§ì€ ëª¨ë¸ì´ ë™ì˜í•˜ëŠ” ê·¸ë£¹ ì°¾ê¸°
    largest_group = max(similarity_groups.values(), key=len)
    representative_model = largest_group[0]
    
    print(f"\nğŸ“Š ë³´íŒ… ê²°ê³¼:")
    for leader, members in similarity_groups.items():
        if len(members) > 1:
            print(f"   ê·¸ë£¹ ({len(members)}ê°œ ëª¨ë¸): {members}")
    
    print(f"\nğŸ† ìµœë‹¤ ë“í‘œ ê·¸ë£¹: {largest_group} ({len(largest_group)}í‘œ)")
    
    # 4. ê²°ê³¼ ìƒì„±
    optimal_answer = all_responses[representative_model]
    
    result = {
        "ìµœì ì˜_ë‹µë³€": optimal_answer,
        "llm_ê²€ì¦_ê²°ê³¼": {},
        "ì‹¬íŒëª¨ë¸": "Voting System",
        "ìƒíƒœ": "ë³´íŒ… ì™„ë£Œ",
        "ì‹ ë¢°ë„": str(int((len(largest_group) / len(all_responses)) * 100)),
        "ë³´íŒ…_ê²°ê³¼": {
            "ì´_ëª¨ë¸_ìˆ˜": len(all_responses),
            "ìµœë‹¤_ë“í‘œ": len(largest_group),
            "ë“í‘œ_ëª¨ë¸": largest_group,
            "ê·¸ë£¹_ì •ë³´": {k: v for k, v in similarity_groups.items()}
        },
        "ì›ë³¸_ì‘ë‹µ": all_responses
    }
    
    # ê° ëª¨ë¸ì˜ ê²€ì¦ ê²°ê³¼ ìƒì„±
    for model in all_responses.keys():
        sentences = extract_sentences_from_response(all_responses[model])
        is_winner = model in largest_group
        
        result["llm_ê²€ì¦_ê²°ê³¼"][model] = {
            "ì •í™•ì„±": "âœ… ë‹¤ìˆ˜ê²° ì±„íƒ" if is_winner else "âŒ ì†Œìˆ˜ ì˜ê²¬",
            "ì˜¤ë¥˜": "ì—†ìŒ" if is_winner else "ë‹¤ìˆ˜ ì˜ê²¬ê³¼ ë¶ˆì¼ì¹˜",
            "ì‹ ë¢°ë„": str(int((len(largest_group) / len(all_responses)) * 100)) if is_winner else "30",
            "ì±„íƒëœ_ì •ë³´": sentences[:3] if is_winner else [],
            "ì œì™¸ëœ_ì •ë³´": [] if is_winner else sentences[:2]
        }
    
    return result


def collect_multi_llm_responses(user_message, judge_model="GPT-4o", selected_models=None, question_type=None, session_id=None, clear_history=False):
    """1ë‹¨ê³„: ì„ íƒëœ LLMë“¤ì—ê²Œ ë³‘ë ¬ ì§ˆì˜ í›„ ì‹¬íŒ ëª¨ë¸ë¡œ ê²€ì¦
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        judge_model: ì‹¬íŒ ëª¨ë¸ ì´ë¦„
        selected_models: ì„ íƒëœ ëª¨ë¸ ëª©ë¡
        question_type: ì§ˆë¬¸ ìœ í˜•
        session_id: ì„¸ì…˜ ID (íˆìŠ¤í† ë¦¬ ê´€ë¦¬ìš©)
        clear_history: íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì—¬ë¶€
    """
    import time
    
    responses = {}
    
    # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš° ê° ëª¨ë¸ì˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if clear_history and selected_models:
        from ..utils.chatbot import chatbots
        model_name_mapping = {
            'GPT-5': 'gpt-5', 'GPT-5-Mini': 'gpt-5-mini',
            'GPT-4.1': 'gpt-4.1', 'GPT-4.1-Mini': 'gpt-4.1-mini',
            'GPT-4o': 'gpt-4o', 'GPT-4o-Mini': 'gpt-4o-mini',
            'GPT-4-Turbo': 'gpt-4-turbo', 'GPT-3.5-Turbo': 'gpt-3.5-turbo',
            'Gemini-2.5-Pro': 'gemini-2.5-pro', 'Gemini-2.5-Flash': 'gemini-2.5-flash',
            'Gemini-2.0-Flash-Exp': 'gemini-2.0-flash-exp', 'Gemini-2.0-Flash-Lite': 'gemini-2.0-flash-lite',
            'Claude-4-Opus': 'claude-4-opus', 'Claude-3.7-Sonnet': 'claude-3.7-sonnet',
            'Claude-3.5-Sonnet': 'claude-3.5-sonnet', 'Claude-3.5-Haiku': 'claude-3.5-haiku',
            'Claude-3-Opus': 'claude-3-opus',
            'HCX-003': 'clova-hcx-003', 'HCX-DASH-001': 'clova-hcx-dash-001',
        }
        for model_display_name in selected_models:
            bot_name = model_name_mapping.get(model_display_name)
            if bot_name and bot_name in chatbots:
                chatbots[bot_name].conversation_history = []
                print(f"   ğŸ”„ {model_display_name} ({bot_name}) íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (collect_multi_llm_responses)")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—”ë“œí¬ì¸íŠ¸ë“¤
    all_llm_endpoints = {
        'GPT-5': 'http://localhost:8000/chat/gpt-5/',
        'GPT-5-Mini': 'http://localhost:8000/chat/gpt-5-mini/',
        'GPT-4.1': 'http://localhost:8000/chat/gpt-4.1/',
        'GPT-4.1-Mini': 'http://localhost:8000/chat/gpt-4.1-mini/',
        'GPT-4o': 'http://localhost:8000/chat/gpt-4o/',
        'GPT-4o-Mini': 'http://localhost:8000/chat/gpt-4o-mini/',
        'GPT-4-Turbo': 'http://localhost:8000/chat/gpt-4-turbo/',
        'GPT-3.5-Turbo': 'http://localhost:8000/chat/gpt-3.5-turbo/',
        'Gemini-2.5-Pro': 'http://localhost:8000/chat/gemini-2.5-pro/',
        'Gemini-2.5-Flash': 'http://localhost:8000/chat/gemini-2.5-flash/',
        'Gemini-2.0-Flash-Exp': 'http://localhost:8000/chat/gemini-2.0-flash-exp/',
        'Gemini-2.0-Flash-Lite': 'http://localhost:8000/chat/gemini-2.0-flash-lite/',
        'Claude-4-Opus': 'http://localhost:8000/chat/claude-4-opus/',
        'Claude-3.7-Sonnet': 'http://localhost:8000/chat/claude-3.7-sonnet/',
        'Claude-3.5-Sonnet': 'http://localhost:8000/chat/claude-3.5-sonnet/',
        'Claude-3.5-Haiku': 'http://localhost:8000/chat/claude-3.5-haiku/',
        'Claude-3-Opus': 'http://localhost:8000/chat/claude-3-opus/',
        'HCX-003': 'http://localhost:8000/chat/clova-hcx-003/',
        'HCX-DASH-001': 'http://localhost:8000/chat/clova-hcx-dash-001/',
    }
    
    # ëª¨ë¸ ì„ íƒ ë¡œì§
    if selected_models:
        print(f"ğŸ“‹ selected_models ì…ë ¥: {selected_models}")
        model_mapping = {
            'gpt-5': 'GPT-5', 'gpt-5-mini': 'GPT-5-Mini',
            'gpt-4.1': 'GPT-4.1', 'gpt-4.1-mini': 'GPT-4.1-Mini',
            'gpt-4o': 'GPT-4o', 'gpt-4o-mini': 'GPT-4o-Mini',
            'gpt-4-turbo': 'GPT-4-Turbo', 'gpt-3.5-turbo': 'GPT-3.5-Turbo',
            'gemini-2.5-pro': 'Gemini-2.5-Pro', 'gemini-2.5-flash': 'Gemini-2.5-Flash',
            'gemini-2.0-flash-exp': 'Gemini-2.0-Flash-Exp', 'gemini-2.0-flash-lite': 'Gemini-2.0-Flash-Lite',
            'claude-4-opus': 'Claude-4-Opus', 'claude-3.7-sonnet': 'Claude-3.7-Sonnet',
            'claude-3.5-sonnet': 'Claude-3.5-Sonnet', 'claude-3.5-haiku': 'Claude-3.5-Haiku',
            'claude-3-opus': 'Claude-3-Opus',
            'clova-hcx-003': 'HCX-003', 'clova-hcx-dash-001': 'HCX-DASH-001',
        }
        
        selected_standard_models = []
        for model in selected_models:
            model_lower = model.lower() if isinstance(model, str) else str(model).lower()
            if model_lower in model_mapping:
                selected_standard_models.append(model_mapping[model_lower])
        
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in selected_standard_models}
    else:
        print(f"âš ï¸ selected_modelsê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ 3ê°œ ì‚¬ìš©")
        default_models = ['GPT-4o-Mini', 'Gemini-2.0-Flash-Lite', 'Claude-3.5-Haiku']
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in default_models}
    
    if not llm_endpoints:
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ¯ ì„ íƒëœ LLM ëª¨ë¸ë“¤: {list(llm_endpoints.keys())}")
    
    async def fetch_response(session, ai_name, endpoint):
        try:
            payload = {'message': user_message, 'user_id': session_id or 'system'}
            async with session.post(endpoint, json=payload, timeout=60) as response:
                if response.status == 200:
                    result = await response.json()
                    response_content = result.get('response', 'ì‘ë‹µ ì—†ìŒ')
                    print(f"âœ… {ai_name} ì‘ë‹µ ìˆ˜ì‹ : {len(str(response_content))}ì")
                    return ai_name, response_content
                else:
                    error_text = await response.text()
                    friendly_msg = get_user_friendly_error_message(Exception(f"HTTP {response.status}"))
                    return ai_name, friendly_msg
        except Exception as e:
            friendly_msg = get_user_friendly_error_message(e)
            return ai_name, friendly_msg
    
    async def collect_all_responses():
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_response(session, ai_name, endpoint) for ai_name, endpoint in llm_endpoints.items()]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in results:
                if isinstance(result, tuple):
                    ai_name, response = result
                    responses[ai_name] = response
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(collect_all_responses())
        loop.close()
        
        print(f"âœ… {len(responses)}ê°œ LLM ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§
        error_patterns = [
            "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€", "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼", "ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œ",
            "API ì¸ì¦ì— ì‹¤íŒ¨", "ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼", "ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        ]
        
        valid_responses = {}
        for ai_name, response in responses.items():
            response_str = str(response)
            is_error = any(pattern in response_str for pattern in error_patterns)
            if len(response_str) < 50 and any(kw in response_str.lower() for kw in ["timeout", "error", "ì˜¤ë¥˜"]):
                is_error = True
            
            if not is_error:
                valid_responses[ai_name] = response
        
        if not valid_responses:
            raise ValueError("ëª¨ë“  LLM ìš”ì²­ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“Š ìœ íš¨í•œ ì‘ë‹µ: {len(valid_responses)}ê°œ")
        final_result = judge_and_generate_optimal_response(valid_responses, user_message, judge_model, question_type, session_id)
        return final_result
        
    except Exception as e:
        print(f"âŒ LLM ì‘ë‹µ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise


def detect_conflicts_in_responses(llm_responses):
    """LLM ì‘ë‹µì—ì„œ ìƒí˜¸ëª¨ìˆœ ê°ì§€ (ì •í™•ë„ í–¥ìƒ ë²„ì „)"""
    
    CONTEXT_STOPWORDS = {
        'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ì¦‰', 'ì´í›„', 'ìµœê·¼',
        'ëŒ€í•œ', 'ê´€ë ¨', 'ê¸°ì¤€', 'ëŒ€í•´', 'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ê²ƒì€', 'ê²ƒì´', 'ê²ƒì„',
        'ê²ƒì—', 'ê²ƒìœ¼ë¡œ', 'ê²ƒì´ë‹¤', 'ê²ƒì…ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤',
        'ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ë°', 'ë“±', 'ë•Œ', 'ë•Œë¬¸', 'ìœ„í•´', 'ì—¬ëŸ¬', 'ë‹¤ì–‘í•œ',
        'ì´', 'ê·¸', 'ì €', 'ë˜ëŠ”', 'í˜¹ì€', 'ìš°ë¦¬', 'í•´ë‹¹', 'ì´ë²ˆ', 'í•´', 'ë…„', 'ì›”', 'ì¼'
    }
    
    def extract_context_tokens(text, start, end):
        window = text[max(0, start - 25):min(len(text), end + 25)]
        tokens = re.findall(r'[A-Za-zê°€-í£]{2,}', window)
        keywords = set()
        for token in tokens:
            token_norm = token.lower()
            if token_norm in CONTEXT_STOPWORDS:
                continue
            keywords.add(token_norm)
        return keywords
    
    def normalize_numeric_tokens(value):
        numbers = re.findall(r'\d+(?:\.\d+)?', value)
        normalized = []
        for num in numbers:
            if '.' in num:
                normalized.append(float(num))
            else:
                normalized.append(int(num))
        return normalized
    
    def values_conflict(category, value_a, info_a, value_b, info_b):
        a_norm = value_a.strip().lower()
        b_norm = value_b.strip().lower()
        
        if not a_norm or not b_norm:
            return False
        if a_norm == b_norm:
            return False
        if a_norm in b_norm or b_norm in a_norm:
            return False
        
        shared_keywords = info_a["keywords"] & info_b["keywords"]
        if not shared_keywords:
            return False
        
        if category in {"dates", "numbers"}:
            nums_a = normalize_numeric_tokens(a_norm)
            nums_b = normalize_numeric_tokens(b_norm)
            if nums_a and nums_b:
                return nums_a != nums_b
            return False
        
        if category == "names":
            similarity = similarity_ratio(a_norm, b_norm)
            return similarity < 0.6
        
        return False
    
    conflicts = {
        "dates": defaultdict(lambda: {"models": set(), "keywords": set()}),
        "numbers": defaultdict(lambda: {"models": set(), "keywords": set()}),
        "names": defaultdict(lambda: {"models": set(), "keywords": set()})
    }
    
    for model_name, response in llm_responses.items():
        for match in re.finditer(r'(\d{4})(?:ë…„)?', response):
            year_str = match.group(1)
            try:
                year = int(year_str)
                if 1000 <= year <= 2100:
                    entry = conflicts["dates"][year_str]
                    entry["models"].add(model_name)
                    entry["keywords"].update(extract_context_tokens(response, match.start(), match.end()))
            except ValueError:
                continue
        
        for match in re.finditer(r'\d+(?:\.\d+)?(?:ëª…|ê°œ|ì›”|ì¼|ì–µ|ë§Œ|ì²œ|ëŒ€|ë…„|ì„¸|%|cm|mm|kg|g)?', response):
            value = match.group(0)
            entry = conflicts["numbers"][value]
            entry["models"].add(model_name)
            entry["keywords"].update(extract_context_tokens(response, match.start(), match.end()))
        
        for match in re.finditer(r'[ê°€-í£]{2,4}(?:\([^)]+\))?', response):
            name = match.group(0)
            name_clean = name.split('(')[0].strip()
            if len(name_clean) < 2:
                continue
            entry = conflicts["names"][name_clean]
            entry["models"].add(model_name)
            entry["keywords"].update(extract_context_tokens(response, match.start(), match.end()))
    
    detected_conflicts = {}
    for category, items in conflicts.items():
        value_infos = []
        for value, info in items.items():
            if len(info["models"]) >= 2:
                value_infos.append((value, info))
        
        if len(value_infos) <= 1:
            continue
        
        conflicting_values = {}
        for i in range(len(value_infos)):
            value_i, info_i = value_infos[i]
            models_i = info_i["models"]
            for j in range(i + 1, len(value_infos)):
                value_j, info_j = value_infos[j]
                models_j = info_j["models"]
                
                if not models_i.isdisjoint(models_j):
                    continue
                
                if values_conflict(category, value_i, info_i, value_j, info_j):
                    conflicting_values.setdefault(value_i, models_i)
                    conflicting_values.setdefault(value_j, models_j)
        
        if conflicting_values:
            detected_conflicts[category] = {
                value: list(models) for value, models in conflicting_values.items()
            }
    
    return detected_conflicts


def extract_sentences_from_response(response_text):
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ì¶œ"""
    
    sentences = []
    
    # 1. ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
    code_blocks = re.findall(r'```[\s\S]*?```', response_text)
    for code_block in code_blocks:
        sentences.append(code_block.strip())
    
    # 2. ì½”ë“œ ì œì™¸ í›„ ë¬¸ì¥ ë¶„ë¦¬
    text_without_code = re.sub(r'```[\s\S]*?```', '', response_text)
    text_sentences = re.split(r'[.!?]\s+', text_without_code)
    
    for sentence in text_sentences:
        sentence = sentence.strip()
        if len(sentence) > 10:
            sentences.append(sentence)
    
    return sentences


def normalize_text(text):
    """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    if not text:
        return ""
    # ê³µë°± í†µì¼
    text = re.sub(r'\s+', ' ', text)
    # ë”°ì˜´í‘œ í†µì¼
    text = text.replace('"', '"').replace('"', '"').replace("'", "'").replace("'", "'")
    # Wikipedia ë©”íƒ€ ì •ë³´ ì œê±°
    text = re.sub(r'\s*\([^)]*Wikipedia[^)]*\)', '', text)
    text = re.sub(r'\s*\([^)]*ë¶ˆì¼ì¹˜[^)]*\)', '', text)
    return text.strip().lower()


def similarity_ratio(a, b):
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def is_sentence_in_response(sentence, original_response, threshold=0.85):
    """ë¬¸ì¥ì´ ì›ë³¸ ì‘ë‹µì— ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì—„ê²©í•˜ê²Œ ê²€ì¦"""
    if not sentence or not original_response:
        return False
    
    sentence_norm = normalize_text(sentence)
    response_norm = normalize_text(original_response)
    
    # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ í•„í„°ë§
    if len(sentence_norm) < 5:
        return False
    
    # 1ì°¨: ì •í™•í•œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
    if sentence_norm in response_norm:
        return True
    
    # 2ì°¨: ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­
    sentence_words = sentence_norm.split()
    response_words = response_norm.split()
    
    # ì§§ì€ ë¬¸ì¥ì€ ì •í™•í•œ ë§¤ì¹­ ìš”êµ¬
    if len(sentence_words) < 5:
        return sentence_norm in response_norm
    
    # ê¸´ ë¬¸ì¥ì€ ìœ ì‚¬ë„ ê¸°ë°˜
    best_ratio = 0.0
    window_size = len(sentence_words)
    
    for i in range(len(response_words) - window_size + 1):
        window = ' '.join(response_words[i:i + window_size])
        ratio = similarity_ratio(sentence_norm, window)
        best_ratio = max(best_ratio, ratio)
    
    if best_ratio >= threshold:
        return True
    
    # 3ì°¨: í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
    stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ',
                 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤'}
    key_words = [w for w in sentence_words if len(w) > 1 and w not in stopwords]
    
    if not key_words:
        return False
    
    match_count = sum(1 for word in key_words if word in response_norm)
    match_ratio = match_count / len(key_words)
    
    return match_ratio >= 0.8


def _build_judge_prompt(user_question, llm_responses, llm_sentences, wikipedia_info):
    """Judge ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    model_sections = [f"[{name} ì›ë³¸]\n{(r[:800] + '...' if len(r) > 800 else r)}" 
                     for name, r in llm_responses.items()]
    sentences_sections = [f"[{name} ë¬¸ì¥ ëª©ë¡ - ì´ ë¬¸ì¥ë§Œ ì‚¬ìš© ê°€ëŠ¥]\n" + 
                         "\n".join([f"  {i+1}. {s}" for i, s in enumerate(sentences)])
                         for name, sentences in llm_sentences.items()]
    wikipedia_section = ""
    if wikipedia_info:
        source_name = wikipedia_info.get('source', 'ê²€ì¦ ì†ŒìŠ¤')
        wikipedia_section = f"""

**ğŸŒ {source_name} ê²€ì¦ ê²°ê³¼ (ê³µì‹ ì •ë³´):**
ì œëª©: {wikipedia_info['title']}
ë‚´ìš©: {wikipedia_info['extract'][:500]}

**ğŸš¨ {source_name} ê²€ì¦ ê¸°ì¤€:**
- {source_name} ì •ë³´ì™€ **ì¼ì¹˜í•˜ëŠ” AI ë‹µë³€ë§Œ ì±„íƒ**
- {source_name} ì •ë³´ì™€ **ë¶ˆì¼ì¹˜í•˜ëŠ” AI ë‹µë³€ì€ ì œì™¸**
- ê° AIì˜ ì±„íƒ/ì œì™¸ íŒë‹¨ ì‹œ {source_name}ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”


"""
    sentences_text = "\n\n".join(sentences_sections)
    model_responses_text = "\n\n".join(model_sections)
    wiki_used = True if wikipedia_info else False
    
    return f"""ì§ˆë¬¸: {user_question}

**ğŸš¨ í•µì‹¬ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
1. **ì•„ë˜ "ë¬¸ì¥ ëª©ë¡"ì— ìˆëŠ” ë¬¸ì¥ë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„± ì ˆëŒ€ ê¸ˆì§€
2. **ê° AIì˜ ë¬¸ì¥ì€ í•´ë‹¹ AIì˜ ëª©ë¡ì—ì„œë§Œ ì„ íƒ** - ë‹¤ë¥¸ AI ë¬¸ì¥ ê°€ì ¸ì˜¤ê¸° ê¸ˆì§€
3. **ì±„íƒ/ì œì™¸ ì •ë³´ëŠ” í•´ë‹¹ AIì˜ ì›ë³¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬**
4. **ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì†ŒìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì±„íƒ/ì œì™¸ íŒë‹¨**

{sentences_text}
{wikipedia_section}

**ì›ë³¸ ë‹µë³€ (ì°¸ê³ ìš©):**
{model_responses_text}

**verification_results ì‘ì„± ê·œì¹™:**

ê° AIë§ˆë‹¤:
```json
"AIëª¨ë¸ëª…": {{
  "accuracy": "ì •í™•ì„± (ê²€ì¦ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ë©´ 'ì •í™•', ë¶ˆì¼ì¹˜í•˜ë©´ 'ë¶€ì •í™•')",
  "errors": "ì˜¤ë¥˜ ì„¤ëª… (ê²€ì¦ ì†ŒìŠ¤ ë¶ˆì¼ì¹˜ ì‹œ ëª…ì‹œ)",
  "confidence": "0-100",
  "adopted_info": ["í•´ë‹¹ AI ë¬¸ì¥ ëª©ë¡ì—ì„œ ê²€ì¦ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” ì›ë¬¸"],
  "rejected_info": ["í•´ë‹¹ AI ë¬¸ì¥ ëª©ë¡ì—ì„œ ê²€ì¦ ì†ŒìŠ¤ì™€ ë¶ˆì¼ì¹˜í•˜ëŠ” ì›ë¬¸"]
}}
```

**ğŸš¨ ì ˆëŒ€ ê·œì¹™:**
1. **í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì— ìˆëŠ” ë¬¸ì¥ë§Œ ë³µì‚¬** - í•œ ê¸€ìë„ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”
2. **ë‹¤ë¥¸ AIì˜ ë¬¸ì¥ ì ˆëŒ€ ë³µì‚¬ ê¸ˆì§€**
3. **ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„± ê¸ˆì§€**
4. **ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•´ë‹¹ ì†ŒìŠ¤ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨**

**optimal_answer:**
- **ë°˜ë“œì‹œ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤!**
- ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ê²€ì¦ ì†ŒìŠ¤ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
- ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ì™€ ì¼ì¹˜í•˜ëŠ” AI ë¬¸ì¥ë“¤ì„ ì¡°í•©í•˜ì—¬ ë‹µë³€ ìƒì„±
- ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì—¬ëŸ¬ AI ê³µí†µ ì •ë³´ ìš°ì„ 
- **ì ˆëŒ€ "ì—†ìŠµë‹ˆë‹¤", "ì—†ìŒ" ê°™ì€ ë¹ˆ ë‹µë³€ì„ ë°˜í™˜í•˜ì§€ ë§ˆì„¸ìš”!**
- **ìµœì†Œ 100ì ì´ìƒì˜ ì˜ë¯¸ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”!**

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "optimal_answer": "ê²€ì¦ ì†ŒìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ê²€ì¦ëœ ë¬¸ì¥ ì¡°í•©",
  "verification_results": {{"ëª¨ë“  AI ê²€ì¦ ê²°ê³¼"}},
  "confidence_score": "0-100",
  "contradictions_detected": [],
  "fact_verification": {{"wikipedia_used": {wiki_used}}},
  "analysis_rationale": "ê²€ì¦ ì†ŒìŠ¤ ê²€ì¦ ê²°ê³¼ ë° ì„ íƒ ê·¼ê±°"
}}"""


def extract_valid_sentences(sentence_list, original_response, ai_name):
    """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œë¡œ ì›ë³¸ì— í¬í•¨ëœ ê²ƒë§Œ ì¶”ì¶œ"""
    if not sentence_list or not original_response:
        return []
    
    valid_sentences = []
    invalid_count = 0
    
    for item in sentence_list:
        if not isinstance(item, str) or not item.strip():
            continue
        
        item_cleaned = normalize_text(item)
        if len(item_cleaned) < 5:
            continue
        
        if is_sentence_in_response(item, original_response):
            valid_sentences.append(item.strip())
        else:
            invalid_count += 1
            print(f"âŒ {ai_name} í™˜ê° ê°ì§€ ë° ì œê±°: '{item[:60]}...'")
    
    if invalid_count > 0:
        print(f"âš ï¸ {ai_name}: {invalid_count}ê°œ í™˜ê° ë¬¸ì¥ ì œê±°ë¨")
    
    return valid_sentences


def judge_and_generate_optimal_response(llm_responses, user_question, judge_model="GPT-5", question_type=None, session_id=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ (Wikipedia ê²€ì¦ + í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë³´íŒ…)"""
    try:
        print(f"\nğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œì‘: {user_question}")
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° ê²€ì¦ í‚¤ì›Œë“œ ì¶”ì¶œ
        verification_keywords = []
        if question_type is None:
            classification_result = classify_question_type(user_question)
            if isinstance(classification_result, dict):
                question_type = classification_result.get('type', 'factual')
                verification_keywords = classification_result.get('keywords', [])
            else:
                # í•˜ìœ„ í˜¸í™˜ì„±: ë¬¸ìì—´ë¡œ ë°˜í™˜ëœ ê²½ìš°
                question_type = classification_result
                verification_keywords = []
        else:
            print(f"ğŸ“ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ ìœ í˜•: {question_type}")
            # ì „ë‹¬ë°›ì€ question_typeì´ ë¬¸ìì—´ì¸ ê²½ìš°, í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•´ ì¬ë¶„ë¥˜
            if isinstance(question_type, str) and question_type not in ['image', 'document', 'code', 'creative']:
                classification_result = classify_question_type(user_question)
                if isinstance(classification_result, dict):
                    verification_keywords = classification_result.get('keywords', [])
        
        # question_typeì´ "general"ì´ê±°ë‚˜ Noneì´ë©´ ë‹¤ì‹œ ë¶„ë¥˜
        if question_type in [None, "general"]:
            print(f"ğŸ”„ ì§ˆë¬¸ ìœ í˜• ì¬ë¶„ë¥˜ ì¤‘...")
            classification_result = classify_question_type(user_question)
            if isinstance(classification_result, dict):
                question_type = classification_result.get('type', 'factual')
                verification_keywords = classification_result.get('keywords', [])
            else:
                question_type = classification_result
                verification_keywords = []
            print(f"ğŸ“ ì¬ë¶„ë¥˜ ê²°ê³¼: {question_type}")
        
        # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
        print(f"\nğŸ“ ê° AI ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• ...")
        llm_sentences = {}
        for model_name, response in llm_responses.items():
            sentences = extract_sentences_from_response(response)
            llm_sentences[model_name] = sentences
            print(f"  - {model_name}: {len(sentences)}ê°œ ë¬¸ì¥")
        
        # ìƒí˜¸ëª¨ìˆœ ê°ì§€
        conflicts = detect_conflicts_in_responses(llm_responses)
        print(f"\nğŸ“Š ìƒí˜¸ëª¨ìˆœ ê°ì§€: {len(conflicts)}ê°œ ì¹´í…Œê³ ë¦¬")
        for category, items in conflicts.items():
            print(f"  - {category}: {items}")
        
        # ğŸš¨ ê²€ì¦ ì†ŒìŠ¤ ê²€ìƒ‰ (ì‚¬ì‹¤ ì§ˆë¬¸ì¼ ë•Œ í•­ìƒ ê²€ìƒ‰)
        wikipedia_info = None
        use_voting = False
        
        if question_type == "factual":
            if len(conflicts) > 0:
                print(f"\nğŸŒ ìƒí˜¸ëª¨ìˆœ ê°ì§€ë¨! ë‹¤ì¤‘ ê²€ì¦ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
            else:
                print(f"\nğŸŒ ì‚¬ì‹¤ ì§ˆë¬¸ ê°ì§€! ë‹¤ì¤‘ ê²€ì¦ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
            
            # ê²€ì¦ í‚¤ì›Œë“œ ì‚¬ìš© (LLMì´ ì¶”ì¶œí•œ í‚¤ì›Œë“œ ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©)
            if verification_keywords:
                search_query = ' '.join(verification_keywords)
                print(f"ğŸ” LLM ì¶”ì¶œ ê²€ì¦ í‚¤ì›Œë“œ ì‚¬ìš©: {verification_keywords} -> '{search_query}'")
            else:
                search_query = user_question
                print(f"ğŸ” ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©: '{search_query}'")
            
            # Wikipedia, Wikidata, DBpedia, DuckDuckGo ì¤‘ ê°€ì¥ ì¢‹ì€ í•˜ë‚˜ ì„ íƒ
            wikipedia_info = get_best_verification_source(search_query)
            
            if wikipedia_info:
                print(f"âœ… ê²€ì¦ ì™„ë£Œ: {wikipedia_info.get('source', 'Unknown')} - {wikipedia_info.get('title', 'No title')}")
                print(f"   ì‹ ë¢°ë„: {wikipedia_info.get('confidence', 0):.2f}")
                # ìƒí˜¸ëª¨ìˆœì´ ì—†ì–´ë„ ê²€ì¦ ì†ŒìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            else:
                print(f"âš ï¸ ëª¨ë“  ê²€ì¦ ì†ŒìŠ¤ ì‹¤íŒ¨ - ê²€ì¦ ì†ŒìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                # ìƒí˜¸ëª¨ìˆœì´ ìˆì„ ë•Œë§Œ ë³´íŒ… ì‹œìŠ¤í…œ ì‚¬ìš©
                if len(conflicts) > 0:
                    print(f"   ìƒí˜¸ëª¨ìˆœì´ ìˆìœ¼ë¯€ë¡œ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë³´íŒ… ì‹œìŠ¤í…œ í™œì„±í™”")
                    use_voting = True
        else:
            print(f"â„¹ï¸ ì§ˆë¬¸ ìœ í˜•ì´ 'factual'ì´ ì•„ë‹ˆë¯€ë¡œ ê²€ì¦ ì†ŒìŠ¤ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤. (í˜„ì¬ ìœ í˜•: {question_type})")
        
        # ğŸ—³ï¸ Wikipediaê°€ ì—†ìœ¼ë©´ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì¶”ê°€ í˜¸ì¶œ ë° ë³´íŒ…
        if use_voting:
            print(f"\nğŸ¯ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì¶”ê°€ í˜¸ì¶œ ì‹œì‘...")
            
            # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ëª©ë¡
            currently_used = list(llm_responses.keys())
            print(f"   í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {currently_used}")
            
            # ì¶”ê°€ í˜¸ì¶œí•  í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ê²°ì •
            premium_models_to_call = get_premium_models_to_call(currently_used)
            
            if premium_models_to_call:
                print(f"   ì¶”ê°€ í˜¸ì¶œí•  ëª¨ë¸: {premium_models_to_call}")
                
                # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¹„ë™ê¸° í˜¸ì¶œ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                premium_responses = loop.run_until_complete(
                    call_additional_premium_models(user_question, premium_models_to_call, session_id)
                )
                loop.close()
                
                if premium_responses:
                    print(f"âœ… {len(premium_responses)}ê°œ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‘ë‹µ ìˆ˜ì‹ ")
                    
                    # ê¸°ì¡´ ì‘ë‹µê³¼ í”„ë¦¬ë¯¸ì—„ ì‘ë‹µ í•©ì¹˜ê¸°
                    all_responses = {**llm_responses, **premium_responses}
                    
                    # ë³´íŒ… ì‹œìŠ¤í…œ ì ìš©
                    voting_result = apply_voting_system(all_responses, user_question)
                    
                    extra_models_used = list(premium_responses.keys())
                    voting_result["ì¶”ê°€_ëª¨ë¸_í˜¸ì¶œ"] = {
                        "ì‚¬ìœ ": "ìƒì¶© ì‘ë‹µ ë° ê²€ì¦ ì†ŒìŠ¤ ë¶€ì¬",
                        "ì¶”ê°€_ëª¨ë¸": extra_models_used,
                        "ì´_í˜¸ì¶œ": len(extra_models_used),
                        "ê¸°ì¡´_ëª¨ë¸": list(llm_responses.keys()),
                        "ì „ì²´_ëª¨ë¸": list(all_responses.keys())
                    }
                    
                    if not voting_result.get("ë¶„ì„_ê·¼ê±°"):
                        voting_summary_models = voting_result.get("ë³´íŒ…_ê²°ê³¼", {}).get("ë“í‘œ_ëª¨ë¸", [])
                        total_models = list(dict.fromkeys(all_responses.keys()))
                        if voting_summary_models:
                            summary_leads = ', '.join(voting_summary_models[:2])
                            if len(voting_summary_models) > 2:
                                summary_leads += " ë“±"
                        else:
                            summary_leads = ', '.join(total_models[:2]) if total_models else "ì¶”ê°€ ëª¨ë¸"
                        reason_text = (
                            f"AI ì‘ë‹µ ê°„ ìƒì¶©ì´ ê°ì§€ë˜ì–´ ì¶”ê°€ì ìœ¼ë¡œ {len(extra_models_used)}ê°œì˜ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸"
                            f"({', '.join(extra_models_used)})ì„ í˜¸ì¶œí–ˆìŠµë‹ˆë‹¤. "
                            f"ê²°ê³¼ì ìœ¼ë¡œ {summary_leads} {len(total_models)}ê°œ ëª¨ë¸ì˜ í•©ì˜ ë‚´ìš©ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤."
                        )
                        voting_result["ë¶„ì„_ê·¼ê±°"] = reason_text
                    
                    print(f"\nğŸ† ë³´íŒ… ì™„ë£Œ: {voting_result['ë³´íŒ…_ê²°ê³¼']['ë“í‘œ_ëª¨ë¸']}")
                    
                    return voting_result
                else:
                    print(f"âš ï¸ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‘ë‹µ ì‹¤íŒ¨ - ê¸°ë³¸ Judge ì‹œìŠ¤í…œ ì‚¬ìš©")
            else:
                print(f"âš ï¸ ì¶”ê°€ í˜¸ì¶œí•  í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì—†ìŒ - ê¸°ë³¸ Judge ì‹œìŠ¤í…œ ì‚¬ìš©")
        
        # Wikipedia ê²€ì¦ì´ ìˆê±°ë‚˜ ë³´íŒ…ì´ ë¶ˆí•„ìš”í•œ ê²½ìš° ê¸°ì¡´ Judge ì‹œìŠ¤í…œ ì‚¬ìš©
        judge_prompt = _build_judge_prompt(user_question, llm_responses, llm_sentences, wikipedia_info)
        
        print(f"\nğŸ“ ì‹¬íŒ ëª¨ë¸({judge_model}) í˜¸ì¶œ...")
        judge_response = call_judge_model(judge_model, judge_prompt)
        
        print(f"\nğŸ“ ì‘ë‹µ íŒŒì‹± ë° í™˜ê° ê²€ì¦...")
        parsed_result = parse_judge_response(judge_response, judge_model, llm_responses, llm_sentences, wikipedia_info)
        
        return parsed_result
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        if llm_responses:
            longest_response = max(llm_responses.values(), key=len)
            result = {
                "ìµœì ì˜_ë‹µë³€": longest_response,
                "llm_ê²€ì¦_ê²°ê³¼": {
                    model: {
                        "ì •í™•ì„±": "âŒ",
                        "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨",
                        "ì‹ ë¢°ë„": "0",
                        "ì±„íƒëœ_ì •ë³´": [],
                        "ì œì™¸ëœ_ì •ë³´": []
                    }
                    for model in llm_responses.keys()
                },
                "ì‹¬íŒëª¨ë¸": judge_model,
                "ìƒíƒœ": "ê²€ì¦ ì‹¤íŒ¨",
                "ì›ë³¸_ì‘ë‹µ": llm_responses
            }
            # ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€ (wikipedia_infoê°€ ìˆëŠ” ê²½ìš°)
            # wikipedia_infoëŠ” í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ Noneìœ¼ë¡œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ í•­ìƒ ì ‘ê·¼ ê°€ëŠ¥
            if wikipedia_info:
                result["ê²€ì¦_ì†ŒìŠ¤"] = {
                    "ì‚¬ìš©ë¨": True,
                    "ì†ŒìŠ¤": wikipedia_info.get("source", "Unknown"),
                    "ì œëª©": wikipedia_info.get("title", ""),
                    "ë‚´ìš©": wikipedia_info.get("extract", "")[:200],
                    "ì‹ ë¢°ë„": wikipedia_info.get("confidence", 0)
                }
            else:
                result["ê²€ì¦_ì†ŒìŠ¤"] = {
                    "ì‚¬ìš©ë¨": False,
                    "ì†ŒìŠ¤": None,
                    "ì œëª©": None,
                    "ë‚´ìš©": None,
                    "ì‹ ë¢°ë„": 0
                }
            return result


def call_judge_model(model_name, prompt):
    """ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ"""
    try:
        if model_name in ['GPT-5', 'GPT-4', 'GPT-4o', 'GPT-4o-mini', 'GPT-3.5-turbo']:
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                raise ValueError("OpenAI API í‚¤ ë¯¸ì„¤ì •")
            
            client = openai.OpenAI(api_key=openai_api_key)
            
            # ëª¨ë¸ëª… ë³€í™˜
            model_map = {
                'GPT-5': 'gpt-5',
                'GPT-4': 'gpt-4',
                'GPT-4o': 'gpt-4o',
                'GPT-4o-mini': 'gpt-4o-mini',
                'GPT-3.5-turbo': 'gpt-3.5-turbo'
            }
            openai_model = model_map.get(model_name, 'gpt-4o')
            
            is_latest = 'gpt-5' in openai_model or 'o1' in openai_model or 'o3' in openai_model
            
            api_params = {
                "model": openai_model,
                "messages": [
                    {"role": "system", "content": """ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸš¨ ì ˆëŒ€ ê·œì¹™:
1. **ê° AIê°€ ì‹¤ì œë¡œ ë§í•œ ë¬¸ì¥ë§Œ** adopted_info/rejected_infoì— ë³µì‚¬
2. **ì ˆëŒ€ ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„± ê¸ˆì§€** - í™˜ê°(hallucination) ê¸ˆì§€!
3. **ê° AI ë¬¸ì¥ì€ í•´ë‹¹ AI ì›ë³¸ì— ìˆì–´ì•¼ í•¨**
4. **ë‹¤ë¥¸ AI ë¬¸ì¥ ë³µì‚¬ ê¸ˆì§€**
5. **ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì†ŒìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì±„íƒ/ì œì™¸ íŒë‹¨**

âœ… ì˜¬ë°”ë¥¸ ë¶„ì„:
- ê° AI ì›ë³¸ì—ì„œ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬
- ê²€ì¦ ì†ŒìŠ¤ì™€ ì¼ì¹˜í•˜ëŠ” ì •ë³´ ì±„íƒ
- ê²€ì¦ ì†ŒìŠ¤ì™€ ë¶ˆì¼ì¹˜í•˜ëŠ” ì •ë³´ ì œì™¸

âŒ í™˜ê°:
- ì›ë³¸ì— ì—†ëŠ” ìƒˆ ë¬¸ì¥ ìƒì„±
- ë‹¤ë¥¸ AI ë¬¸ì¥ì„ í•´ë‹¹ AI ì±„íƒ/ì œì™¸ì— í¬í•¨
- AIê°€ ë§í•˜ì§€ ì•Šì€ ë‚´ìš© ë§Œë“¤ì–´ë‚´ê¸°

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ."""},
                    {"role": "user", "content": prompt}
                ]
            }
            
            if not is_latest:
                api_params["temperature"] = 0.0
            
            completion_limit = get_openai_completion_limit(openai_model)
            if is_latest:
                api_params["max_completion_tokens"] = completion_limit
            else:
                api_params["max_tokens"] = completion_limit
                api_params["response_format"] = {"type": "json_object"}
            
            response = client.chat.completions.create(**api_params)
            return response.choices[0].message.content.strip()
        else:
            return call_judge_model('GPT-4o', prompt)
            
    except Exception as e:
        print(f"âŒ ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise


def parse_judge_response(judge_response, judge_model, llm_responses=None, llm_sentences=None, wikipedia_info=None):
    """ì‹¬íŒ ëª¨ë¸ JSON ì‘ë‹µ íŒŒì‹± ë° ì—„ê²©í•œ í™˜ê° ê²€ì¦"""
    try:
        
        # JSON ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', judge_response, re.DOTALL)
        if not json_match:
            return create_fallback_result(judge_model, llm_responses, wikipedia_info)
        
        json_str = json_match.group()
        try:
            parsed_data = json.loads(json_str)
            print(f"âœ… JSON íŒŒì‹± ì„±ê³µ")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return create_fallback_result(judge_model, llm_responses, wikipedia_info)
        
        optimal_answer = parsed_data.get("optimal_answer", "").strip()
        
        # optimal_answerê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ê²½ìš°, Wikipedia ì •ë³´ë‚˜ AI ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
        if not optimal_answer or len(optimal_answer) < 10 or "ì—†ìŠµë‹ˆë‹¤" in optimal_answer or "ì—†ìŒ" in optimal_answer:
            print(f"âš ï¸ Judgeê°€ ì œê³µí•œ optimal_answerê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¶€ì ì ˆí•¨: '{optimal_answer}'")
            
            # Wikipedia ì •ë³´ê°€ ìˆìœ¼ë©´ Wikipedia ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            if wikipedia_info:
                wiki_title = wikipedia_info.get('title', '')
                wiki_extract = wikipedia_info.get('extract', '')
                if wiki_extract:
                    # Wikipedia ë‚´ìš©ì˜ ì²« ë¶€ë¶„ì„ ìµœì  ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
                    optimal_answer = f"{wiki_title}ì— ëŒ€í•œ ì •ë³´:\n\n{wiki_extract[:500]}"
                    print(f"âœ… Wikipedia ì •ë³´ë¡œ ìµœì  ë‹µë³€ ìƒì„±: {len(optimal_answer)}ì")
            
            # Wikipediaë„ ì—†ìœ¼ë©´ ê°€ì¥ ê¸´ AI ì‘ë‹µ ì‚¬ìš©
            if (not optimal_answer or len(optimal_answer) < 10) and llm_responses:
                longest_response = max(llm_responses.values(), key=len)
                if longest_response and len(longest_response) > 10:
                    optimal_answer = longest_response[:1000]  # ìµœëŒ€ 1000ì
                    print(f"âœ… ê°€ì¥ ê¸´ AI ì‘ë‹µìœ¼ë¡œ ìµœì  ë‹µë³€ ìƒì„±: {len(optimal_answer)}ì")
        
        result = {
            "ìµœì ì˜_ë‹µë³€": optimal_answer,
            "llm_ê²€ì¦_ê²°ê³¼": {},
            "ì‹¬íŒëª¨ë¸": judge_model,
            "ìƒíƒœ": "ì„±ê³µ",
            "ì‹ ë¢°ë„": parsed_data.get("confidence_score", "50"),
            "ìƒí˜¸ëª¨ìˆœ": parsed_data.get("contradictions_detected", []),
            "ì‚¬ì‹¤ê²€ì¦": parsed_data.get("fact_verification", {}),
            "ë¶„ì„_ê·¼ê±°": parsed_data.get("analysis_rationale", "")
        }
        
        # ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ëŠ” ì•„ë˜ì—ì„œ í†µí•© ì²˜ë¦¬
        
        # ê²€ì¦ ê²°ê³¼ íŒŒì‹± ë° ì—„ê²©í•œ í™˜ê° ê²€ì¦
        verification_results = parsed_data.get("verification_results", {})
        processed_models = set()
        
        for model_name, verification in verification_results.items():
            processed_models.add(model_name)
            
            adopted_raw = verification.get("adopted_info", [])
            rejected_raw = verification.get("rejected_info", [])
            
            print(f"\nğŸ” {model_name} ê²€ì¦:")
            print(f"   Judge ì œê³µ adopted: {len(adopted_raw)}ê°œ")
            print(f"   Judge ì œê³µ rejected: {len(rejected_raw)}ê°œ")
            
            adopted_info = []
            rejected_info = []
            
            if llm_responses and model_name in llm_responses:
                original_response = llm_responses[model_name]
                
                # ì—„ê²©í•œ í™˜ê° ê²€ì¦
                adopted_info = extract_valid_sentences(adopted_raw, original_response, model_name)
                rejected_info = extract_valid_sentences(rejected_raw, original_response, model_name)
                
                print(f"   ê²€ì¦ í›„ adopted: {len(adopted_info)}ê°œ")
                print(f"   ê²€ì¦ í›„ rejected: {len(rejected_info)}ê°œ")
                
                # ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ì—ì„œ ì¶”ì¶œ
                if not adopted_info and not rejected_info:
                    print(f"âš ï¸ {model_name}: ëª¨ë‘ ë¹„ì–´ìˆìŒ, ì›ë³¸ì—ì„œ ì¶”ì¶œ")
                    sentences = extract_sentences_from_response(original_response)
                    
                    # Wikipedia ì •ë³´ê°€ ìˆìœ¼ë©´ Wikipedia ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
                    if wikipedia_info and sentences:
                        wiki_text = wikipedia_info['extract'].lower()
                        for sentence in sentences[:3]:
                            sentence_lower = sentence.lower()
                            # Wikipedia ë‚´ìš©ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸
                            similarity = similarity_ratio(sentence_lower, wiki_text)
                            if similarity > 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì±„íƒ
                                adopted_info.append(sentence)
                            else:
                                rejected_info.append(sentence)
                    else:
                        adopted_info = sentences[:3] if sentences else []
                    
                    print(f"   ì›ë³¸ ì¶”ì¶œ í›„ adopted: {len(adopted_info)}ê°œ, rejected: {len(rejected_info)}ê°œ")
            
            result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                "ì •í™•ì„±": verification.get("accuracy", "ì •í™•"),
                "ì˜¤ë¥˜": verification.get("errors", "ì—†ìŒ"),
                "ì‹ ë¢°ë„": verification.get("confidence", "50"),
                "ì±„íƒëœ_ì •ë³´": adopted_info,
                "ì œì™¸ëœ_ì •ë³´": rejected_info
            }
        
        # Judgeê°€ ëˆ„ë½í•œ ëª¨ë¸ ì²˜ë¦¬
        if llm_responses:
            for model_name in llm_responses.keys():
                if model_name not in processed_models:
                    print(f"\nâš ï¸ {model_name}: Judge ê²°ê³¼ ëˆ„ë½, ê¸°ë³¸ ì •ë³´ ìƒì„±")
                    sentences = extract_sentences_from_response(llm_responses[model_name])
                    
                    adopted_info = []
                    rejected_info = []
                    
                    # Wikipedia ì •ë³´ê°€ ìˆìœ¼ë©´ Wikipedia ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
                    if wikipedia_info and sentences:
                        wiki_text = wikipedia_info['extract'].lower()
                        for sentence in sentences[:3]:
                            sentence_lower = sentence.lower()
                            similarity = similarity_ratio(sentence_lower, wiki_text)
                            if similarity > 0.3:
                                adopted_info.append(sentence)
                            else:
                                rejected_info.append(sentence)
                    else:
                        adopted_info = sentences[:3] if sentences else []
                    
                    result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                        "ì •í™•ì„±": "âœ…" if adopted_info else "âŒ",
                        "ì˜¤ë¥˜": "ì—†ìŒ" if adopted_info else "ê²€ì¦ ì†ŒìŠ¤ ë¶ˆì¼ì¹˜",
                        "ì‹ ë¢°ë„": "50",
                        "ì±„íƒëœ_ì •ë³´": adopted_info,
                        "ì œì™¸ëœ_ì •ë³´": rejected_info
                    }
            
            result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
        
        # ìµœì¢… í†µê³„
        print(f"\nğŸ“Š ìµœì¢… ê²€ì¦ í†µê³„:")
        total_adopted = sum(len(v.get("ì±„íƒëœ_ì •ë³´", [])) for v in result["llm_ê²€ì¦_ê²°ê³¼"].values())
        total_rejected = sum(len(v.get("ì œì™¸ëœ_ì •ë³´", [])) for v in result["llm_ê²€ì¦_ê²°ê³¼"].values())
        print(f"   ì „ì²´ ì±„íƒ: {total_adopted}ê°œ")
        print(f"   ì „ì²´ ì œì™¸: {total_rejected}ê°œ")
        print(f"   ì²˜ë¦¬ ëª¨ë¸: {len(result['llm_ê²€ì¦_ê²°ê³¼'])}ê°œ")
        
        # ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€ (ì—†ì„ ë•Œë„ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ)
        if wikipedia_info:
            result["ê²€ì¦_ì†ŒìŠ¤"] = {
                "ì‚¬ìš©ë¨": True,
                "ì†ŒìŠ¤": wikipedia_info.get("source", "Unknown"),
                "ì œëª©": wikipedia_info.get("title", ""),
                "ë‚´ìš©": wikipedia_info.get("extract", "")[:200],
                "ì‹ ë¢°ë„": wikipedia_info.get("confidence", 0)
            }
            print(f"   ê²€ì¦ ì†ŒìŠ¤: âœ… {wikipedia_info.get('source', 'Unknown')} ì‚¬ìš©ë¨")
        else:
            result["ê²€ì¦_ì†ŒìŠ¤"] = {
                "ì‚¬ìš©ë¨": False,
                "ì†ŒìŠ¤": None,
                "ì œëª©": None,
                "ë‚´ìš©": None,
                "ì‹ ë¢°ë„": 0
            }
            print(f"   ê²€ì¦ ì†ŒìŠ¤: âŒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ")
        
        return result
        
    except Exception as e:
        print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return create_fallback_result(judge_model, llm_responses, wikipedia_info)


def create_fallback_result(judge_model, llm_responses=None, wikipedia_info=None):
    """í´ë°± ê²°ê³¼ ìƒì„±"""
    result = {
        "ìµœì ì˜_ë‹µë³€": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "llm_ê²€ì¦_ê²°ê³¼": {},
        "ì‹¬íŒëª¨ë¸": judge_model,
        "ìƒíƒœ": "íŒŒì‹± ì‹¤íŒ¨",
        "ì‹ ë¢°ë„": "0",
        "ìƒí˜¸ëª¨ìˆœ": [],
        "ì‚¬ì‹¤ê²€ì¦": {}
    }
    
    # ê²€ì¦ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
    if wikipedia_info:
        result["ê²€ì¦_ì†ŒìŠ¤"] = {
            "ì‚¬ìš©ë¨": True,
            "ì†ŒìŠ¤": wikipedia_info.get("source", "Unknown"),
            "ì œëª©": wikipedia_info.get("title", ""),
            "ë‚´ìš©": wikipedia_info.get("extract", "")[:200],
            "ì‹ ë¢°ë„": wikipedia_info.get("confidence", 0)
        }
    else:
        result["ê²€ì¦_ì†ŒìŠ¤"] = {
            "ì‚¬ìš©ë¨": False,
            "ì†ŒìŠ¤": None,
            "ì œëª©": None,
            "ë‚´ìš©": None,
            "ì‹ ë¢°ë„": 0
    }
    
    if llm_responses:
        for model in llm_responses.keys():
            sentences = extract_sentences_from_response(llm_responses[model])
            result["llm_ê²€ì¦_ê²°ê³¼"][model] = {
                "ì •í™•ì„±": "âŒ",
                "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨",
                "ì‹ ë¢°ë„": "0",
                "ì±„íƒëœ_ì •ë³´": sentences[:3] if sentences else [],
                "ì œì™¸ëœ_ì •ë³´": []
            }
        result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
    
    return result


def format_optimal_response(final_result):
    """ìµœì  ë‹µë³€ í¬ë§·íŒ…"""
    try:
        optimal_answer = final_result.get("ìµœì ì˜_ë‹µë³€", "")
        
        if not optimal_answer or len(optimal_answer.strip()) == 0:
            optimal_answer = "ìµœì  ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        return f"""## ìµœì ì˜ ë‹µë³€

{optimal_answer}
"""
    except Exception as e:
        print(f"âŒ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
        return f"""**ìµœì ì˜ ë‹µë³€:**

{final_result.get('ìµœì ì˜_ë‹µë³€', 'ë‹µë³€ ìƒì„± ì‹¤íŒ¨')}

*í¬ë§·íŒ… ì˜¤ë¥˜*
"""