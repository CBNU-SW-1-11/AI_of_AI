"""
ìµœì  ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤
ì‹¬íŒ ëª¨ë¸ì„ í†µí•œ ë‹¤ì¤‘ LLM ì‘ë‹µ ê²€ì¦ ë° ìµœì  ë‹µë³€ ìƒì„±
"""
import os
import re
import json
import asyncio
import aiohttp
import openai
import anthropic
from groq import Groq
import ollama

# ë¡œì»¬ imports
from ..utils.error_handlers import get_user_friendly_error_message
from ..utils.ai_utils import enforce_korean_instruction, get_openai_completion_limit


def detect_question_type_from_content(content):
    """ì§ˆë¬¸ ë‚´ìš©ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ìœ í˜• ê°ì§€: code, image, document, creative, general"""
    import re
    
    content_lower = content.lower()
    
    # ì½”ë“œ ê´€ë ¨ í‚¤ì›Œë“œ (ì½”ë“œ ì‘ì„±, êµ¬í˜„, í•¨ìˆ˜, ì•Œê³ ë¦¬ì¦˜ ë“±)
    code_keywords = ['ì½”ë“œ', 'code', 'í•¨ìˆ˜', 'function', 'í”„ë¡œê·¸ë˜ë°', 'programming', 'ì•Œê³ ë¦¬ì¦˜', 'algorithm', 
                     'êµ¬í˜„', 'implement', 'ì‘ì„±', 'write', 'ê°œë°œ', 'develop', 'ìŠ¤í¬ë¦½íŠ¸', 'script',
                     'íŒŒì´ì¬', 'python', 'ìë°”', 'java', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'javascript', 'c++', 'c#']
    
    # ì´ë¯¸ì§€ ê´€ë ¨ í‚¤ì›Œë“œ
    image_keywords = ['ì´ë¯¸ì§€', 'image', 'ì‚¬ì§„', 'photo', 'ê·¸ë¦¼', 'picture', 'ì‹œê°', 'visual', 'í™”ë©´']
    
    # ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ
    document_keywords = ['ë¬¸ì„œ', 'document', 'pdf', 'íŒŒì¼', 'file', 'ìš”ì•½', 'summary', 'ë‚´ìš©', 'content']
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ í‚¤ì›Œë“œ
    creative_keywords = ['ê¸€ì“°ê¸°', 'writing', 'ì°½ì‘', 'creative', 'ì†Œì„¤', 'novel', 'ì‹œ', 'poem', 'ì—ì„¸ì´', 'essay',
                        'ì´ì•¼ê¸°', 'story', 'ë‚´ìš© ì‘ì„±', 'write content', 'ë¬¸ì¥', 'sentence']
    
    # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in code_keywords):
        # ì‹¤ì œ ì½”ë“œ ì‘ì„± ìš”ì²­ì¸ì§€ í™•ì¸ (ì˜ˆ: "ì½”ë“œ ì‘ì„±", "í•¨ìˆ˜ ë§Œë“¤ì–´ì¤˜", "êµ¬í˜„í•´ì¤˜" ë“±)
        code_patterns = [
            r'ì½”ë“œ.*ì‘ì„±|ì‘ì„±.*ì½”ë“œ',
            r'í•¨ìˆ˜.*ë§Œë“¤|ë§Œë“¤.*í•¨ìˆ˜',
            r'êµ¬í˜„.*í•´|í•´.*êµ¬í˜„',
            r'ì½”ë“œ.*ë³´ì—¬|ë³´ì—¬.*ì½”ë“œ',
            r'í”„ë¡œê·¸ë¨.*ì‘ì„±|ì‘ì„±.*í”„ë¡œê·¸ë¨',
            r'íŒŒì´ì¬.*ì½”ë“œ|ì½”ë“œ.*íŒŒì´ì¬',
            r'ì•Œê³ ë¦¬ì¦˜.*êµ¬í˜„|êµ¬í˜„.*ì•Œê³ ë¦¬ì¦˜'
        ]
        if any(re.search(pattern, content_lower) for pattern in code_patterns):
            return 'code'
    
    # ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ (ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ì—…ë¡œë“œëœ ê²½ìš°ëŠ” has_imageë¡œ ì²˜ë¦¬ë¨)
    if any(keyword in content_lower for keyword in image_keywords):
        # ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        image_patterns = [
            r'ì´ë¯¸ì§€.*ë¶„ì„|ë¶„ì„.*ì´ë¯¸ì§€',
            r'ì‚¬ì§„.*ì„¤ëª…|ì„¤ëª….*ì‚¬ì§„',
            r'ê·¸ë¦¼.*ë­|ë­.*ê·¸ë¦¼',
            r'ì´ë¯¸ì§€.*ë­|ë­.*ì´ë¯¸ì§€'
        ]
        if any(re.search(pattern, content_lower) for pattern in image_patterns):
            return 'image'
    
    # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in document_keywords):
        # ë¬¸ì„œ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        document_patterns = [
            r'ë¬¸ì„œ.*ë¶„ì„|ë¶„ì„.*ë¬¸ì„œ',
            r'íŒŒì¼.*ë‚´ìš©|ë‚´ìš©.*íŒŒì¼',
            r'pdf.*ìš”ì•½|ìš”ì•½.*pdf',
            r'ë¬¸ì„œ.*ìš”ì•½|ìš”ì•½.*ë¬¸ì„œ'
        ]
        if any(re.search(pattern, content_lower) for pattern in document_patterns):
            return 'document'
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in creative_keywords):
        # ì°½ì‘ ìš”ì²­ì¸ì§€ í™•ì¸
        creative_patterns = [
            r'ê¸€.*ì“°|ì“°.*ê¸€',
            r'ì†Œì„¤.*ì‘ì„±|ì‘ì„±.*ì†Œì„¤',
            r'ì‹œ.*ì‘ì„±|ì‘ì„±.*ì‹œ',
            r'ì´ì•¼ê¸°.*ë§Œë“¤|ë§Œë“¤.*ì´ì•¼ê¸°',
            r'ì°½ì‘.*í•´|í•´.*ì°½ì‘',
            r'ì—ì„¸ì´.*ì‘ì„±|ì‘ì„±.*ì—ì„¸ì´'
        ]
        if any(re.search(pattern, content_lower) for pattern in creative_patterns):
            return 'creative'
    
    # ê¸°ë³¸ê°’: ì¼ë°˜ ì§ˆë¬¸
    return 'general'


def classify_question_type(question):
    """ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜: ì‚¬ì‹¤(Factual) vs ì˜ê²¬(Opinion)"""
    try:
        import openai
        import os
        
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        classification_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì´ "ì‚¬ì‹¤ì  ì§ˆë¬¸", "ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸", ë˜ëŠ” "ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸"ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë¶„ë¥˜ ê¸°ì¤€:
- ì‚¬ì‹¤ì  ì§ˆë¬¸: ê°ê´€ì  ì‚¬ì‹¤, ì •í™•í•œ ë‹µì´ ì¡´ì¬ (ì˜ˆ: ì„¤ë¦½ì—°ë„, ìœ„ì¹˜, ì—­ì‚¬ì  ì‚¬ì‹¤)
- ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸: ì£¼ê´€ì  í‰ê°€, ì¶”ì²œ, ì„ í˜¸ë„ (ì˜ˆ: ë§›ì§‘ ì¶”ì²œ, ì¢‹ì€ ì¹´í˜, ìµœê³ ì˜ ì œí’ˆ)
- ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸: ì½”ë“œ ì‘ì„±, í”„ë¡œê·¸ë˜ë° ì˜ˆì œ, ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ìš”ì²­ (ì˜ˆ: "ë³„ì°ê¸° ì½”ë“œ", "íŒŒì´ì¬ìœ¼ë¡œ ì‘ì„±", "í•¨ìˆ˜ ë§Œë“¤ì–´ì¤˜")

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "type": "factual" ë˜ëŠ” "opinion" ë˜ëŠ” "code",
  "confidence": 0.0-1.0,
  "reason": "ë¶„ë¥˜ ì´ìœ "
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": classification_prompt}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        print(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: {result['type']} (ì‹ ë¢°ë„: {result['confidence']})")
        print(f"   ì´ìœ : {result['reason']}")
        
        return result['type']
        
    except Exception as e:
        print(f"âš ï¸ ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 'factual' ì‚¬ìš©")
        return "factual"


def collect_multi_llm_responses(user_message, judge_model="GPT-4o", selected_models=None, question_type=None):
    """1ë‹¨ê³„: ì„ íƒëœ LLMë“¤ì—ê²Œ ë³‘ë ¬ ì§ˆì˜ í›„ ì‹¬íŒ ëª¨ë¸ë¡œ ê²€ì¦"""
    import asyncio
    import aiohttp
    import json
    import time
    
    responses = {}
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—”ë“œí¬ì¸íŠ¸ë“¤ (ëª…ì‹œì  ëª¨ë¸ëª… ì‚¬ìš©)
    all_llm_endpoints = {
        # GPT ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'GPT-5': 'http://localhost:8000/chat/gpt-5/',
        'GPT-5-Mini': 'http://localhost:8000/chat/gpt-5-mini/',
        'GPT-4.1': 'http://localhost:8000/chat/gpt-4.1/',
        'GPT-4.1-Mini': 'http://localhost:8000/chat/gpt-4.1-mini/',
        'GPT-4o': 'http://localhost:8000/chat/gpt-4o/',
        'GPT-4o-Mini': 'http://localhost:8000/chat/gpt-4o-mini/',
        'GPT-4-Turbo': 'http://localhost:8000/chat/gpt-4-turbo/',
        'GPT-3.5-Turbo': 'http://localhost:8000/chat/gpt-3.5-turbo/',
        
        # Gemini ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'Gemini-2.5-Pro': 'http://localhost:8000/chat/gemini-2.5-pro/',
        'Gemini-2.5-Flash': 'http://localhost:8000/chat/gemini-2.5-flash/',
        'Gemini-2.0-Flash-Exp': 'http://localhost:8000/chat/gemini-2.0-flash-exp/',
        'Gemini-2.0-Flash-Lite': 'http://localhost:8000/chat/gemini-2.0-flash-lite/',
        
        # Claude ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'Claude-4-Opus': 'http://localhost:8000/chat/claude-4-opus/',
        'Claude-3.7-Sonnet': 'http://localhost:8000/chat/claude-3.7-sonnet/',
        'Claude-3.5-Sonnet': 'http://localhost:8000/chat/claude-3.5-sonnet/',
        'Claude-3.5-Haiku': 'http://localhost:8000/chat/claude-3.5-haiku/',
        'Claude-3-Opus': 'http://localhost:8000/chat/claude-3-opus/',
        
        # HyperCLOVA X ëª¨ë¸ë“¤
        'HCX-003': 'http://localhost:8000/chat/clova-hcx-003/',
        'HCX-DASH-001': 'http://localhost:8000/chat/clova-hcx-dash-001/',
    }
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§ (ê¸°ë³¸ê°’: ëª¨ë“  ëª¨ë¸)
    if selected_models:
        print(f"ğŸ“‹ selected_models ì…ë ¥: {selected_models} (íƒ€ì…: {type(selected_models)})")
        # ì„ íƒëœ ëª¨ë¸ëª…ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        model_mapping = {
            # GPT ëª¨ë¸ë“¤
            'gpt-5': 'GPT-5',
            'gpt-5-mini': 'GPT-5-Mini',
            'gpt-4.1': 'GPT-4.1',
            'gpt-4.1-mini': 'GPT-4.1-Mini',
            'gpt-4o': 'GPT-4o',
            'gpt-4o-mini': 'GPT-4o-Mini',
            'gpt-4-turbo': 'GPT-4-Turbo',
            'gpt-3.5-turbo': 'GPT-3.5-Turbo',
            
            # Gemini ëª¨ë¸ë“¤
            'gemini-2.5-pro': 'Gemini-2.5-Pro',
            'gemini-2.5-flash': 'Gemini-2.5-Flash',
            'gemini-2.0-flash-exp': 'Gemini-2.0-Flash-Exp',
            'gemini-2.0-flash-lite': 'Gemini-2.0-Flash-Lite',
            
            # Claude ëª¨ë¸ë“¤
            'claude-4-opus': 'Claude-4-Opus',
            'claude-3.7-sonnet': 'Claude-3.7-Sonnet',
            'claude-3.5-sonnet': 'Claude-3.5-Sonnet',
            'claude-3.5-haiku': 'Claude-3.5-Haiku',
            'claude-3-opus': 'Claude-3-Opus',
            
            # HyperCLOVA X ëª¨ë¸ë“¤
            'clova-hcx-003': 'HCX-003',
            'clova-hcx-dash-001': 'HCX-DASH-001',
        }
        
        selected_standard_models = []
        for model in selected_models:
            model_lower = model.lower() if isinstance(model, str) else str(model).lower()
            if model_lower in model_mapping:
                selected_standard_models.append(model_mapping[model_lower])
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ëª…: {model}")
        
        # ì„ íƒëœ ëª¨ë¸ë“¤ì˜ ì—”ë“œí¬ì¸íŠ¸ë§Œ ì‚¬ìš©
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in selected_standard_models}
        print(f"ğŸ“‹ ë§¤í•‘ëœ í‘œì¤€ ëª¨ë¸: {selected_standard_models}")
    else:
        # ì„ íƒëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ 3ê°œ ì‚¬ìš© (ë¹„ìš© ì ˆê°)
        print(f"âš ï¸ selected_modelsê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ 3ê°œ ì‚¬ìš©")
        default_models = ['GPT-4o-Mini', 'Gemini-2.0-Flash-Lite', 'Claude-3.5-Haiku']
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in default_models}
    
    if not llm_endpoints:
        print(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—”ë“œí¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. selected_modelsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    print(f"ğŸ¯ ì„ íƒëœ LLM ëª¨ë¸ë“¤: {list(llm_endpoints.keys())} (ì´ {len(llm_endpoints)}ê°œ)")
    
    async def fetch_response(session, ai_name, endpoint):
        """ê°œë³„ LLMì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°"""
        try:
            payload = {
                'message': user_message,
                'user_id': 'system'
            }
            
            print(f"ğŸ”„ {ai_name} ëª¨ë¸ì— ìš”ì²­ ì „ì†¡ ì¤‘... (ì—”ë“œí¬ì¸íŠ¸: {endpoint})")
            async with session.post(endpoint, json=payload, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    response_content = result.get('response', 'ì‘ë‹µ ì—†ìŒ')
                    print(f"âœ… {ai_name} ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(str(response_content))}ì")
                    print(f"ğŸ“„ {ai_name} ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 200ì): {str(response_content)[:200]}...")
                    return ai_name, response_content
                else:
                    # HTTP ìƒíƒœ ì½”ë“œ ì˜¤ë¥˜ë¥¼ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
                    error_text = await response.text()
                    print(f"âŒ {ai_name} HTTP ì˜¤ë¥˜: {response.status}, ë‚´ìš©: {error_text[:200]}")
                    error_msg = Exception(f"HTTP {response.status}: {error_text}")
                    friendly_msg = get_user_friendly_error_message(error_msg)
                    return ai_name, friendly_msg
        except Exception as e:
            # ì˜ˆì™¸ë¥¼ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
            print(f"âŒ {ai_name} ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
            friendly_msg = get_user_friendly_error_message(e)
            return ai_name, friendly_msg
    
    async def collect_all_responses():
        """ëª¨ë“  LLMì—ì„œ ë™ì‹œì— ì‘ë‹µ ìˆ˜ì§‘"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for ai_name, endpoint in llm_endpoints.items():
                task = fetch_response(session, ai_name, endpoint)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, tuple):
                    ai_name, response = result
                    responses[ai_name] = response
                elif isinstance(result, Exception):
                    print(f"LLM ì‘ë‹µ ìˆ˜ì§‘ ì˜¤ë¥˜: {result}")
    
    try:
        # ë¹„ë™ê¸° ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(collect_all_responses())
        loop.close()
        
        print(f"âœ… {len(responses)}ê°œ LLMì—ì„œ ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ: {list(responses.keys())}")
        
        # ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§ (íƒ€ì„ì•„ì›ƒ/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±)
        # get_user_friendly_error_messageê°€ ë°˜í™˜í•˜ëŠ” ì •í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´
        error_patterns = [
            "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "ëª¨ë¸ ì‚¬ìš©ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
            "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”",
            "ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ëŒ€í™” ê¸¸ì´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤",
            "ì½˜í…ì¸  ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"
        ]
        
        valid_responses = {}
        error_responses = {}
        
        for ai_name, response in responses.items():
            response_str = str(response)
            # ì •í™•í•œ ì—ëŸ¬ íŒ¨í„´ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ì´ ì•„ë‹Œ ì „ì²´ ë©”ì‹œì§€ í™•ì¸)
            is_error = any(pattern in response_str for pattern in error_patterns)
            
            # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê³  ì—ëŸ¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë©´ ì—ëŸ¬ë¡œ ê°„ì£¼
            if len(response_str) < 50 and any(keyword in response_str.lower() for keyword in ["timeout", "connection", "error", "ì˜¤ë¥˜", "ì‹¤íŒ¨"]):
                is_error = True
            
            if is_error:
                error_responses[ai_name] = response
                print(f"âš ï¸ {ai_name} ì‘ë‹µì´ ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ê°ì§€ë¨: {response_str[:100]}...")
            else:
                valid_responses[ai_name] = response
                print(f"âœ… {ai_name} ìœ íš¨í•œ ì‘ë‹µ: {len(response_str)}ì")
        
        print(f"ğŸ“Š ìœ íš¨í•œ ì‘ë‹µ: {len(valid_responses)}ê°œ, ì—ëŸ¬ ì‘ë‹µ: {len(error_responses)}ê°œ")
        
        # ìœ íš¨í•œ ì‘ë‹µì´ ì—†ìœ¼ë©´ ì—ëŸ¬
        if not valid_responses:
            if error_responses:
                error_summary = ", ".join([f"{name}: {msg[:50]}..." for name, msg in list(error_responses.items())[:3]])
                raise ValueError(f"ëª¨ë“  LLM ìš”ì²­ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({error_summary})")
            else:
                print(f"âŒ ìˆ˜ì§‘ëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤!")
                raise ValueError("LLMì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ìœ íš¨í•œ ì‘ë‹µë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì  ë‹µë³€ ìƒì„±
        print(f"âš–ï¸ ì‹¬íŒ ëª¨ë¸({judge_model})ë¡œ ê²€ì¦ ë° ìµœì  ë‹µë³€ ìƒì„± ì‹œì‘... (ìœ íš¨í•œ ì‘ë‹µ {len(valid_responses)}ê°œ ì‚¬ìš©)")
        print(f"ğŸ“‹ ì§ˆë¬¸ ìœ í˜•: {question_type}")
        final_result = judge_and_generate_optimal_response(valid_responses, user_message, judge_model, question_type=question_type)
        print(f"âœ… ìµœì  ë‹µë³€ ìƒì„± ì™„ë£Œ: {type(final_result)}, í‚¤: {list(final_result.keys()) if isinstance(final_result, dict) else 'N/A'}")
        return final_result
        
    except Exception as e:
        print(f"âŒ LLM ì‘ë‹µ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ì‘ë‹µë“¤
        fallback_responses = {
            'GPT-3.5-turbo': f'GPT ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.',
            'Claude-3.5-haiku': f'Claude ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.',
            'Llama-3.1-8b': f'Llama ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.'
        }
        return judge_and_generate_optimal_response(fallback_responses, user_message, judge_model)


def detect_conflicts_in_responses(llm_responses):
    """LLM ì‘ë‹µì—ì„œ ìƒí˜¸ëª¨ìˆœ ê°ì§€ (í•˜ë“œì½”ë”© ì—†ì´ ë²”ìš©ì )"""
    import re
    from collections import defaultdict
    
    conflicts = {
        "dates": defaultdict(list),
        "locations": defaultdict(list), 
        "numbers": defaultdict(list),
        "general_facts": defaultdict(list)
    }
    
    # ê° LLM ì‘ë‹µì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    for model_name, response in llm_responses.items():
        # ì—°ë„ íŒ¨í„´ ì¶”ì¶œ (4ìë¦¬ ìˆ«ì, 1900-2024 ë²”ìœ„)
        year_pattern = r'(\d{4})'
        year_matches = re.findall(year_pattern, response)
        
        for year_str in year_matches:
            try:
                year = int(year_str)
                if 1900 <= year <= 2024:  # í•©ë¦¬ì ì¸ ì—°ë„ ë²”ìœ„
                    conflicts["dates"][year_str].append(model_name)
            except ValueError:
                continue
        
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (ì‹œ/ë„/êµ¬/êµ° íŒ¨í„´)
        locations = re.findall(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)', response)
        for location in locations:
            conflicts["locations"][location].append(model_name)
        
        # ìˆ˜ì¹˜ ì •ë³´ ì¶”ì¶œ (ë‹¨ìœ„ í¬í•¨, ì—°ë„ ì œì™¸)
        numbers = re.findall(r'\d+(?:ëª…|ê°œ|ì›”|ì¼|ì–µ|ë§Œ|ì²œ)', response)
        for number in numbers:
            conflicts["numbers"][number].append(model_name)
    
    # ìƒí˜¸ëª¨ìˆœ í•„í„°ë§ (2ê°œ ì´ìƒ ë‹¤ë¥¸ ê°’ì´ ìˆì„ ë•Œë§Œ)
    detected_conflicts = {}
    
    for category, items in conflicts.items():
        if len(items) > 1:  # ì„œë¡œ ë‹¤ë¥¸ ê°’ì´ 2ê°œ ì´ìƒ
            detected_conflicts[category] = dict(items)
    
    return detected_conflicts


def extract_sentences_from_response(response_text):
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í¬í•¨)"""
    import re
    
    sentences = []
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ (```ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„)
    code_blocks = re.findall(r'```[\s\S]*?```', response_text)
    for code_block in code_blocks:
        sentences.append(code_block.strip())
    
    # 2. ì½”ë“œ ë¸”ë¡ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ì¶”ì¶œ
    text_without_code = re.sub(r'```[\s\S]*?```', '', response_text)
    
    # 3. ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
    text_sentences = re.split(r'[.!?]\s+', text_without_code)
    for sentence in text_sentences:
        sentence = sentence.strip()
        if len(sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            sentences.append(sentence)
    
    return sentences


def judge_and_generate_optimal_response(llm_responses, user_question, judge_model="GPT-5", question_type=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ: LLM ë¹„êµ + ì„ íƒì  ì›¹ ê²€ì¦ + ë‹¤ìˆ˜ê²°"""
    try:
        print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œì‘: {user_question}")
        print(f"ğŸ“‹ judge_and_generate_optimal_responseì— ì „ë‹¬ëœ llm_responses í‚¤: {list(llm_responses.keys()) if llm_responses else 'None'}")
        
        # 0ë‹¨ê³„: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ì „ë‹¬ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìë™ ë¶„ë¥˜)
        if question_type is None:
            question_type = classify_question_type(user_question)
        else:
            print(f"ğŸ“‹ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ ìœ í˜•: {question_type}")
        
        # 1ë‹¨ê³„: ê° AI ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        print(f"ğŸ“ ê° AI ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• ...")
        llm_sentences = {}
        for model_name, response in llm_responses.items():
            sentences = extract_sentences_from_response(response)
            llm_sentences[model_name] = sentences
            print(f"  - {model_name}: {len(sentences)}ê°œ ë¬¸ì¥ ì¶”ì¶œ")
        
        # 2ë‹¨ê³„: ìƒí˜¸ëª¨ìˆœ ê°ì§€
        conflicts = detect_conflicts_in_responses(llm_responses)
        print(f"ğŸ“Š ê°ì§€ëœ ìƒí˜¸ëª¨ìˆœ: {conflicts}")
        
        # 3ë‹¨ê³„: ì‹¬íŒ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        model_sections = []
        for model_name, response in llm_responses.items():
            model_sections.append(f"[{model_name} ë‹µë³€]\n{response}")
        
        model_responses_text = "\n\n".join(model_sections)
        
        # ê° AIì˜ ë¬¸ì¥ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        sentences_sections = []
        for model_name, sentences in llm_sentences.items():
            sentences_list = "\n".join([f"  {i+1}. {s[:100]}..." if len(s) > 100 else f"  {i+1}. {s}" for i, s in enumerate(sentences)])
            sentences_sections.append(f"[{model_name} ë¬¸ì¥ ëª©ë¡]\n{sentences_list}")
        
        sentences_text = "\n\n".join(sentences_sections)
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ (ì½”ë“œ ì§ˆë¬¸ì€ ê°„ë‹¨í•˜ê²Œ)
        if question_type == "code":
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ì œê³µëœ AI ì½”ë“œ ë‹µë³€ë“¤:**
{model_responses_text}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ ê·œì¹™:**
1. **ë°˜ë“œì‹œ ìœ„ AI ë‹µë³€ì˜ ì‹¤ì œ ì½”ë“œë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ì½”ë“œ ì‘ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ì—¬ëŸ¬ AIì˜ ì½”ë“œë¥¼ ì¡°í•©í•˜ì—¬ ìµœì í™”** - ë‹¨ì¼ AI ì½”ë“œ ë³µì‚¬ ê¸ˆì§€
3. **ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í˜•ì‹ ìœ ì§€** (```python ... ```)
4. **ê° AIì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬**í•˜ì—¬ adopted_info/rejected_info ì‘ì„±

**verification_results ì‘ì„± ê·œì¹™:**
- **adopted_info**: í•´ë‹¹ AIê°€ ì œê³µí•œ ì½”ë“œ ì¤‘ **ì‹¤ì œë¡œ ì‚¬ìš©ëœ ë¶€ë¶„**ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **rejected_info**: í•´ë‹¹ AIê°€ ì œê³µí•œ ì½”ë“œ ì¤‘ **ì‚¬ìš©ë˜ì§€ ì•Šì€ ë¶€ë¶„**ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **ë°˜ë“œì‹œ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì§ì ‘ ë³µì‚¬**í•´ì•¼ í•¨

JSON ì‘ë‹µ:
{{
  "optimal_answer": "ì—¬ëŸ¬ AI ì½”ë“œë¥¼ ì¡°í•©í•œ ìµœì  ì½”ë“œ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)",
  "verification_results": {{
    "AIëª¨ë¸ëª…": {{
      "accuracy": "ì •í™•ì„±",
      "errors": "ì˜¤ë¥˜ ì„¤ëª…",
      "confidence": "ì‹ ë¢°ë„",
      "adopted_info": ["ì‹¤ì œ ì‚¬ìš©ëœ ì½”ë“œ ë¶€ë¶„"],
      "rejected_info": ["ì‚¬ìš©ë˜ì§€ ì•Šì€ ì½”ë“œ ë¶€ë¶„"]
    }}
  }},
  "confidence_score": "ì‹ ë¢°ë„ 0-100",
  "contradictions_detected": [],
  "fact_verification": {{}},
  "analysis_rationale": "ì¡°í•© ê·¼ê±°"
}}
"""
        else:
            # ì¼ë°˜ ì§ˆë¬¸ìš© í”„ë¡¬í”„íŠ¸ (êµì§‘í•© ê¸°ë°˜)
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ì œê³µëœ AI ë‹µë³€ë“¤:**
{model_responses_text}

**ê° AIì˜ ë¬¸ì¥ ëª©ë¡ (ì„ íƒ ê°€ëŠ¥í•œ ë¬¸ì¥ë“¤):**
{sentences_text}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ í•µì‹¬ ê·œì¹™:**
1. **ìœ„ ë¬¸ì¥ ëª©ë¡ì˜ ë¬¸ì¥ë“¤ë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ë¬¸ì¥ ì‘ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ê° AIê°€ ì‹¤ì œë¡œ ë§í•œ ë¬¸ì¥ë§Œ ì±„íƒ/ì œì™¸ ê°€ëŠ¥**
3. **ì—¬ëŸ¬ AIì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜ì˜¨ ì •ë³´ ìš°ì„  ì±„íƒ** (êµì§‘í•© ê¸°ë°˜)
4. **í•œ AIë§Œ ì–¸ê¸‰í•œ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ê²€í† **
5. **ì ˆëŒ€ ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ë¡ í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”**

**optimal_answer ì‘ì„± ë°©ë²•:**
- ìœ„ ë¬¸ì¥ ëª©ë¡ì—ì„œ **ì‹¤ì œ ë¬¸ì¥ì„ ì„ íƒ**í•˜ì—¬ ì¡°í•©
- **ì—¬ëŸ¬ AIê°€ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰í•œ ì •ë³´** ìš°ì„  ì„ íƒ
- ì„ íƒí•œ ë¬¸ì¥ì€ **ì›ë¬¸ ê·¸ëŒ€ë¡œ** ì‚¬ìš© (ìˆ˜ì •/ìš”ì•½ ê¸ˆì§€)
- ë¬¸ì¥ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° (ë¬¸ì¥ ìˆœì„œ ì¡°ì • ê°€ëŠ¥)

**verification_results ì‘ì„± ê·œì¹™:**
- **adopted_info**: í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì—ì„œ **ì‹¤ì œë¡œ ì±„íƒëœ ë¬¸ì¥**ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **rejected_info**: í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì—ì„œ **ì œì™¸ëœ ë¬¸ì¥**ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **ë°˜ë“œì‹œ í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì— ìˆëŠ” ë¬¸ì¥ë§Œ ì‚¬ìš©**
- **ë‹¤ë¥¸ AIì˜ ë¬¸ì¥ì„ í•´ë‹¹ AIì˜ ì±„íƒ/ì œì™¸ ì •ë³´ì— í¬í•¨í•˜ë©´ ì•ˆë¨**

**ì˜ˆì‹œ:**
GPT-4oê°€ "ì¶©ë¶ëŒ€í•™êµëŠ” 1951ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."ë¼ê³  ë§í–ˆë‹¤ë©´
â†’ adopted_info: ["ì¶©ë¶ëŒ€í•™êµëŠ” 1951ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."]

Geminiê°€ "ì¶©ë¶ëŒ€í•™êµëŠ” 1946ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."ë¼ê³  ë§í–ˆë‹¤ë©´
â†’ rejected_info: ["ì¶©ë¶ëŒ€í•™êµëŠ” 1946ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."]

**âŒ ì ˆëŒ€ ê¸ˆì§€:**
- ìœ„ ë¬¸ì¥ ëª©ë¡ì— ì—†ëŠ” ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±
- ì—¬ëŸ¬ ë¬¸ì¥ì„ ìš”ì•½í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì¥ ë§Œë“¤ê¸°
- AIê°€ ë§í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ í•´ë‹¹ AIì˜ ì±„íƒ/ì œì™¸ ì •ë³´ì— í¬í•¨
- ë‹¤ë¥¸ AIì˜ ë¬¸ì¥ì„ í•´ë‹¹ AIì˜ ì±„íƒ/ì œì™¸ ì •ë³´ì— í¬í•¨

JSON ì‘ë‹µ:
{{
  "optimal_answer": "ìœ„ ë¬¸ì¥ ëª©ë¡ì—ì„œ ì„ íƒí•œ ë¬¸ì¥ë“¤ì„ ì¡°í•©í•œ ë‹µë³€",
  "verification_results": {{
    "AIëª¨ë¸ëª…": {{
      "accuracy": "ì •í™•ì„±",
      "errors": "ì˜¤ë¥˜ ì„¤ëª…",
      "confidence": "ì‹ ë¢°ë„",
      "adopted_info": ["í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì—ì„œ ì±„íƒëœ ì›ë¬¸"],
      "rejected_info": ["í•´ë‹¹ AIì˜ ë¬¸ì¥ ëª©ë¡ì—ì„œ ì œì™¸ëœ ì›ë¬¸"]
    }}
  }},
  "confidence_score": "ì‹ ë¢°ë„ 0-100",
  "contradictions_detected": ["ìƒí˜¸ëª¨ìˆœ ì‚¬í•­"],
  "fact_verification": {{"dates": [], "locations": [], "facts": []}},
  "analysis_rationale": "ë¬¸ì¥ ì„ íƒ ê·¼ê±°"
}}
"""
        
        # ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ
        print(f"ğŸ“ ì‹¬íŒ ëª¨ë¸({judge_model}) í˜¸ì¶œ ì‹œì‘...")
        judge_response = call_judge_model(judge_model, judge_prompt)
        print(f"âœ… ì‹¬íŒ ëª¨ë¸ ì‘ë‹µ ë°›ìŒ: {len(judge_response) if judge_response else 0}ì")
        
        # ê²°ê³¼ íŒŒì‹±
        print(f"ğŸ“ ì‹¬íŒ ëª¨ë¸ ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
        parsed_result = parse_judge_response(judge_response, judge_model, llm_responses, llm_sentences)
        print(f"âœ… íŒŒì‹± ì™„ë£Œ")
        
        return parsed_result
        
    except Exception as e:
        print(f"âŒ ì‹¬íŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        
        # í´ë°±: ê°€ì¥ ê¸´ ì‘ë‹µì„ ìµœì  ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
        if llm_responses:
            longest_response = max(llm_responses.values(), key=len)
            return {
                "ìµœì ì˜_ë‹µë³€": longest_response,
                "llm_ê²€ì¦_ê²°ê³¼": {
                    model: {
                        "ì •í™•ì„±": "âŒ",
                        "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨ - Judge ëª¨ë¸ ì˜¤ë¥˜",
                        "ì‹ ë¢°ë„": "0",
                        "ì±„íƒëœ_ì •ë³´": [],
                        "ì œì™¸ëœ_ì •ë³´": []
                    }
                    for model in llm_responses.keys()
                },
                "ì‹¬íŒëª¨ë¸": judge_model,
                "ìƒíƒœ": "ê²€ì¦ ì‹¤íŒ¨",
                "ì›ë³¸_ì‘ë‹µ": llm_responses
            }
        return {
            "ìµœì ì˜_ë‹µë³€": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "llm_ê²€ì¦_ê²°ê³¼": {},
            "ì‹¬íŒëª¨ë¸": judge_model,
            "ìƒíƒœ": "ì˜¤ë¥˜",
            "ì›ë³¸_ì‘ë‹µ": llm_responses or {}
        }


def call_judge_model(model_name, prompt):
    """ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ"""
    try:
        if model_name in ['GPT-5', 'GPT-3.5-turbo', 'GPT-4', 'GPT-4o', 'GPT-4o-mini']:
            # OpenAI ëª¨ë¸ ì‚¬ìš©
            import openai
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            
            client = openai.OpenAI(api_key=openai_api_key)
            
            # ëª¨ë¸ëª…ì„ OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            openai_model_name = model_name.lower().replace('-', '-')
            if model_name == 'GPT-5':
                openai_model_name = 'gpt-5'
            elif model_name == 'GPT-4':
                openai_model_name = 'gpt-4'
            elif model_name == 'GPT-4o':
                openai_model_name = 'gpt-4o'
            elif model_name == 'GPT-4o-mini':
                openai_model_name = 'gpt-4o-mini'
            elif model_name == 'GPT-3.5-turbo':
                openai_model_name = 'gpt-3.5-turbo'
            
            # ìµœì‹  OpenAI ëª¨ë¸(o1, o3 ë“±)ì€ max_completion_tokens ì‚¬ìš© ë° temperature ë¯¸ì§€ì›
            is_latest_model = any(model in openai_model_name.lower() for model in ['o1', 'o3', 'gpt-5'])
            
            api_params = {
                "model": openai_model_name,
                "messages": [
                    {"role": "system", "content": """ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—­í• ì€ ê° AIì˜ ë‹µë³€ì„ **ìˆëŠ” ê·¸ëŒ€ë¡œ ë¶„ì„**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸš¨ ì ˆëŒ€ ê·œì¹™:
1. **ê° AIê°€ ì‹¤ì œë¡œ ë§í•œ ë¬¸ì¥ë§Œ** adopted_info/rejected_infoì— ë³µì‚¬
2. **ì ˆëŒ€ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”** - í™˜ê°(hallucination) ê¸ˆì§€!
3. **ê° AIì˜ ë¬¸ì¥ì€ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤**
4. **ë‹¤ë¥¸ AIì˜ ë¬¸ì¥ì„ ë³µì‚¬í•˜ë©´ ì•ˆë©ë‹ˆë‹¤**
5. **ì—¬ëŸ¬ AIê°€ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰í•œ ì •ë³´ë¥¼ ìš°ì„  ì±„íƒ**

âœ… ì˜¬ë°”ë¥¸ ë¶„ì„:
- ê° AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬
- ì—¬ëŸ¬ AIê°€ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰í•œ ì •ë³´ ìš°ì„  ì„ íƒ
- í•œ AIë§Œ ì–¸ê¸‰í•œ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ê²€í† 

âŒ ì˜ëª»ëœ ë¶„ì„ (í™˜ê°):
- ì›ë³¸ ë‹µë³€ì— ì—†ëŠ” ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±
- ì—¬ëŸ¬ ë¬¸ì¥ì„ ìš”ì•½í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì¥ ë§Œë“¤ê¸°
- ë‹¤ë¥¸ AIì˜ ë¬¸ì¥ì„ í•´ë‹¹ AIì˜ ì±„íƒ/ì œì™¸ ì •ë³´ì— í¬í•¨
- AIê°€ ë§í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ê¸°

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""},
                    {"role": "user", "content": prompt}
                ],
            }
            
            if not is_latest_model:
                api_params["temperature"] = 0.0
            
            completion_limit = get_openai_completion_limit(openai_model_name)
            if is_latest_model:
                api_params["max_completion_tokens"] = completion_limit
            else:
                api_params["max_tokens"] = completion_limit
                api_params["response_format"] = {"type": "json_object"}
            
            response = client.chat.completions.create(**api_params)
            response_content = response.choices[0].message.content.strip()
            
            if response.choices[0].finish_reason == 'length':
                print(f"âš ï¸ {model_name} ì‘ë‹µì´ í† í° ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤")
                response_content += "\n\n[ì‘ë‹µì´ í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤.]"
            
            return response_content
            
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ GPT-5 ì‚¬ìš©
            return call_judge_model('GPT-5', prompt)
            
    except Exception as e:
        print(f"âŒ ì‹¬íŒ ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        raise e


def parse_judge_response(judge_response, judge_model, llm_responses=None, llm_sentences=None):
    """ì‹¬íŒ ëª¨ë¸ JSON ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦"""
    try:
        import json
        import re
        
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', judge_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                parsed_data = json.loads(json_str)
                print(f"âœ… JSON íŒŒì‹± ì„±ê³µ!")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return create_fallback_result(judge_model, llm_responses)
            
            result = {
                "ìµœì ì˜_ë‹µë³€": parsed_data.get("optimal_answer", ""),
                "llm_ê²€ì¦_ê²°ê³¼": {},
                "ì‹¬íŒëª¨ë¸": judge_model,
                "ìƒíƒœ": "ì„±ê³µ",
                "ì‹ ë¢°ë„": parsed_data.get("confidence_score", "50"),
                "ìƒí˜¸ëª¨ìˆœ": parsed_data.get("contradictions_detected", []),
                "ì‚¬ì‹¤ê²€ì¦": parsed_data.get("fact_verification", {}),
                "ë¶„ì„_ê·¼ê±°": parsed_data.get("analysis_rationale", "")
            }
            
            # ê²€ì¦ ê²°ê³¼ íŒŒì‹± ë° ê²€ì¦
            verification_results = parsed_data.get("verification_results", {})
            
            for model_name, verification in verification_results.items():
                adopted_info = verification.get("adopted_info", [])
                rejected_info = verification.get("rejected_info", [])
                
                # ğŸš¨ í™˜ê° ê²€ì¦: ê° AIì˜ ì‹¤ì œ ì‘ë‹µê³¼ ëŒ€ì¡°
                if llm_responses and model_name in llm_responses:
                    original_response = llm_responses[model_name]
                    
                    # adopted_info ê²€ì¦
                    validated_adopted = []
                    for item in adopted_info:
                        if isinstance(item, str) and item.strip():
                            # ì‹¤ì œ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ìœ ì‚¬ë„ ê²€ì‚¬)
                            item_normalized = re.sub(r'\s+', ' ', item.strip().lower())
                            response_normalized = re.sub(r'\s+', ' ', original_response.lower())
                            
                            # ê¸´ ë¬¸ì¥ì€ ë¶€ë¶„ ë§¤ì¹­ í—ˆìš© (80% ì´ìƒ)
                            if len(item_normalized) > 50:
                                words = item_normalized.split()
                                match_count = sum(1 for word in words if word in response_normalized)
                                if match_count / len(words) >= 0.8:
                                    validated_adopted.append(item.strip())
                                else:
                                    print(f"âš ï¸ {model_name} adopted_info í™˜ê° ê°ì§€: {item[:50]}...")
                            else:
                                # ì§§ì€ ë¬¸ì¥ì€ ì •í™•í•œ ë§¤ì¹­ ìš”êµ¬
                                if item_normalized in response_normalized:
                                    validated_adopted.append(item.strip())
                                else:
                                    print(f"âš ï¸ {model_name} adopted_info í™˜ê° ê°ì§€: {item[:50]}...")
                    
                    # rejected_info ê²€ì¦
                    validated_rejected = []
                    for item in rejected_info:
                        if isinstance(item, str) and item.strip():
                            item_normalized = re.sub(r'\s+', ' ', item.strip().lower())
                            response_normalized = re.sub(r'\s+', ' ', original_response.lower())
                            
                            if len(item_normalized) > 50:
                                words = item_normalized.split()
                                match_count = sum(1 for word in words if word in response_normalized)
                                if match_count / len(words) >= 0.8:
                                    validated_rejected.append(item.strip())
                                else:
                                    print(f"âš ï¸ {model_name} rejected_info í™˜ê° ê°ì§€: {item[:50]}...")
                            else:
                                if item_normalized in response_normalized:
                                    validated_rejected.append(item.strip())
                                else:
                                    print(f"âš ï¸ {model_name} rejected_info í™˜ê° ê°ì§€: {item[:50]}...")
                    
                    # ê²€ì¦ëœ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    adopted_info = validated_adopted
                    rejected_info = validated_rejected
                    
                    # ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‘ë‹µì—ì„œ ìë™ ì¶”ì¶œ
                    if not adopted_info and not rejected_info:
                        print(f"âš ï¸ {model_name}: ê²€ì¦ í›„ ì±„íƒ/ì œì™¸ ì •ë³´ê°€ ëª¨ë‘ ë¹„ì–´ìˆìŒ. ì›ë³¸ì—ì„œ ì¶”ì¶œ...")
                        sentences = extract_sentences_from_response(original_response)
                        adopted_info = sentences[:3] if sentences else []
                
                print(f"ğŸ“Š {model_name}: ê²€ì¦ í›„ adopted={len(adopted_info)}ê°œ, rejected={len(rejected_info)}ê°œ")
                
                result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                    "ì •í™•ì„±": verification.get("accuracy", "ì •í™•"),
                    "ì˜¤ë¥˜": verification.get("errors", "ì—†ìŒ"),
                    "ì‹ ë¢°ë„": verification.get("confidence", "50"),
                    "ì±„íƒëœ_ì •ë³´": adopted_info,
                    "ì œì™¸ëœ_ì •ë³´": rejected_info
                }
            
            # ëˆ„ë½ëœ ëª¨ë¸ ì²˜ë¦¬
            if llm_responses:
                for model_name in llm_responses.keys():
                    if model_name not in result["llm_ê²€ì¦_ê²°ê³¼"]:
                        print(f"âš ï¸ {model_name}: Judge ê²°ê³¼ ëˆ„ë½. ê¸°ë³¸ ì •ë³´ ìƒì„±...")
                        sentences = extract_sentences_from_response(llm_responses[model_name])
                        result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                            "ì •í™•ì„±": "âœ…",
                            "ì˜¤ë¥˜": "ì •í™•í•œ ì •ë³´ ì œê³µ",
                            "ì‹ ë¢°ë„": "50",
                            "ì±„íƒëœ_ì •ë³´": sentences[:3] if sentences else [],
                            "ì œì™¸ëœ_ì •ë³´": []
                        }
                
                result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
            
            return result
        else:
            return create_fallback_result(judge_model, llm_responses)
            
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return create_fallback_result(judge_model, llm_responses)


def create_fallback_result(judge_model, llm_responses=None):
    """í´ë°± ê²°ê³¼ ìƒì„±"""
    if llm_responses:
        actual_models = list(llm_responses.keys())
    else:
        actual_models = []
    
    result = {
        "ìµœì ì˜_ë‹µë³€": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "llm_ê²€ì¦_ê²°ê³¼": {},
        "ì‹¬íŒëª¨ë¸": judge_model,
        "ìƒíƒœ": "íŒŒì‹± ì‹¤íŒ¨",
        "ì‹ ë¢°ë„": "0",
        "ìƒí˜¸ëª¨ìˆœ": [],
        "ì‚¬ì‹¤ê²€ì¦": {}
    }
    
    for model in actual_models:
        adopted_info = []
        if llm_responses and model in llm_responses:
            sentences = extract_sentences_from_response(llm_responses[model])
            adopted_info = sentences[:3] if sentences else []
        
        result["llm_ê²€ì¦_ê²°ê³¼"][model] = {
            "ì •í™•ì„±": "âŒ",
            "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨ - Judge ëª¨ë¸ ì˜¤ë¥˜",
            "ì‹ ë¢°ë„": "0",
            "ì±„íƒëœ_ì •ë³´": adopted_info,
            "ì œì™¸ëœ_ì •ë³´": []
        }
    
    if llm_responses:
        result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
    
    return result


def format_optimal_response(final_result):
    """ìµœì  ë‹µë³€ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    try:
        print(f"ğŸ” format_optimal_response ì‹œì‘...")
        
        optimal_answer = final_result.get("ìµœì ì˜_ë‹µë³€", "")
        verification_results = final_result.get("llm_ê²€ì¦_ê²°ê³¼", {})
        
        # ìµœì  ë‹µë³€ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²´í¬
        if not optimal_answer or len(optimal_answer.strip()) == 0:
            print(f"âš ï¸ ìµœì  ë‹µë³€ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! í´ë°± ë©”ì‹œì§€ ìƒì„±...")
            optimal_answer = "ìµœì  ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê° AI ëª¨ë¸ì˜ ê°œë³„ ì‘ë‹µì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ë©”ì¸ ë‹µë³€ êµ¬ì„± (ì±„íŒ… ì°½ì—ëŠ” ìµœì  ë‹µë³€ ë³¸ë¬¸ë§Œ í‘œì‹œ)
        formatted_response = f"""## ìµœì ì˜ ë‹µë³€

{optimal_answer}
"""
        
        return formatted_response
        
    except Exception as e:
        print(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
        return f"""**ìµœì ì˜ ë‹µë³€:**

{final_result.get('ìµœì ì˜_ë‹µë³€', 'ë‹µë³€ ìƒì„± ì‹¤íŒ¨')}

*í¬ë§·íŒ… ì˜¤ë¥˜ ë°œìƒ*
"""