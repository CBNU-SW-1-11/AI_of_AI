"""
ìµœì  ì‘ë‹µ ìƒì„± ì„œë¹„ìŠ¤
ì‹¬íŒ ëª¨ë¸ì„ í†µí•œ ë‹¤ì¤‘ LLM ì‘ë‹µ ê²€ì¦ ë° ìµœì  ë‹µë³€ ìƒì„±
"""
import os
import re
import json
import asyncio
import aiohttp
import openai
import anthropic
from groq import Groq
import ollama

# ë¡œì»¬ import
from ..utils.error_handlers import get_user_friendly_error_message
from ..utils.ai_utils import enforce_korean_instruction, get_openai_completion_limit


def detect_question_type_from_content(content):
    """ì§ˆë¬¸ ë‚´ìš©ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ ìœ í˜• ê°ì§€: code, image, document, creative, general"""
    import re
    
    content_lower = content.lower()
    
    # ì½”ë“œ ê´€ë ¨ í‚¤ì›Œë“œ (ì½”ë“œ ì‘ì„±, êµ¬í˜„, í•¨ìˆ˜, ì•Œê³ ë¦¬ì¦˜ ë“±)
    code_keywords = ['ì½”ë“œ', 'code', 'í•¨ìˆ˜', 'function', 'í”„ë¡œê·¸ë˜ë°', 'programming', 'ì•Œê³ ë¦¬ì¦˜', 'algorithm', 
                     'êµ¬í˜„', 'implement', 'ì‘ì„±', 'write', 'ê°œë°œ', 'develop', 'ìŠ¤í¬ë¦½íŠ¸', 'script',
                     'íŒŒì´ì¬', 'python', 'ìë°”', 'java', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'javascript', 'c++', 'c#']
    
    # ì´ë¯¸ì§€ ê´€ë ¨ í‚¤ì›Œë“œ
    image_keywords = ['ì´ë¯¸ì§€', 'image', 'ì‚¬ì§„', 'photo', 'ê·¸ë¦¼', 'picture', 'ì‹œê°', 'visual', 'í™”ë©´']
    
    # ë¬¸ì„œ ê´€ë ¨ í‚¤ì›Œë“œ
    document_keywords = ['ë¬¸ì„œ', 'document', 'pdf', 'íŒŒì¼', 'file', 'ìš”ì•½', 'summary', 'ë‚´ìš©', 'content']
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ í‚¤ì›Œë“œ
    creative_keywords = ['ê¸€ì“°ê¸°', 'writing', 'ì°½ì‘', 'creative', 'ì†Œì„¤', 'novel', 'ì‹œ', 'poem', 'ì—ì„¸ì´', 'essay',
                        'ì´ì•¼ê¸°', 'story', 'ë‚´ìš© ì‘ì„±', 'write content', 'ë¬¸ì¥', 'sentence']
    
    # ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in code_keywords):
        # ì‹¤ì œ ì½”ë“œ ì‘ì„± ìš”ì²­ì¸ì§€ í™•ì¸ (ì˜ˆ: "ì½”ë“œ ì‘ì„±", "í•¨ìˆ˜ ë§Œë“¤ì–´ì¤˜", "êµ¬í˜„í•´ì¤˜" ë“±)
        code_patterns = [
            r'ì½”ë“œ.*ì‘ì„±|ì‘ì„±.*ì½”ë“œ',
            r'í•¨ìˆ˜.*ë§Œë“¤|ë§Œë“¤.*í•¨ìˆ˜',
            r'êµ¬í˜„.*í•´|í•´.*êµ¬í˜„',
            r'ì½”ë“œ.*ë³´ì—¬|ë³´ì—¬.*ì½”ë“œ',
            r'í”„ë¡œê·¸ë¨.*ì‘ì„±|ì‘ì„±.*í”„ë¡œê·¸ë¨',
            r'íŒŒì´ì¬.*ì½”ë“œ|ì½”ë“œ.*íŒŒì´ì¬',
            r'ì•Œê³ ë¦¬ì¦˜.*êµ¬í˜„|êµ¬í˜„.*ì•Œê³ ë¦¬ì¦˜'
        ]
        if any(re.search(pattern, content_lower) for pattern in code_patterns):
            return 'code'
    
    # ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ (ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ì—…ë¡œë“œëœ ê²½ìš°ëŠ” has_imageë¡œ ì²˜ë¦¬ë¨)
    if any(keyword in content_lower for keyword in image_keywords):
        # ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        image_patterns = [
            r'ì´ë¯¸ì§€.*ë¶„ì„|ë¶„ì„.*ì´ë¯¸ì§€',
            r'ì‚¬ì§„.*ì„¤ëª…|ì„¤ëª….*ì‚¬ì§„',
            r'ê·¸ë¦¼.*ë­|ë­.*ê·¸ë¦¼',
            r'ì´ë¯¸ì§€.*ë­|ë­.*ì´ë¯¸ì§€'
        ]
        if any(re.search(pattern, content_lower) for pattern in image_patterns):
            return 'image'
    
    # ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in document_keywords):
        # ë¬¸ì„œ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸
        document_patterns = [
            r'ë¬¸ì„œ.*ë¶„ì„|ë¶„ì„.*ë¬¸ì„œ',
            r'íŒŒì¼.*ë‚´ìš©|ë‚´ìš©.*íŒŒì¼',
            r'pdf.*ìš”ì•½|ìš”ì•½.*pdf',
            r'ë¬¸ì„œ.*ìš”ì•½|ìš”ì•½.*ë¬¸ì„œ'
        ]
        if any(re.search(pattern, content_lower) for pattern in document_patterns):
            return 'document'
    
    # ì°½ì‘/ê¸€ì“°ê¸° ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
    if any(keyword in content_lower for keyword in creative_keywords):
        # ì°½ì‘ ìš”ì²­ì¸ì§€ í™•ì¸
        creative_patterns = [
            r'ê¸€.*ì“°|ì“°.*ê¸€',
            r'ì†Œì„¤.*ì‘ì„±|ì‘ì„±.*ì†Œì„¤',
            r'ì‹œ.*ì‘ì„±|ì‘ì„±.*ì‹œ',
            r'ì´ì•¼ê¸°.*ë§Œë“¤|ë§Œë“¤.*ì´ì•¼ê¸°',
            r'ì°½ì‘.*í•´|í•´.*ì°½ì‘',
            r'ì—ì„¸ì´.*ì‘ì„±|ì‘ì„±.*ì—ì„¸ì´'
        ]
        if any(re.search(pattern, content_lower) for pattern in creative_patterns):
            return 'creative'
    
    # ê¸°ë³¸ê°’: ì¼ë°˜ ì§ˆë¬¸
    return 'general'



def classify_question_type(question):
    """ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜: ì‚¬ì‹¤(Factual) vs ì˜ê²¬(Opinion)"""
    try:
        import openai
        import os
        
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        classification_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì´ "ì‚¬ì‹¤ì  ì§ˆë¬¸", "ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸", ë˜ëŠ” "ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸"ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë¶„ë¥˜ ê¸°ì¤€:
- ì‚¬ì‹¤ì  ì§ˆë¬¸: ê°ê´€ì  ì‚¬ì‹¤, ì •í™•í•œ ë‹µì´ ì¡´ì¬ (ì˜ˆ: ì„¤ë¦½ì—°ë„, ìœ„ì¹˜, ì—­ì‚¬ì  ì‚¬ì‹¤)
- ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸: ì£¼ê´€ì  í‰ê°€, ì¶”ì²œ, ì„ í˜¸ë„ (ì˜ˆ: ë§›ì§‘ ì¶”ì²œ, ì¢‹ì€ ì¹´í˜, ìµœê³ ì˜ ì œí’ˆ)
- ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸: ì½”ë“œ ì‘ì„±, í”„ë¡œê·¸ë˜ë° ì˜ˆì œ, ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ìš”ì²­ (ì˜ˆ: "ë³„ì°ê¸° ì½”ë“œ", "íŒŒì´ì¬ìœ¼ë¡œ ì‘ì„±", "í•¨ìˆ˜ ë§Œë“¤ì–´ì¤˜")

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "type": "factual" ë˜ëŠ” "opinion" ë˜ëŠ” "code",
  "confidence": 0.0-1.0,
  "reason": "ë¶„ë¥˜ ì´ìœ "
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
                {"role": "user", "content": classification_prompt}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        print(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: {result['type']} (ì‹ ë¢°ë„: {result['confidence']})")
        print(f"   ì´ìœ : {result['reason']}")
        
        return result['type']
        
    except Exception as e:
        print(f"âš ï¸ ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 'factual' ì‚¬ìš©")
        return "factual"



def collect_multi_llm_responses(user_message, judge_model="GPT-4o", selected_models=None, question_type=None):
    """1ë‹¨ê³„: ì„ íƒëœ LLMë“¤ì—ê²Œ ë³‘ë ¬ ì§ˆì˜ í›„ ì‹¬íŒ ëª¨ë¸ë¡œ ê²€ì¦"""
    import asyncio
    import aiohttp
    import json
    import time
    
    responses = {}
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—”ë“œí¬ì¸íŠ¸ë“¤ (ëª…ì‹œì  ëª¨ë¸ëª… ì‚¬ìš©)
    all_llm_endpoints = {
        # GPT ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'GPT-5': 'http://localhost:8000/chat/gpt-5/',
        'GPT-5-Mini': 'http://localhost:8000/chat/gpt-5-mini/',
        'GPT-4.1': 'http://localhost:8000/chat/gpt-4.1/',
        'GPT-4.1-Mini': 'http://localhost:8000/chat/gpt-4.1-mini/',
        'GPT-4o': 'http://localhost:8000/chat/gpt-4o/',
        'GPT-4o-Mini': 'http://localhost:8000/chat/gpt-4o-mini/',
        'GPT-4-Turbo': 'http://localhost:8000/chat/gpt-4-turbo/',
        'GPT-3.5-Turbo': 'http://localhost:8000/chat/gpt-3.5-turbo/',
        
        # Gemini ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'Gemini-2.5-Pro': 'http://localhost:8000/chat/gemini-2.5-pro/',
        'Gemini-2.5-Flash': 'http://localhost:8000/chat/gemini-2.5-flash/',
        'Gemini-2.0-Flash-Exp': 'http://localhost:8000/chat/gemini-2.0-flash-exp/',
        'Gemini-2.0-Flash-Lite': 'http://localhost:8000/chat/gemini-2.0-flash-lite/',
        
        # Claude ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
        'Claude-4-Opus': 'http://localhost:8000/chat/claude-4-opus/',
        'Claude-3.7-Sonnet': 'http://localhost:8000/chat/claude-3.7-sonnet/',
        'Claude-3.5-Sonnet': 'http://localhost:8000/chat/claude-3.5-sonnet/',
        'Claude-3.5-Haiku': 'http://localhost:8000/chat/claude-3.5-haiku/',
        'Claude-3-Opus': 'http://localhost:8000/chat/claude-3-opus/',
        
        # HyperCLOVA X ëª¨ë¸ë“¤
        'HCX-003': 'http://localhost:8000/chat/clova-hcx-003/',
        'HCX-DASH-001': 'http://localhost:8000/chat/clova-hcx-dash-001/',
    }
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë¸ë“¤ë§Œ í•„í„°ë§ (ê¸°ë³¸ê°’: ëª¨ë“  ëª¨ë¸)
    if selected_models:
        print(f"ğŸ“‹ selected_models ì…ë ¥: {selected_models} (íƒ€ì…: {type(selected_models)})")
        # ì„ íƒëœ ëª¨ë¸ëª…ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        model_mapping = {
            # GPT ëª¨ë¸ë“¤
            'gpt-5': 'GPT-5',
            'gpt-5-mini': 'GPT-5-Mini',
            'gpt-4.1': 'GPT-4.1',
            'gpt-4.1-mini': 'GPT-4.1-Mini',
            'gpt-4o': 'GPT-4o',
            'gpt-4o-mini': 'GPT-4o-Mini',
            'gpt-4-turbo': 'GPT-4-Turbo',
            'gpt-3.5-turbo': 'GPT-3.5-Turbo',
            
            # Gemini ëª¨ë¸ë“¤
            'gemini-2.5-pro': 'Gemini-2.5-Pro',
            'gemini-2.5-flash': 'Gemini-2.5-Flash',
            'gemini-2.0-flash-exp': 'Gemini-2.0-Flash-Exp',
            'gemini-2.0-flash-lite': 'Gemini-2.0-Flash-Lite',
            
            # Claude ëª¨ë¸ë“¤
            'claude-4-opus': 'Claude-4-Opus',
            'claude-3.7-sonnet': 'Claude-3.7-Sonnet',
            'claude-3.5-sonnet': 'Claude-3.5-Sonnet',
            'claude-3.5-haiku': 'Claude-3.5-Haiku',
            'claude-3-opus': 'Claude-3-Opus',
            
            # HyperCLOVA X ëª¨ë¸ë“¤
            'clova-hcx-003': 'HCX-003',
            'clova-hcx-dash-001': 'HCX-DASH-001',
        }
        
        selected_standard_models = []
        for model in selected_models:
            model_lower = model.lower() if isinstance(model, str) else str(model).lower()
            if model_lower in model_mapping:
                selected_standard_models.append(model_mapping[model_lower])
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ëª…: {model}")
        
        # ì„ íƒëœ ëª¨ë¸ë“¤ì˜ ì—”ë“œí¬ì¸íŠ¸ë§Œ ì‚¬ìš©
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in selected_standard_models}
        print(f"ğŸ“‹ ë§¤í•‘ëœ í‘œì¤€ ëª¨ë¸: {selected_standard_models}")
    else:
        # ì„ íƒëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ 3ê°œ ì‚¬ìš© (ë¹„ìš© ì ˆê°)
        print(f"âš ï¸ selected_modelsê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ 3ê°œ ì‚¬ìš©")
        default_models = ['GPT-4o-Mini', 'Gemini-2.0-Flash-Lite', 'Claude-3.5-Haiku']
        llm_endpoints = {k: v for k, v in all_llm_endpoints.items() if k in default_models}
    
    if not llm_endpoints:
        print(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—”ë“œí¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. selected_modelsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    print(f"ğŸ¯ ì„ íƒëœ LLM ëª¨ë¸ë“¤: {list(llm_endpoints.keys())} (ì´ {len(llm_endpoints)}ê°œ)")
    
    async def fetch_response(session, ai_name, endpoint):
        """ê°œë³„ LLMì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°"""
        try:
            payload = {
                'message': user_message,
                'user_id': 'system'
            }
            
            print(f"ğŸ”„ {ai_name} ëª¨ë¸ì— ìš”ì²­ ì „ì†¡ ì¤‘... (ì—”ë“œí¬ì¸íŠ¸: {endpoint})")
            async with session.post(endpoint, json=payload, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    response_content = result.get('response', 'ì‘ë‹µ ì—†ìŒ')
                    print(f"âœ… {ai_name} ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(str(response_content))}ì")
                    print(f"ğŸ“„ {ai_name} ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 200ì): {str(response_content)[:200]}...")
                    return ai_name, response_content
                else:
                    # HTTP ìƒíƒœ ì½”ë“œ ì˜¤ë¥˜ë¥¼ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
                    error_text = await response.text()
                    print(f"âŒ {ai_name} HTTP ì˜¤ë¥˜: {response.status}, ë‚´ìš©: {error_text[:200]}")
                    error_msg = Exception(f"HTTP {response.status}: {error_text}")
                    friendly_msg = get_user_friendly_error_message(error_msg)
                    return ai_name, friendly_msg
        except Exception as e:
            # ì˜ˆì™¸ë¥¼ ì¹œí™”ì  ë©”ì‹œì§€ë¡œ ë³€í™˜
            print(f"âŒ {ai_name} ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
            friendly_msg = get_user_friendly_error_message(e)
            return ai_name, friendly_msg
    
    async def collect_all_responses():
        """ëª¨ë“  LLMì—ì„œ ë™ì‹œì— ì‘ë‹µ ìˆ˜ì§‘"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            for ai_name, endpoint in llm_endpoints.items():
                task = fetch_response(session, ai_name, endpoint)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, tuple):
                    ai_name, response = result
                    responses[ai_name] = response
                elif isinstance(result, Exception):
                    print(f"LLM ì‘ë‹µ ìˆ˜ì§‘ ì˜¤ë¥˜: {result}")
    
    try:
        # ë¹„ë™ê¸° ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(collect_all_responses())
        loop.close()
        
        print(f"âœ… {len(responses)}ê°œ LLMì—ì„œ ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ: {list(responses.keys())}")
        
        # ì—ëŸ¬ ë©”ì‹œì§€ í•„í„°ë§ (íƒ€ì„ì•„ì›ƒ/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±)
        # get_user_friendly_error_messageê°€ ë°˜í™˜í•˜ëŠ” ì •í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´
        error_patterns = [
            "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            "ëª¨ë¸ ì‚¬ìš©ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤",
            "ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
            "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”",
            "ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ëŒ€í™” ê¸¸ì´ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤",
            "ì½˜í…ì¸  ì •ì±…ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"
        ]
        
        valid_responses = {}
        error_responses = {}
        
        for ai_name, response in responses.items():
            response_str = str(response)
            # ì •í™•í•œ ì—ëŸ¬ íŒ¨í„´ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ì´ ì•„ë‹Œ ì „ì²´ ë©”ì‹œì§€ í™•ì¸)
            is_error = any(pattern in response_str for pattern in error_patterns)
            
            # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê³  ì—ëŸ¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë©´ ì—ëŸ¬ë¡œ ê°„ì£¼
            if len(response_str) < 50 and any(keyword in response_str.lower() for keyword in ["timeout", "connection", "error", "ì˜¤ë¥˜", "ì‹¤íŒ¨"]):
                is_error = True
            
            if is_error:
                error_responses[ai_name] = response
                print(f"âš ï¸ {ai_name} ì‘ë‹µì´ ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ê°ì§€ë¨: {response_str[:100]}...")
            else:
                valid_responses[ai_name] = response
                print(f"âœ… {ai_name} ìœ íš¨í•œ ì‘ë‹µ: {len(response_str)}ì")
        
        print(f"ğŸ“Š ìœ íš¨í•œ ì‘ë‹µ: {len(valid_responses)}ê°œ, ì—ëŸ¬ ì‘ë‹µ: {len(error_responses)}ê°œ")
        
        # ìœ íš¨í•œ ì‘ë‹µì´ ì—†ìœ¼ë©´ ì—ëŸ¬
        if not valid_responses:
            if error_responses:
                error_summary = ", ".join([f"{name}: {msg[:50]}..." for name, msg in list(error_responses.items())[:3]])
                raise ValueError(f"ëª¨ë“  LLM ìš”ì²­ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({error_summary})")
            else:
                print(f"âŒ ìˆ˜ì§‘ëœ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤!")
                raise ValueError("LLMì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ìœ íš¨í•œ ì‘ë‹µë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì  ë‹µë³€ ìƒì„±
        print(f"âš–ï¸ ì‹¬íŒ ëª¨ë¸({judge_model})ë¡œ ê²€ì¦ ë° ìµœì  ë‹µë³€ ìƒì„± ì‹œì‘... (ìœ íš¨í•œ ì‘ë‹µ {len(valid_responses)}ê°œ ì‚¬ìš©)")
        print(f"ğŸ“‹ ì§ˆë¬¸ ìœ í˜•: {question_type}")
        final_result = judge_and_generate_optimal_response(valid_responses, user_message, judge_model, question_type=question_type)
        print(f"âœ… ìµœì  ë‹µë³€ ìƒì„± ì™„ë£Œ: {type(final_result)}, í‚¤: {list(final_result.keys()) if isinstance(final_result, dict) else 'N/A'}")
        return final_result
        
    except Exception as e:
        print(f"âŒ LLM ì‘ë‹µ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ì‘ë‹µë“¤
        fallback_responses = {
            'GPT-3.5-turbo': f'GPT ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.',
            'Claude-3.5-haiku': f'Claude ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.',
            'Llama-3.1-8b': f'Llama ì‘ë‹µ (ìˆ˜ì§‘ ì‹¤íŒ¨): {user_message}ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.'
        }
        return judge_and_generate_optimal_response(fallback_responses, user_message, judge_model)



def detect_conflicts_in_responses(llm_responses):
    """LLM ì‘ë‹µì—ì„œ ìƒí˜¸ëª¨ìˆœ ê°ì§€ (í•˜ë“œì½”ë”© ì—†ì´ ë²”ìš©ì )"""
    import re
    from collections import defaultdict
    
    conflicts = {
        "dates": defaultdict(list),
        "locations": defaultdict(list), 
        "numbers": defaultdict(list),
        "general_facts": defaultdict(list)
    }
    
    # ê° LLM ì‘ë‹µì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    for model_name, response in llm_responses.items():
        # ì—°ë„ íŒ¨í„´ ì¶”ì¶œ (4ìë¦¬ ìˆ«ì, 1900-2024 ë²”ìœ„)
        year_pattern = r'(\d{4})'
        year_matches = re.findall(year_pattern, response)
        
        for year_str in year_matches:
            try:
                year = int(year_str)
                if 1900 <= year <= 2024:  # í•©ë¦¬ì ì¸ ì—°ë„ ë²”ìœ„
                    conflicts["dates"][year_str].append(model_name)
            except ValueError:
                continue
        
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ (ì‹œ/ë„/êµ¬/êµ° íŒ¨í„´)
        locations = re.findall(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)', response)
        for location in locations:
            conflicts["locations"][location].append(model_name)
        
        # ìˆ˜ì¹˜ ì •ë³´ ì¶”ì¶œ (ë‹¨ìœ„ í¬í•¨, ì—°ë„ ì œì™¸)
        numbers = re.findall(r'\d+(?:ëª…|ê°œ|ì›”|ì¼|ì–µ|ë§Œ|ì²œ)', response)
        for number in numbers:
            conflicts["numbers"][number].append(model_name)
    
    # ìƒí˜¸ëª¨ìˆœ í•„í„°ë§ (2ê°œ ì´ìƒ ë‹¤ë¥¸ ê°’ì´ ìˆì„ ë•Œë§Œ)
    detected_conflicts = {}
    
    for category, items in conflicts.items():
        if len(items) > 1:  # ì„œë¡œ ë‹¤ë¥¸ ê°’ì´ 2ê°œ ì´ìƒ
            detected_conflicts[category] = dict(items)
    
    return detected_conflicts



def judge_and_generate_optimal_response(llm_responses, user_question, judge_model="GPT-5", question_type=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ: LLM ë¹„êµ + ì„ íƒì  ì›¹ ê²€ì¦ + ë‹¤ìˆ˜ê²°"""
    try:
        print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œì‘: {user_question}")
        print(f"ğŸ“‹ judge_and_generate_optimal_responseì— ì „ë‹¬ëœ llm_responses í‚¤: {list(llm_responses.keys()) if llm_responses else 'None'}")
        print(f"ğŸ“‹ llm_responses ì „ì²´: {llm_responses}")
        
        # 0ë‹¨ê³„: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ì „ë‹¬ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìë™ ë¶„ë¥˜)
        if question_type is None:
            question_type = classify_question_type(user_question)
        else:
            print(f"ğŸ“‹ ì „ë‹¬ë°›ì€ ì§ˆë¬¸ ìœ í˜•: {question_type}")
        
        # 1ë‹¨ê³„: ìƒí˜¸ëª¨ìˆœ ê°ì§€
        conflicts = detect_conflicts_in_responses(llm_responses)
        print(f"ğŸ“Š ê°ì§€ëœ ìƒí˜¸ëª¨ìˆœ: {conflicts}")
        print(f"ğŸ” ìƒí˜¸ëª¨ìˆœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(conflicts)}")
        for category, items in conflicts.items():
            print(f"  - {category}: {items}")
        
        # 2ë‹¨ê³„: ì˜ê²¬ ì§ˆë¬¸ - Tie-breaker í™•ì¸
        if question_type == "opinion" and len(llm_responses) == 2:
            print("ğŸ—³ï¸ ì˜ê²¬ ì§ˆë¬¸ + 2ê°œ ëª¨ë¸ â†’ Tie-breaker í˜¸ì¶œ")
            # Tie-breaker ë¡œì§ì€ ë‚˜ì¤‘ì— êµ¬í˜„
            pass
        
        # 3ë‹¨ê³„: ì›¹ ê²€ì¦ (ì‚¬ì‹¤ ì§ˆë¬¸ë§Œ) ë˜ëŠ” ë‹¤ìˆ˜ê²° (ì˜ê²¬ ì§ˆë¬¸) ë˜ëŠ” ì½”ë“œ í’ˆì§ˆ í‰ê°€ (ì½”ë“œ ì§ˆë¬¸)
        verified_facts = {}
        web_verification_used = False
        
        if question_type == "code":
            print(f"ğŸ’» ì½”ë“œ ì§ˆë¬¸ ê°ì§€ â†’ Wikipedia ê²€ì¦ ìƒëµ, ì½”ë“œ í’ˆì§ˆ í‰ê°€ ì‚¬ìš©")
            web_result = {"verified": False}
        elif question_type == "image":
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì§ˆë¬¸ ê°ì§€ â†’ Wikipedia ê²€ì¦ ìƒëµ, OCR/Ollama ê²€ì¦ ì‚¬ìš©")
            web_result = {"verified": False}
        elif question_type == "document":
            print(f"ğŸ“„ ë¬¸ì„œ ì§ˆë¬¸ ê°ì§€ â†’ Wikipedia ê²€ì¦ ìƒëµ, ë¬¸ì„œ ë¶„ì„ ì‚¬ìš©")
            web_result = {"verified": False}
        elif question_type == "factual":
            print(f"ğŸŒ Wikipedia ì›¹ ê²€ì¦ ì‹œì‘... ì§ˆë¬¸: '{user_question}'")
            
            # ë²”ìš©ì  ì›¹ ê²€ì¦ - ì‚¬ì‹¤ ì§ˆë¬¸ì—ë§Œ ì ìš©
            web_result = quick_web_verify("general", {}, user_question)
        else:
            print(f"ğŸ—³ï¸ ì˜ê²¬ ì§ˆë¬¸ â†’ Wikipedia ê²€ì¦ ìƒëµ, ë‹¤ìˆ˜ê²° ë°©ì‹ ì‚¬ìš©")
            web_result = {"verified": False}
        
        if web_result.get("verified"):
            # ê²€ì¦ëœ ì •ë³´ë¥¼ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ì— ì €ì¥
            if web_result.get('extracted_year'):
                verified_facts["dates"] = web_result
            if web_result.get('location'):
                if "locations" not in verified_facts:
                    verified_facts["locations"] = web_result
                else:
                    verified_facts["locations"].update(web_result)
            if not verified_facts:  # ì•„ë¬´ê²ƒë„ ì €ì¥ë˜ì§€ ì•Šì€ ê²½ìš°
                verified_facts["general_facts"] = web_result
                
            web_verification_used = True
            
            # ê²€ì¦ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
            info_parts = []
            if web_result.get('extracted_year'):
                info_parts.append(f"ì—°ë„ {web_result.get('extracted_year')}ë…„")
            if web_result.get('location'):
                info_parts.append(f"ìœ„ì¹˜ {web_result.get('location')}")
            if web_result.get('type'):
                info_parts.append(f"ìœ í˜• {web_result.get('type')}")
            
            print(f"âœ… ì›¹ ê²€ì¦ ì„±ê³µ: {', '.join(info_parts)}")
        else:
            print(f"âš ï¸ ì›¹ ê²€ì¦ ì‹¤íŒ¨: {web_result.get('error')}")
        
        # ìƒí˜¸ëª¨ìˆœ ê¸°ë°˜ ê²€ì¦ (ì›¹ ê²€ì¦ ì„±ê³µ/ì‹¤íŒ¨ì™€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰)
        if conflicts:
            print("âš¡ ìƒí˜¸ëª¨ìˆœ ë°œê²¬! ìƒí˜¸ëª¨ìˆœ ê¸°ë°˜ ê²€ì¦ ì‹œì‘...")
            print(f"ğŸ” ì²˜ë¦¬í•  ìƒí˜¸ëª¨ìˆœ: {conflicts}")
            
            for conflict_type, conflict_values in conflicts.items():
                # ì›¹ ê²€ì¦ì´ ì´ë¯¸ ì„±ê³µí•œ í•­ëª©ì€ ë®ì–´ì“°ì§€ ì•ŠìŒ
                if conflict_type not in verified_facts or not verified_facts[conflict_type].get("verified"):
                    verified_facts[conflict_type] = {
                        "verified": False,
                        "conflict_detected": True,
                        "conflict_values": list(conflict_values.keys()),
                        "conflict_details": dict(conflict_values)  # {ê°’: [AIëª©ë¡]}
                    }
                    print(f"âœ… ìƒí˜¸ëª¨ìˆœ ì²˜ë¦¬ë¨: {conflict_type} -> {verified_facts[conflict_type]}")
                else:
                    print(f"â„¹ï¸ {conflict_type}ëŠ” ì´ë¯¸ Wikipedia ê²€ì¦ ì™„ë£Œ, ìƒí˜¸ëª¨ìˆœ ì²˜ë¦¬ ê±´ë„ˆëœ€")
        else:
            print("â„¹ï¸ ìƒí˜¸ëª¨ìˆœ ì—†ìŒ")
        
        # 3ë‹¨ê³„: ì‹¬íŒ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì›¹ ê²€ì¦ ê²°ê³¼ í¬í•¨)
        model_sections = []
        verification_json_entries = []
        
        for model_name, response in llm_responses.items():
            model_sections.append(f"[{model_name} ë‹µë³€]\n{response}")
            verification_json_entries.append(f'    "{model_name}": {{"accuracy": "ì •í™•ì„±_íŒì •", "errors": "êµ¬ì²´ì _ì˜¤ë¥˜_ì„¤ëª…", "confidence": "ì‹ ë¢°ë„_0-100", "adopted_info": ["ì±„íƒëœ_ì •ë³´ë“¤"], "rejected_info": ["ì œì™¸ëœ_ì •ë³´ë“¤ê³¼_ì´ìœ "]}}')
        
        model_responses_text = "\n\n".join(model_sections)
        verification_json_format = ",\n".join(verification_json_entries)
        
        # ì›¹ ê²€ì¦ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ (ë²”ìš©ì )
        web_verification_text = ""
        if web_verification_used:
            # ëª¨ë“  ê²€ì¦ëœ ì‚¬ì‹¤ì— ëŒ€í•´ ë²”ìš©ì ìœ¼ë¡œ ì²˜ë¦¬
            verified_info_parts = []
            
            for fact_type, verification in verified_facts.items():
                if verification.get('verified'):
                    if verification.get('extracted_year'):
                        verified_info_parts.append(f"- **âœ… ê³µì‹ ì—°ë„**: {verification['extracted_year']}ë…„")
                    if verification.get('location'):
                        verified_info_parts.append(f"- **âœ… ê³µì‹ ìœ„ì¹˜**: {verification['location']}")
                    if verification.get('type'):
                        verified_info_parts.append(f"- **âœ… ê³µì‹ ìœ í˜•**: {verification['type']}")
                    if verification.get('abstract') and not any([verification.get('extracted_year'), verification.get('location'), verification.get('type')]):
                        # ê¸°íƒ€ ê²€ì¦ëœ ì •ë³´
                        verified_info_parts.append(f"- **âœ… ê²€ì¦ëœ ì •ë³´**: {verification['abstract'][:100]}...")
            
            if verified_info_parts:
                verified_info_text = '\n'.join(verified_info_parts)
                # ì²« ë²ˆì§¸ ê²€ì¦ ê²°ê³¼ì˜ ì‹ ë¢°ë„ ì‚¬ìš©
                first_verification = next(iter(verified_facts.values()))
                
                # Wikipedia ì›ë¬¸ í¬í•¨ (LLMì´ ì§ì ‘ ë¹„êµ ë¶„ì„ ê°€ëŠ¥)
                wikipedia_full_text = first_verification.get('full_text', '') or first_verification.get('abstract', '')
                wikipedia_excerpt = wikipedia_full_text[:500] if len(wikipedia_full_text) > 500 else wikipedia_full_text
                
                web_verification_text = f"""

**ğŸŒ Wikipedia ì›¹ ê²€ì¦ ê²°ê³¼ (ì‹ ë¢°ë„ {first_verification.get('confidence', 0.9)*100:.0f}%):**
{verified_info_text}
- **ì¶œì²˜**: {first_verification.get('source', 'Wikipedia')}
- **í˜ì´ì§€**: {first_verification.get('page_title', 'í™•ì¸ë¨')}

**ğŸ“– Wikipedia ì›ë¬¸:**
{wikipedia_excerpt}

ğŸš¨ **ì ˆëŒ€ ì¤€ìˆ˜ ê·œì¹™**: ìœ„ Wikipedia ì›ë¬¸ì€ ê³µì‹ ê²€ì¦ëœ ì •ë³´ì…ë‹ˆë‹¤.

**ğŸ“‹ Wikipedia ê²€ì¦ ê¸°ì¤€:**
1. **ì¼ì¹˜í•˜ëŠ” ì •ë³´ = ì±„íƒ**: LLMì´ Wikipediaì™€ ë™ì¼í•œ ì •ë³´ë¥¼ ë§í–ˆë‹¤ë©´ â†’ **ë°˜ë“œì‹œ adopted_infoì— í¬í•¨**
2. **ë¶ˆì¼ì¹˜í•˜ëŠ” ì •ë³´ = ì œì™¸**: LLMì´ Wikipediaì™€ ë‹¤ë¥¸ ì •ë³´ë¥¼ ë§í–ˆë‹¤ë©´ â†’ rejected_infoì— í¬í•¨

**âœ… ì˜¬ë°”ë¥¸ ì²˜ë¦¬ ì˜ˆì‹œ:**
- Wikipedia: "1951ë…„ ì„¤ë¦½"
- LLM A: "1951ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤" â†’ âœ… **ì¼ì¹˜** â†’ **adopted_infoì— í¬í•¨**
- LLM B: "1946ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤" â†’ âŒ **ë¶ˆì¼ì¹˜** â†’ rejected_infoì— í¬í•¨

**âŒ ì˜ëª»ëœ ì²˜ë¦¬ (ì ˆëŒ€ ê¸ˆì§€):**
- Wikipedia: "1951ë…„ ì„¤ë¦½"
- LLM A: "1951ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤" â†’ âŒ "ë¶ˆì¼ì¹˜"ë¼ê³  í‘œì‹œí•˜ë©´ ì•ˆë¨!

**ê° LLM ë‹µë³€ì„ Wikipedia ì›ë¬¸ê³¼ ì§ì ‘ ë¹„êµí•˜ì—¬:**
- ì¼ì¹˜í•˜ëŠ” ë‚´ìš©ì€ **ë°˜ë“œì‹œ ì±„íƒ** (adopted_info)
- ë¶ˆì¼ì¹˜í•˜ëŠ” ë‚´ìš©ë§Œ **ì œì™¸** (rejected_info)
"""
        # ìƒí˜¸ëª¨ìˆœì´ ê°ì§€ëœ ê²½ìš° (ì›¹ ê²€ì¦ ì‹¤íŒ¨ ì‹œ)
        elif any(fact.get("conflict_detected") for fact in verified_facts.values()):
            # ëª¨ë“  ìƒí˜¸ëª¨ìˆœ ìœ í˜•ì— ëŒ€í•´ ë²”ìš©ì ìœ¼ë¡œ ì²˜ë¦¬
            conflict_summaries = []
            conflict_ai_details = []
            
            for conflict_type, conflict_data in verified_facts.items():
                if conflict_data.get("conflict_detected"):
                    conflict_values = conflict_data.get("conflict_values", [])
                    conflict_details = conflict_data.get("conflict_details", {})
                    
                    # ìœ í˜•ë³„ í•œêµ­ì–´ ë¼ë²¨ ë§¤í•‘
                    type_labels = {
                        "dates": "ë‚ ì§œ/ì—°ë„",
                        "locations": "ìœ„ì¹˜",
                        "numbers": "ìˆ˜ì¹˜",
                        "general_facts": "ì¼ë°˜ ì‚¬ì‹¤"
                    }
                    type_label = type_labels.get(conflict_type, conflict_type)
                    
                    conflict_summaries.append(f"- **{type_label} ë¶ˆì¼ì¹˜**: {', '.join(conflict_values)}")
                    
                    # ê° AIë³„ ìƒí˜¸ëª¨ìˆœ ìƒì„¸ ì •ë³´ ìƒì„±
                    for value, ai_list in conflict_details.items():
                        ai_names = ', '.join(ai_list)
                        conflict_ai_details.append(f"- {value} ({type_label}): {ai_names}")
            
            conflict_summary_text = '\n'.join(conflict_summaries)
            conflict_ai_text = '\n'.join(conflict_ai_details)
            
            web_verification_text = f"""

**âš ï¸ ìƒí˜¸ëª¨ìˆœ ê°ì§€ë¨ (ì›¹ ê²€ì¦ ì‹¤íŒ¨):**
{conflict_summary_text}
- **ì¡°ì¹˜**: í™•ì‹ í•  ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” ìµœì  ë‹µë³€ì—ì„œ ìƒëµí•˜ì„¸ìš”

**ğŸš¨ ê° AIë³„ ìƒí˜¸ëª¨ìˆœ ìƒì„¸:**
{conflict_ai_text}

**ğŸš¨ ê° AIë³„ ì˜¤ë¥˜ ì²˜ë¦¬ ê·œì¹™ (í•„ìˆ˜ ì¤€ìˆ˜):**
- ìœ„ì—ì„œ ìƒí˜¸ëª¨ìˆœì— ì°¸ì—¬í•œ ëª¨ë“  AIëŠ” ë°˜ë“œì‹œ "í‹€ë¦° ì •ë³´"ì— "ì •ë³´ ë¶ˆí™•ì‹¤ (ë‹¤ë¥¸ AIì™€ ìƒì¶©)"ì„ ê¸°ë¡í•˜ì„¸ìš”
- ìƒí˜¸ëª¨ìˆœì´ ìˆëŠ” ì •ë³´ëŠ” ì ˆëŒ€ "í‹€ë¦° ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ì•ˆë©ë‹ˆë‹¤
- ì˜ˆì‹œ: GPT-4o Miniê°€ 1946ë…„ì´ë¼ê³  í–ˆê³ , Geminiê°€ 1951ë…„ì´ë¼ê³  í–ˆë‹¤ë©´ â†’ ë‘˜ ë‹¤ "í‹€ë¦° ì •ë³´"ì— "ì„¤ë¦½ì—°ë„ ë¶ˆí™•ì‹¤ (ë‹¤ë¥¸ AIì™€ ìƒì¶©)"ì„ ê¸°ë¡
"""
        
        # ìƒí˜¸ëª¨ìˆœ ì •ë³´ê°€ ìˆìœ¼ë©´ ë” ê°•ë ¥í•œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        contradiction_warning = ""
        has_conflicts = any(fact.get("conflict_detected") for fact in verified_facts.values())
        print(f"ğŸ” ìƒí˜¸ëª¨ìˆœ ê²½ê³  ìƒì„± ì—¬ë¶€: {has_conflicts}")
        print(f"ğŸ” verified_facts: {verified_facts}")
        
        if has_conflicts:
            contradiction_warning = f"""

**ğŸš¨ ìƒí˜¸ëª¨ìˆœ ê°ì§€ë¨ - í•„ìˆ˜ ì²˜ë¦¬ ê·œì¹™:**
{web_verification_text}

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:**
- ìƒí˜¸ëª¨ìˆœì— ì°¸ì—¬í•œ AIì—ê²Œ "í‹€ë¦° ì •ë³´ ì—†ìŒ"ì´ë¼ê³  í•˜ë©´ ì•ˆë©ë‹ˆë‹¤
- ìƒí˜¸ëª¨ìˆœì— ì°¸ì—¬í•œ AIì—ê²Œ "ì •í™•í•œ ì •ë³´ ì œê³µ"ì´ë¼ê³  í•˜ë©´ ì•ˆë©ë‹ˆë‹¤
- ë°˜ë“œì‹œ "í‹€ë¦° ì •ë³´"ì— êµ¬ì²´ì ì¸ ìƒí˜¸ëª¨ìˆœ ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”

**âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:**
- GPT-4o Mini: "í‹€ë¦° ì •ë³´: ì„¤ë¦½ì—°ë„ ë¶ˆí™•ì‹¤ (ë‹¤ë¥¸ AIì™€ ìƒì¶©)"
- Gemini 2.0 Flash Lite: "í‹€ë¦° ì •ë³´: ì„¤ë¦½ì—°ë„ ë¶ˆí™•ì‹¤ (ë‹¤ë¥¸ AIì™€ ìƒì¶©)"
- Claude 3.5 Haiku: "í‹€ë¦° ì •ë³´: ì„¤ë¦½ì—°ë„ ë¶ˆí™•ì‹¤ (ë‹¤ë¥¸ AIì™€ ìƒì¶©)"
"""

        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­
        if question_type == "code":
            # ì½”ë“œ ì§ˆë¬¸ ì „ìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ (í† í° ì ˆì•½)
            question_type_instruction = """
**ğŸ’» ì´ ì§ˆë¬¸ì€ ì½”ë“œ/í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ì…ë‹ˆë‹¤:**
- Wikipedia ê²€ì¦ ë¶ˆí•„ìš” - ì½”ë“œ í’ˆì§ˆ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”
- ì½”ë“œì˜ ì •í™•ì„±, ì™„ì „ì„±, ê°€ë…ì„±, ì‹¤í–‰ ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ì„¸ìš”
- ì—¬ëŸ¬ AIì˜ ì½”ë“œë¥¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì½”ë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì¡°í•©í•˜ì„¸ìš”
- ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í˜•ì‹(```python ... ```)ì„ ìœ ì§€í•˜ì„¸ìš”
"""
            
            # ì½”ë“œ ì§ˆë¬¸ ì „ìš© ê°„ë‹¨í•œ Judge í”„ë¡¬í”„íŠ¸ (í† í° ì ˆì•½)
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ì œê³µëœ AI ì½”ë“œ ë‹µë³€ë“¤:**
{model_responses_text}

**ìµœì  ë‹µë³€ ìƒì„± ê·œì¹™:**
1. **ë°˜ë“œì‹œ ì—¬ëŸ¬ AIì˜ ì½”ë“œë¥¼ ì¡°í•©** - ë‹¨ì¼ AIì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
2. ì—¬ëŸ¬ AIì˜ ì½”ë“œë¥¼ ë¹„êµí•˜ì—¬ **ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ì½”ë“œ**ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì¡°í•©í•˜ì„¸ìš”
3. ì½”ë“œê°€ **ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  ì™„ì „í•œì§€** í™•ì¸í•˜ì„¸ìš”
4. **ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í˜•ì‹**ì„ ìœ ì§€í•˜ì„¸ìš” (```python ... ```)
5. ì—¬ëŸ¬ ì½”ë“œì˜ ì¥ì ì„ ì¡°í•©í•˜ì—¬ ë” ë‚˜ì€ ì½”ë“œë¥¼ ë§Œë“œì„¸ìš” - ë‹¨ì¼ AI ì½”ë“œ ë³µì‚¬ ê¸ˆì§€

**ê° AI ì½”ë“œ í‰ê°€ ê¸°ì¤€:**
- **ì •í™•ì„±**: ìš”êµ¬ì‚¬í•­ ë§Œì¡± ì—¬ë¶€
- **ì™„ì „ì„±**: ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€
- **ê°€ë…ì„±**: ì½”ë“œ ê°€ë…ì„±
- **ìµœì ì„±**: íš¨ìœ¨ì„±ê³¼ ê°„ê²°ì„±

**ğŸš¨ ì¤‘ìš”: verification_results ì‘ì„± ê·œì¹™:**
ê° AIì˜ ì½”ë“œ ë‹µë³€ì—ì„œ:
- **adopted_info**: í•´ë‹¹ AIê°€ ì œê³µí•œ ì½”ë“œ ì¤‘ **ìœ ìš©í•˜ê³  ì •í™•í•œ ë¶€ë¶„**ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì˜ˆ: "```python\\n...\\n```" í˜•ì‹ì˜ ì½”ë“œ ë¸”ë¡)
- **rejected_info**: í•´ë‹¹ AIê°€ ì œê³µí•œ ì½”ë“œ ì¤‘ **ì˜¤ë¥˜ê°€ ìˆê±°ë‚˜ ë¶ˆì™„ì „í•œ ë¶€ë¶„**ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ [])
- **ë°˜ë“œì‹œ ê° AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬**í•˜ì—¬ adopted_info/rejected_infoì— í¬í•¨í•˜ì„¸ìš”
- **ì ˆëŒ€ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì§€ ë§ˆì„¸ìš”!** ê° AIê°€ ì œê³µí•œ ì½”ë“œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ adopted_infoì— í¬í•¨í•˜ì„¸ìš”

**ğŸ¨ optimal_answer í¬ë§·íŒ… ê·œì¹™ (í•„ìˆ˜!):**
- **ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡** í˜•ì‹ í•„ìˆ˜: ```python\nì½”ë“œ\n```
- ì½”ë“œ ì„¤ëª…ì€ **ê°„ë‹¨í•œ ë¬¸ë‹¨**ìœ¼ë¡œ ì‘ì„± (ì½”ë“œ ì „í›„)
- ì—¬ëŸ¬ ì˜ˆì œê°€ ìˆìœ¼ë©´ **## ì œëª©**ìœ¼ë¡œ êµ¬ë¶„

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "optimal_answer": "ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ì½”ë“œë¥¼ ì¡°í•©í•œ ìµœì  ì½”ë“œ (ë‹¨ì¼ AI ì½”ë“œ ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€, ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ í¬í•¨)",
  "verification_results": {{
    {verification_json_format}
  }},
  "confidence_score": "ì½”ë“œ í’ˆì§ˆ ì‹ ë¢°ë„ (0-100)",
  "contradictions_detected": [],
  "fact_verification": {{}},
  "analysis_rationale": "ì–´ë–¤ AIì˜ ì–´ë–¤ ì½”ë“œë¥¼ ì¡°í•©í–ˆëŠ”ì§€ì™€ ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…"
}}
"""
        elif question_type == "image":
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì§ˆë¬¸ - OCR/Ollama ê²€ì¦ ê²°ê³¼ ê¸°ë°˜**
- ê° AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ìµœì ì˜ ë‹µë³€ ìƒì„±
- Wikipedia ê²€ì¦ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)

**ì œê³µëœ AI ë‹µë³€ë“¤:**
{model_responses_text}
{contradiction_warning}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ í•µì‹¬ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!):**
1. **ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ AI ë‹µë³€ì˜ ì›ë³¸ ë¬¸ì¥ë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ë¬¸ì¥ ì‘ì„±/ìš”ì•½/ì¬êµ¬ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ì—¬ëŸ¬ AI ë‹µë³€ ë°˜ë“œì‹œ ì¡°í•©** - ë‹¨ì¼ ëª¨ë¸ ì„ íƒ ì ˆëŒ€ ê¸ˆì§€, ë‹¨ì¼ ëª¨ë¸ì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
3. **í• ë£¨ì‹œë„¤ì´ì…˜ ì ˆëŒ€ ê¸ˆì§€** - ìœ„ AI ë‹µë³€ì— ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš© ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
4. **optimal_answerëŠ” ë°˜ë“œì‹œ ìœ„ AI ë‹µë³€ë“¤ì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥ë“¤ë¡œë§Œ êµ¬ì„±** - ì ˆëŒ€ ìƒˆë¡œìš´ ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
5. **ê° AIì˜ ì›ë³¸ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬**í•˜ì—¬ adopted_info/rejected_info ì‘ì„±
6. **ê° AIë§ˆë‹¤ ë°˜ë“œì‹œ adopted_info ë˜ëŠ” rejected_info ì¤‘ í•˜ë‚˜ì—ëŠ” ë‚´ìš© í¬í•¨** (ë‘˜ ë‹¤ ë¹ˆ ë°°ì—´ ì ˆëŒ€ ê¸ˆì§€)
7. **optimal_answerëŠ” ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
8. **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:**
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì˜ˆì œ, ì½”ë“œ, ì„¤ëª… ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì„ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë‚´ìš© ì¶”ê°€ ê¸ˆì§€

**adopted_info/rejected_info ì‘ì„±:**
- adopted_info: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì›ë³¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬
- rejected_info: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ ë¶€ì •í™•í•˜ê±°ë‚˜ ëª¨ìˆœë˜ëŠ” ì •ë³´ë§Œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **ìƒí˜¸ ë°°íƒ€ì ** - ê°™ì€ ë¬¸ì¥ì´ ì–‘ìª½ì— ë™ì‹œ ì¡´ì¬ ê¸ˆì§€
- **adopted_info/rejected_infoëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì§ì ‘ ë³µì‚¬í•œ ë¬¸ì¥ì´ì–´ì•¼ í•¨**

**ë§ˆí¬ë‹¤ìš´ í¬ë§·:**
- ë¦¬ìŠ¤íŠ¸: `- í•­ëª©`
- ì œëª©: `## ì£¼ì œ`
- ê°•ì¡°: `**êµµê²Œ**`

JSON ì‘ë‹µ:
{{
  "optimal_answer": "ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©í•œ ë‹µë³€ (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€, ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ)",
  "verification_results": {{
    {verification_json_format}
  }},
  "confidence_score": "0-100",
  "contradictions_detected": ["ìƒí˜¸ëª¨ìˆœ ì‚¬í•­"],
  "fact_verification": {{"dates": [], "locations": [], "facts": []}},
  "analysis_rationale": "ì–´ë–¤ AIì˜ ì–´ë–¤ ì •ë³´ë¥¼ ì±„íƒ/ì œì™¸í–ˆëŠ”ì§€ ìƒì„¸íˆ ì„¤ëª…"
}}

**âš ï¸ optimal_answer ì‘ì„± ì‹œ í•„ìˆ˜ ì‚¬í•­:**
- **ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
- **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ë˜, ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
"""
        elif question_type == "opinion":
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ğŸ“Š ì˜ê²¬/ì¶”ì²œ ì§ˆë¬¸ - ë‹¤ìˆ˜ê²° ë°©ì‹ ì‚¬ìš©**
- ì—¬ëŸ¬ AIê°€ ê³µí†µì ìœ¼ë¡œ ì¶”ì²œí•˜ëŠ” í•­ëª©ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ì†Œìˆ˜ ì˜ê²¬ë„ í¬í•¨í•˜ë˜ ë‹¤ìˆ˜ ì˜ê²¬ ìš°ì„  ë°°ì¹˜

**ì œê³µëœ AI ë‹µë³€ë“¤:**
{model_responses_text}
{web_verification_text}
{contradiction_warning}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ í•µì‹¬ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!):**
1. **ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ AI ë‹µë³€ì˜ ì›ë³¸ ë¬¸ì¥ë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ë¬¸ì¥ ì‘ì„±/ìš”ì•½/ì¬êµ¬ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ì—¬ëŸ¬ AI ë‹µë³€ ë°˜ë“œì‹œ ì¡°í•©** - ë‹¨ì¼ ëª¨ë¸ ì„ íƒ ì ˆëŒ€ ê¸ˆì§€, ë‹¨ì¼ ëª¨ë¸ì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
3. **í• ë£¨ì‹œë„¤ì´ì…˜ ì ˆëŒ€ ê¸ˆì§€** - ìœ„ AI ë‹µë³€ì— ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš© ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
4. **optimal_answerëŠ” ë°˜ë“œì‹œ ìœ„ AI ë‹µë³€ë“¤ì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥ë“¤ë¡œë§Œ êµ¬ì„±** - ì ˆëŒ€ ìƒˆë¡œìš´ ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
5. **ê° AIì˜ ì›ë³¸ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬**í•˜ì—¬ adopted_info/rejected_info ì‘ì„±
6. **ê° AIë§ˆë‹¤ ë°˜ë“œì‹œ adopted_info ë˜ëŠ” rejected_info ì¤‘ í•˜ë‚˜ì—ëŠ” ë‚´ìš© í¬í•¨** (ë‘˜ ë‹¤ ë¹ˆ ë°°ì—´ ì ˆëŒ€ ê¸ˆì§€)
7. **optimal_answerëŠ” ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
8. **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:**
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì˜ˆì œ, ì½”ë“œ, ì„¤ëª… ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì„ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
- ë‹¨ìˆœ ì¸ì‚¬ ì§ˆë¬¸ì—ëŠ” ë‹¨ìˆœ ì¸ì‚¬ ë‹µë³€ë§Œ ì œê³µ (ì¶”ê°€ ì„¤ëª… ê¸ˆì§€)

**adopted_info/rejected_info ì‘ì„±:**
- adopted_info: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ ìœ ìš©í•œ ì›ë³¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬
- rejected_info: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ Wikipedia ë¶ˆì¼ì¹˜ ë˜ëŠ” ìƒì¶© ì •ë³´ë§Œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
- **ìƒí˜¸ ë°°íƒ€ì ** - ê°™ì€ ë¬¸ì¥ì´ ì–‘ìª½ì— ë™ì‹œ ì¡´ì¬ ê¸ˆì§€
- **adopted_info/rejected_infoëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì§ì ‘ ë³µì‚¬í•œ ë¬¸ì¥ì´ì–´ì•¼ í•¨**

**ë§ˆí¬ë‹¤ìš´ í¬ë§·:**
- ë¦¬ìŠ¤íŠ¸: `- í•­ëª©`
- ì œëª©: `## ì£¼ì œ`
- ê°•ì¡°: `**êµµê²Œ**`

JSON ì‘ë‹µ:
{{
  "optimal_answer": "ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©í•œ ë‹µë³€ (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€, ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ)",
  "verification_results": {{
    {verification_json_format}
  }},
  "confidence_score": "0-100",
  "contradictions_detected": ["ìƒí˜¸ëª¨ìˆœ ì‚¬í•­"],
  "fact_verification": {{"dates": [], "locations": [], "facts": []}},
  "analysis_rationale": "ì–´ë–¤ AIì˜ ì–´ë–¤ ì •ë³´ë¥¼ ì±„íƒ/ì œì™¸í–ˆëŠ”ì§€ ìƒì„¸íˆ ì„¤ëª…"
}}

**âš ï¸ optimal_answer ì‘ì„± ì‹œ í•„ìˆ˜ ì‚¬í•­:**
- **ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
- **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ì§ˆë¬¸ì´ "hi", "ì•ˆë…•" ê°™ì€ ë‹¨ìˆœ ì¸ì‚¬ë¼ë©´ â†’ ìœ„ AI ë‹µë³€ì˜ ì¸ì‚¬ ë¬¸ì¥ë“¤ì„ ì¡°í•© (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ê¸ˆì§€)
- ì§ˆë¬¸ì´ í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ â†’ í”„ë¡œê·¸ë˜ë° ì˜ˆì œë‚˜ ì½”ë“œ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
"""
        else:
            # ì¼ë°˜/ì‚¬ì‹¤ ì§ˆë¬¸ (factual, general, document, image, creative ë“±)
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}

**ğŸ” ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ - Wikipedia ê²€ì¦ ê¸°ì¤€ ì‚¬ìš©**
- Wikipediaì™€ **ëª…í™•íˆ ëª¨ìˆœ**ë˜ëŠ” ì •ë³´ë§Œ ì œì™¸
- Wikipediaì— ì—†ì§€ë§Œ **ëª¨ìˆœë˜ì§€ ì•ŠëŠ”** ìœ ìš©í•œ ì •ë³´ëŠ” í¬í•¨ (í•™ê³¼, íŠ¹ì§•, ì—­ì‚¬ ë“±)
- ì—¬ëŸ¬ AI ë‹µë³€ ì¢…í•©í•˜ì—¬ **í’ë¶€í•œ ìµœì  ë‹µë³€** ìƒì„±

**ì œê³µëœ AI ë‹µë³€ë“¤:**
{model_responses_text}
{web_verification_text}
{contradiction_warning}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ í•µì‹¬ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!):**
1. **ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ AI ë‹µë³€ì˜ ì›ë³¸ ë¬¸ì¥ë§Œ ì‚¬ìš©** - ìƒˆë¡œìš´ ë¬¸ì¥ ì‘ì„±/ìš”ì•½/ì¬êµ¬ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ì—¬ëŸ¬ AI ë‹µë³€ ë°˜ë“œì‹œ ì¡°í•©** - ë‹¨ì¼ ëª¨ë¸ ì„ íƒ ì ˆëŒ€ ê¸ˆì§€, ë‹¨ì¼ ëª¨ë¸ì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
3. **í• ë£¨ì‹œë„¤ì´ì…˜ ì ˆëŒ€ ê¸ˆì§€** - ìœ„ AI ë‹µë³€ì— ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš© ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
4. **optimal_answerëŠ” ë°˜ë“œì‹œ ìœ„ AI ë‹µë³€ë“¤ì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥ë“¤ë¡œë§Œ êµ¬ì„±** - ì ˆëŒ€ ìƒˆë¡œìš´ ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
5. **optimal_answerëŠ” ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
6. **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:**
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì˜ˆì œ, ì½”ë“œ, ì„¤ëª… ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì„ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
- ë‹¨ìˆœ ì¸ì‚¬ ì§ˆë¬¸ì—ëŠ” ë‹¨ìˆœ ì¸ì‚¬ ë‹µë³€ë§Œ ì œê³µ (ì¶”ê°€ ì„¤ëª… ê¸ˆì§€)
- ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬ optimal_answerì— ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€

**ì •ë³´ ì±„íƒ ê¸°ì¤€:**
- âœ… **adopted_info**: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ Wikipedia ì¼ì¹˜ ì •ë³´ + ëª¨ìˆœë˜ì§€ ì•ŠëŠ” ìœ ìš©í•œ ì •ë³´ (ì›ë³¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬)
- âŒ **rejected_info**: ìœ„ AI ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ Wikipedia ëª…í™•íˆ ëª¨ìˆœë˜ëŠ” ì •ë³´ë§Œ (ì›ë³¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬)
- **ê° AIë§ˆë‹¤ ë°˜ë“œì‹œ adopted_info ë˜ëŠ” rejected_info ì¤‘ í•˜ë‚˜ì—ëŠ” ë‚´ìš© í¬í•¨** (ë‘˜ ë‹¤ ë¹ˆ ë°°ì—´ ì ˆëŒ€ ê¸ˆì§€)
- **ìƒí˜¸ ë°°íƒ€ì ** - ê°™ì€ ë¬¸ì¥ì´ ì–‘ìª½ì— ë™ì‹œ ì¡´ì¬ ê¸ˆì§€
- **adopted_info/rejected_infoëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì§ì ‘ ë³µì‚¬í•œ ë¬¸ì¥ì´ì–´ì•¼ í•¨**

**ë§ˆí¬ë‹¤ìš´ í¬ë§·:**
- ì œëª©: `## ì£¼ì œ`, `### ì†Œì£¼ì œ`
- ë¦¬ìŠ¤íŠ¸: `- í•­ëª©`
- ê°•ì¡°: `**êµµê²Œ**`
- ë¬¸ë‹¨: 2-3ë¬¸ì¥, ë¹ˆ ì¤„ë¡œ êµ¬ë¶„

JSON ì‘ë‹µ:
{{
  "optimal_answer": "ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©í•œ ë‹µë³€ (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€, ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ)",
  "verification_results": {{
    {verification_json_format}
  }},
  "confidence_score": "0-100",
  "contradictions_detected": ["ìƒí˜¸ëª¨ìˆœ ì‚¬í•­"],
  "fact_verification": {{"dates": [], "locations": [], "facts": []}},
  "analysis_rationale": "ì–´ë–¤ AIì˜ ì–´ë–¤ ì •ë³´ë¥¼ ì±„íƒ/ì œì™¸í–ˆëŠ”ì§€, Wikipedia ê²€ì¦ ê²°ê³¼ ë°˜ì˜ ë°©ë²• ìƒì„¸ ì„¤ëª…"
}}

**âš ï¸ optimal_answer ì‘ì„± ì‹œ í•„ìˆ˜ ì‚¬í•­:**
- **ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
- **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ì§ˆë¬¸ì´ "hi", "ì•ˆë…•" ê°™ì€ ë‹¨ìˆœ ì¸ì‚¬ë¼ë©´ â†’ ìœ„ AI ë‹µë³€ì˜ ì¸ì‚¬ ë¬¸ì¥ë“¤ì„ ì¡°í•© (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ê¸ˆì§€)
- ì§ˆë¬¸ì´ í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ â†’ í”„ë¡œê·¸ë˜ë° ì˜ˆì œë‚˜ ì½”ë“œ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

"""

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì²´í¬
        prompt_length = len(judge_prompt)
        print(f"ğŸ“ ì‹¬íŒ ëª¨ë¸ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length}ì ({prompt_length // 1000}Kì)")
        
        # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½ (ê° LLM ì‘ë‹µì„ ìš”ì•½)
        if prompt_length > 50000:  # 50Kì ì´ìƒì´ë©´
            print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({prompt_length}ì). LLM ì‘ë‹µì„ ìš”ì•½í•©ë‹ˆë‹¤...")
            # ê° LLM ì‘ë‹µì„ ìš”ì•½ (ì²˜ìŒ 4000ì + ë 500ìë§Œ ìœ ì§€)
            summarized_responses = {}
            for model_name, response in llm_responses.items():
                if len(response) > 5000:
                    summarized_responses[model_name] = response[:4000] + "\n\n... (ì¤‘ëµ) ...\n\n" + response[-500:]
                    print(f"  - {model_name}: {len(response)}ì â†’ {len(summarized_responses[model_name])}ìë¡œ ìš”ì•½")
                else:
                    summarized_responses[model_name] = response
            
            llm_responses = summarized_responses
            
            # í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
            model_sections = []
            for model_name, response in llm_responses.items():
                model_sections.append(f"[{model_name} ë‹µë³€]\n{response}")
            model_responses_text = "\n\n".join(model_sections)
            
            # í”„ë¡¬í”„íŠ¸ ì „ì²´ ì¬êµ¬ì„± (model_responses_text ë¶€ë¶„ë§Œ êµì²´)
            judge_prompt = judge_prompt.replace(
                judge_prompt.split(model_responses_text)[0] + model_responses_text,
                judge_prompt.split(model_responses_text)[0] + model_responses_text
            )
            # ì‹¤ì œë¡œëŠ” ìœ„ ë°©ì‹ì´ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ ì¬êµ¬ì„±
            judge_prompt = f"""
ì§ˆë¬¸: {user_question}
{question_type_instruction}

{model_responses_text}
{web_verification_text}
{contradiction_warning}

**ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­ (ë§¤ìš° ì¤‘ìš”!):**
1. **ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ AI ë‹µë³€ì˜ ì›ë³¸ ë¬¸ì¥ë§Œ ì‚¬ìš©í•˜ì„¸ìš”** - ìƒˆë¡œìš´ ë¬¸ì¥ ì‘ì„±/ìš”ì•½/ì¬êµ¬ì„± ì ˆëŒ€ ê¸ˆì§€
2. **ì—¬ëŸ¬ AI ë‹µë³€ ë°˜ë“œì‹œ ì¡°í•©** - ë‹¨ì¼ ëª¨ë¸ ì„ íƒ ì ˆëŒ€ ê¸ˆì§€, ë‹¨ì¼ ëª¨ë¸ì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
3. **ì ˆëŒ€ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”**
4. **AIê°€ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ë§›ì§‘, ì¹´í˜, ì¥ì†Œ, ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”**
5. **í• ë£¨ì‹œë„¤ì´ì…˜ ì ˆëŒ€ ê¸ˆì§€!** - ìœ„ ë‹µë³€ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì‘ì„± ê¸ˆì§€
6. **ìœ„ì— ì œê³µëœ AI ë‹µë³€ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”** - 1ê°œë§Œ ìˆìœ¼ë©´ "ë‹¤ë¥¸ AI"ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
7. **optimal_answerì˜ ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ìœ„ AI ë‹µë³€ë“¤ì—ì„œ ì§ì ‘ ì¶”ì¶œí•œ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤** - ì ˆëŒ€ ìƒˆë¡œìš´ ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
8. **optimal_answerëŠ” ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©** - ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€
9. **ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ** - AIê°€ ë‹µë³€í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
10. **adopted_info/rejected_infoëŠ” ë°˜ë“œì‹œ ìœ„ì— ì œê³µëœ í•´ë‹¹ AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ì§ì ‘ ë³µì‚¬í•œ ë¬¸ì¥ì´ì–´ì•¼ í•©ë‹ˆë‹¤**
11. **ê° AIë§ˆë‹¤ ë°˜ë“œì‹œ adopted_info ë˜ëŠ” rejected_info ì¤‘ í•˜ë‚˜ì—ëŠ” ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”** (ë‘˜ ë‹¤ ë¹ˆ ë°°ì—´ ì ˆëŒ€ ê¸ˆì§€)

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:**
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì˜ˆì œ, ì½”ë“œ, ì„¤ëª… ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì„ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë‚´ìš© ì¶”ê°€ ê¸ˆì§€
- ë‹¨ìˆœ ì¸ì‚¬ ì§ˆë¬¸ì—ëŠ” ë‹¨ìˆœ ì¸ì‚¬ ë‹µë³€ë§Œ ì œê³µ (ì¶”ê°€ ì„¤ëª… ê¸ˆì§€)
- ë‹¨ì¼ AIì˜ ë‹µë³€ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬ optimal_answerì— ì‚¬ìš©í•˜ëŠ” ê²ƒ ì ˆëŒ€ ê¸ˆì§€

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:

{{
  "optimal_answer": "ë°˜ë“œì‹œ 2ê°œ ì´ìƒì˜ AI ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì—¬ ì¡°í•©í•œ ë‹µë³€ (ë‹¨ì¼ AI ë‹µë³€ ë³µì‚¬ ì ˆëŒ€ ê¸ˆì§€, ê° AIê°€ ì‹¤ì œë¡œ ë‹µë³€í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ)",
  "verification_results": {{
    {verification_json_format}
  }},
  "confidence_score": "ì „ì²´ ì‘ë‹µì— ëŒ€í•œ ì‹ ë¢°ë„ (0-100)",
  "contradictions_detected": ["ë°œê²¬ëœ ìƒí˜¸ëª¨ìˆœ ì‚¬í•­ë“¤"],
  "fact_verification": {{
    "dates": ["ê²€ì¦ëœ ì—°ë„ ì •ë³´ë“¤"],
    "locations": ["ê²€ì¦ëœ ìœ„ì¹˜ ì •ë³´ë“¤"],
    "facts": ["ê²€ì¦ëœ ê¸°íƒ€ ì‚¬ì‹¤ë“¤"]
  }},
  "analysis_rationale": "ìµœì  ë‹µë³€ ìƒì„± ê·¼ê±° - ê° AIì˜ ë‹µë³€ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì±„íƒí–ˆëŠ”ì§€, ì–´ë–¤ ì •ë³´ê°€ í‹€ë ¸ê±°ë‚˜ ìƒë°˜ë˜ì–´ì„œ ì œì™¸í–ˆëŠ”ì§€, Wikipedia ê²€ì¦ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ ë°˜ì˜í–ˆëŠ”ì§€ ìƒì„¸íˆ ì„¤ëª…"
}}

**âš ï¸ optimal_answer ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:**
- ì§ˆë¬¸ì´ "hi", "ì•ˆë…•" ê°™ì€ ë‹¨ìˆœ ì¸ì‚¬ë¼ë©´ â†’ ìœ„ AI ë‹µë³€ì˜ ì¸ì‚¬ ë¬¸ì¥ë§Œ ì‚¬ìš© (ì¶”ê°€ ì„¤ëª… ì ˆëŒ€ ê¸ˆì§€)
- ì§ˆë¬¸ì´ í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ë©´ â†’ í”„ë¡œê·¸ë˜ë° ì˜ˆì œë‚˜ ì½”ë“œ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
- ìœ„ AI ë‹µë³€ì— ì—†ëŠ” ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
"""
            print(f"ğŸ“ ìš”ì•½ í›„ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(judge_prompt)}ì")
        
        # ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ
        print(f"ğŸ“ ì‹¬íŒ ëª¨ë¸({judge_model}) í˜¸ì¶œ ì‹œì‘... (í”„ë¡¬í”„íŠ¸: {len(judge_prompt)}ì)")
        try:
            judge_response = call_judge_model(judge_model, judge_prompt)
            print(f"âœ… ì‹¬íŒ ëª¨ë¸ ì‘ë‹µ ë°›ìŒ: {len(judge_response) if judge_response else 0}ì")
            if judge_response:
                print(f"ğŸ“„ ì‹¬íŒ ëª¨ë¸ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {judge_response[:300]}...")
            else:
                print(f"âŒ ì‹¬íŒ ëª¨ë¸ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            import traceback
            print(f"âŒ ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            print(f"âŒ ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            raise
        
        # ê²°ê³¼ íŒŒì‹±
        print(f"ğŸ“ ì‹¬íŒ ëª¨ë¸ ì‘ë‹µ íŒŒì‹± ì‹œì‘...")
        print(f"ğŸ“„ ì‹¬íŒ ëª¨ë¸ ì „ì²´ ì‘ë‹µ (ì²˜ìŒ 2000ì): {judge_response[:2000]}...")
        print(f"ğŸ“„ ì‹¬íŒ ëª¨ë¸ ì „ì²´ ì‘ë‹µ (ë 500ì): ...{judge_response[-500:]}")
        parsed_result = parse_judge_response(judge_response, judge_model, llm_responses)
        print(f"âœ… íŒŒì‹± ì™„ë£Œ: {list(parsed_result.keys()) if isinstance(parsed_result, dict) else 'N/A'}")
        print(f"ğŸ“„ íŒŒì‹±ëœ ìµœì ì˜_ë‹µë³€: {parsed_result.get('ìµœì ì˜_ë‹µë³€', '')[:300]}...")
        
        # ì›¹ ê²€ì¦ ì •ë³´ ì¶”ê°€
        parsed_result["ì›¹_ê²€ì¦_ì‚¬ìš©"] = web_verification_used
        if verified_facts:
            parsed_result["ì›¹_ê²€ì¦_ê²°ê³¼"] = verified_facts
            parsed_result["ê²€ì¦_ì„±ëŠ¥"] = {
                "ìƒí˜¸ëª¨ìˆœ_ê°ì§€": len(conflicts),
                "ì›¹_ê²€ì¦_ì„±ê³µ": len(verified_facts),
                "ë¹„ìš©": "$0.003" if web_verification_used else "$0.000"
            }
        
        # Wikipedia ê²€ì¦ ì—°ë„ë¡œ í›„ì²˜ë¦¬ (ì˜ëª»ëœ ì—°ë„ ì œê±°)
        if web_verification_used and verified_facts:
            verified_year = None
            for fact_type, verification in verified_facts.items():
                if verification.get('verified') and verification.get('extracted_year'):
                    verified_year = verification['extracted_year']
                    break
            
            if verified_year and parsed_result.get("ìµœì ì˜_ë‹µë³€"):
                import re
                optimal_answer = parsed_result["ìµœì ì˜_ë‹µë³€"]
                
                # ìµœì  ë‹µë³€ì—ì„œ ë‹¤ë¥¸ ì—°ë„ë¥¼ ì°¾ìŒ
                years_in_answer = re.findall(r'(\d{4})ë…„', optimal_answer)
                wrong_years = [y for y in years_in_answer if y != verified_year and 1900 <= int(y) <= 2024]
                
                # ì˜ëª»ëœ ì—°ë„ê°€ ìˆìœ¼ë©´ ì œê±°
                if wrong_years:
                    print(f"âš ï¸ Wikipedia ê²€ì¦ ì—°ë„ {verified_year}ì™€ ë‹¤ë¥¸ ì—°ë„ ë°œê²¬: {wrong_years}")
                    for wrong_year in wrong_years:
                        # í•´ë‹¹ ì—°ë„ë¥¼ í¬í•¨í•œ ë¬¸ì¥ íŒ¨í„´ ì°¾ê¸°
                        patterns_to_remove = [
                            rf'{wrong_year}ë…„.*?ì„¤ë¦½.*?[.!ê°€-í£]',
                            rf'{wrong_year}ë…„.*?ê°œêµ.*?[.!ê°€-í£]',
                            rf'{wrong_year}ë…„.*?ì°½ë¦½.*?[.!ê°€-í£]',
                            rf'{wrong_year}ë…„ì—.*?[.!ê°€-í£]{0,50}',
                        ]
                        
                        for pattern in patterns_to_remove:
                            optimal_answer = re.sub(pattern, '', optimal_answer, flags=re.DOTALL)
                    
                    # ì •ë¦¬
                    optimal_answer = re.sub(r'[ \t]+', ' ', optimal_answer)
                    optimal_answer = re.sub(r'\n{3,}', '\n\n', optimal_answer)
                    optimal_answer = optimal_answer.strip()
                    
                    # ìµœì¢… ë‹µë³€ì´ ë¹„ì—ˆìœ¼ë©´ ê²€ì¦ëœ ì—°ë„ë¡œ ì¬êµ¬ì„±
                    if not optimal_answer or len(optimal_answer) < 50:
                        # ì›ë˜ LLM ë‹µë³€ì—ì„œ ê²€ì¦ëœ ì—°ë„ë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì°¾ê¸°
                        if llm_responses:
                            for model, response in llm_responses.items():
                                if verified_year in response:
                                    # ê²€ì¦ëœ ì—°ë„ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ
                                    sentences = re.split(r'[.!]\s+', response)
                                    matching_sentences = [s for s in sentences if verified_year in s and 150 <= len(s) <= 400]
                                    if matching_sentences:
                                        optimal_answer = matching_sentences[0]
                                        break
                        
                        # ì—¬ì „íˆ ë¹„ì—ˆìœ¼ë©´ ìƒì„±
                        if not optimal_answer:
                            optimal_answer = f"Wikipedia ê²€ì¦ ê²°ê³¼ì— ë”°ë¥´ë©´ ì¶©ë¶ëŒ€í•™êµëŠ” {verified_year}ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."
                    
                    parsed_result["ìµœì ì˜_ë‹µë³€"] = optimal_answer
                    print(f"âœ… Wikipedia í›„ì²˜ë¦¬ ì™„ë£Œ: {verified_year}ë…„ ìœ ì§€, {wrong_years}ë…„ ì œê±°")
        
        print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì™„ë£Œ: ì›¹ê²€ì¦={web_verification_used}, ìƒí˜¸ëª¨ìˆœ={len(conflicts)}")
        
        return parsed_result
        
    except Exception as e:
        print(f"âŒ ì‹¬íŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        
        # í´ë°±: ê°€ì¥ ê¸´ ì‘ë‹µì„ ìµœì  ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
        if llm_responses:
            longest_response = max(llm_responses.values(), key=len)
            return {
                "ìµœì ì˜_ë‹µë³€": longest_response,
                "llm_ê²€ì¦_ê²°ê³¼": {
                    model: {
                        "ì •í™•ì„±": "âŒ",
                        "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨ - Judge ëª¨ë¸ ì˜¤ë¥˜",
                        "ì‹ ë¢°ë„": "0",
                        "ì±„íƒëœ_ì •ë³´": [],
                        "ì œì™¸ëœ_ì •ë³´": []
                    }
                    for model in llm_responses.keys()
                },
                "ì‹¬íŒëª¨ë¸": judge_model,
                "ìƒíƒœ": "ê²€ì¦ ì‹¤íŒ¨",
                "ì›ë³¸_ì‘ë‹µ": llm_responses
            }
        return {
            "ìµœì ì˜_ë‹µë³€": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "llm_ê²€ì¦_ê²°ê³¼": {},
            "ì‹¬íŒëª¨ë¸": judge_model,
            "ìƒíƒœ": "ì˜¤ë¥˜",
            "ì›ë³¸_ì‘ë‹µ": llm_responses or {}
        }



def call_judge_model(model_name, prompt):
    """ì‹¬íŒ ëª¨ë¸ í˜¸ì¶œ"""
    try:
        if model_name in ['GPT-5', 'GPT-3.5-turbo', 'GPT-4', 'GPT-4o', 'GPT-4o-mini']:
            # OpenAI ëª¨ë¸ ì‚¬ìš©
            import openai
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            
            client = openai.OpenAI(api_key=openai_api_key)
            
            # ëª¨ë¸ëª…ì„ OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            openai_model_name = model_name.lower().replace('-', '-')
            if model_name == 'GPT-5':
                # GPT-5ëŠ” ì‹¤ì œë¡œ o1, o3 ë“±ì˜ ìµœì‹  ëª¨ë¸ì¼ ìˆ˜ ìˆìŒ
                # ì‚¬ìš©ìê°€ ì§€ì •í•œ ëª¨ë¸ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (o1, o3 ë“±)
                openai_model_name = 'gpt-5'  # ì‹¤ì œ ëª¨ë¸ëª… ì‚¬ìš© ì‹œë„
                print(f"ğŸ” GPT-5 ëª¨ë¸ëª…: {openai_model_name} (API í˜¸ì¶œ ì‹œë„)")
            elif model_name == 'GPT-4':
                openai_model_name = 'gpt-4'
            elif model_name == 'GPT-4o':
                openai_model_name = 'gpt-4o'
            elif model_name == 'GPT-4o-mini':
                openai_model_name = 'gpt-4o-mini'
            elif model_name == 'GPT-3.5-turbo':
                openai_model_name = 'gpt-3.5-turbo'
            
            # ìµœì‹  OpenAI ëª¨ë¸(o1, o3 ë“±)ì€ max_completion_tokens ì‚¬ìš© ë° temperature ë¯¸ì§€ì›
            # ê¸°ì¡´ ëª¨ë¸ì€ max_tokens ì‚¬ìš©
            is_latest_model = any(model in openai_model_name.lower() for model in ['o1', 'o3', 'gpt-5'])
            
            api_params = {
                "model": openai_model_name,
                "messages": [
                    {"role": "system", "content": """ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—­í• ì€ ê° AIì˜ ë‹µë³€ì„ **ìˆëŠ” ê·¸ëŒ€ë¡œ ë¶„ì„**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸš¨ ì ˆëŒ€ ê·œì¹™:
1. ê° AIê°€ **ì‹¤ì œë¡œ ë§í•œ ë‚´ìš©ë§Œ** adopted_info/rejected_infoì— ë³µì‚¬
2. ê° AIì˜ ë‹µë³€ì€ **ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤** - ëª¨ë“  AIê°€ ë˜‘ê°™ì€ ë¬¸ì¥ì„ ë§í•  í•„ìš”ëŠ” ì—†ìŒ
3. AIê°€ íŠ¹ì • ì •ë³´(ì—°ë„, ìœ„ì¹˜, ì´ë¦„ ë“±)ë¥¼ ë§í–ˆë‹¤ë©´ â†’ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì ˆëŒ€ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”!)
4. ì ˆëŒ€ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
5. ê° AIê°€ ì‹¤ì œë¡œ ë§í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ë©´ ì•ˆë¨ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€!)
6. **íŠ¹íˆ ì£¼ì˜**: AIê°€ "1946ë…„"ì´ë¼ê³  ë§í–ˆë‹¤ë©´, ì ˆëŒ€ "1951ë…„"ìœ¼ë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”!
7. adopted_info/rejected_infoì—ëŠ” ê° AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì•¼ í•©ë‹ˆë‹¤

âœ… ì˜¬ë°”ë¥¸ ë¶„ì„:
- ê° AIì˜ ì›ë³¸ ë‹µë³€ì—ì„œ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬ adopted_info/rejected_infoì— í¬í•¨
- ê° AIë§ˆë‹¤ ë‹¤ë¥¸ ë‚´ìš©ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ (ì´ê²ƒì´ ì •ìƒ)
- Wikipedia ê²€ì¦ ê²°ê³¼ê°€ ìˆë‹¤ë©´, ê° AIì˜ ì›ë³¸ ë‹µë³€ê³¼ ë¹„êµí•˜ì—¬ ì¼ì¹˜/ë¶ˆì¼ì¹˜ íŒë‹¨

âŒ ì˜ëª»ëœ ë¶„ì„ (í• ë£¨ì‹œë„¤ì´ì…˜):
- ëª¨ë“  AIê°€ ë˜‘ê°™ì€ ë¬¸ì¥ì„ ê°€ì§„ adopted_info (ì´ëŠ” ë¶ˆê°€ëŠ¥í•¨)
- ì›ë³¸ ë‹µë³€ì— ì—†ëŠ” ì •ë³´ë¥¼ ìƒˆë¡œ ë§Œë“¤ì–´ë‚´ê¸° (ì˜ˆ: AIê°€ 1946ë…„ì´ë¼ê³  í–ˆëŠ”ë° 1951ë…„ìœ¼ë¡œ ë°”ê¾¸ê¸°)
- ìµœì  ë‹µë³€ì˜ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ê° AIì˜ ë‹µë³€ì„ ë°”ê¾¸ê¸°

**ë‹¹ì‹ ì€ ê° AIì˜ ì›ë³¸ ë‹µë³€ì„ ì½ê³ , ê° AIê°€ ë­ë¼ê³  í–ˆëŠ”ì§€ ì •í™•íˆ ë¶„ì„í•˜ì„¸ìš”.**

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""},
                    {"role": "user", "content": prompt}
                ],
            }
            
            # ìµœì‹  ëª¨ë¸ì€ temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            if not is_latest_model:
                api_params["temperature"] = 0.0  # ë” ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
            
            completion_limit = get_openai_completion_limit(openai_model_name)
            # ìµœì‹  ëª¨ë¸ì€ max_completion_tokens, ê¸°ì¡´ ëª¨ë¸ì€ max_tokens ì‚¬ìš©
            if is_latest_model:
                api_params["max_completion_tokens"] = completion_limit
            else:
                api_params["max_tokens"] = completion_limit
                api_params["response_format"] = {"type": "json_object"}  # JSON í˜•ì‹ ê°•ì œ
            
            response = client.chat.completions.create(**api_params)
            
            response_content = response.choices[0].message.content.strip()
            
            # ì‘ë‹µì´ ì˜ë ¸ëŠ”ì§€ í™•ì¸
            if response.choices[0].finish_reason == 'length':
                print(f"âš ï¸ {model_name} ì‘ë‹µì´ í† í° ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤ (finish_reason: length)")
                response_content += "\n\n[ì‘ë‹µì´ í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤. ë” ê¸´ ë‹µë³€ì´ í•„ìš”í•˜ì‹œë©´ ì§ˆë¬¸ì„ ë‚˜ëˆ„ì–´ ì£¼ì„¸ìš”.]"
            elif response.choices[0].finish_reason:
                print(f"ğŸ“ {model_name} ì‘ë‹µ ì™„ë£Œ (finish_reason: {response.choices[0].finish_reason})")
            
            print(f"ğŸ“ {model_name} ì‘ë‹µ ê¸¸ì´: {len(response_content)}ì")
            
            return response_content
            
        elif model_name == 'Claude-3.5-haiku':
            # Claude ëª¨ë¸ ì‚¬ìš© (ëŒ€ì•ˆ)
            import anthropic
            anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
            if not anthropic_api_key:
                raise ValueError("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            
            client = anthropic.Anthropic(api_key=anthropic_api_key)
            response = client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=1500,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text
            
        elif model_name == 'LLaMA 3.1 8B':
            # LLaMA ëª¨ë¸ ì‚¬ìš© (Groq API)
            import groq
            groq_api_key = os.getenv('GROQ_API_KEY')
            if not groq_api_key:
                raise ValueError("Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            
            client = groq.Groq(api_key=groq_api_key)
            response = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ì‹¤ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ê³  í‹€ë¦° ì •ë³´ë¥¼ ëª…í™•íˆ ì§€ì í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ GPT-5 ì‚¬ìš©
            return call_judge_model('GPT-5', prompt)
            
    except Exception as e:
        print(f"âŒ ì‹¬íŒ ëª¨ë¸ {model_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        
        # í´ë°±: ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹œë„ (GPT-5 -> GPT-4o -> GPT-4o-mini -> GPT-3.5-turbo)
        fallback_models = {
            'GPT-5': 'GPT-4o',
            'GPT-4o': 'GPT-4o-mini',
            'GPT-4o-mini': 'GPT-3.5-turbo',
            'GPT-3.5-turbo': None
        }
        
        fallback_model = fallback_models.get(model_name)
        if fallback_model:
            print(f"ğŸ”„ {model_name} ì‹¤íŒ¨, {fallback_model}ë¡œ í´ë°± ì‹œë„...")
            try:
                return call_judge_model(fallback_model, prompt)
            except Exception as fallback_error:
                print(f"âŒ í´ë°± ëª¨ë¸ {fallback_model}ë„ ì‹¤íŒ¨: {fallback_error}")
                raise e
        else:
            raise e



def parse_judge_response(judge_response, judge_model, llm_responses=None):
    """ì‹¬íŒ ëª¨ë¸ JSON ì‘ë‹µ íŒŒì‹±"""
    try:
        import json
        import re
        
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', judge_response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            print(f"ğŸ“‹ ì¶”ì¶œëœ JSON ë¬¸ìì—´ (ì²˜ìŒ 500ì): {json_str[:500]}...")
            print(f"ğŸ“‹ ì¶”ì¶œëœ JSON ë¬¸ìì—´ (ë 500ì): ...{json_str[-500:]}")
            try:
                parsed_data = json.loads(json_str)
                print(f"âœ… JSON íŒŒì‹± ì„±ê³µ!")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"âŒ JSON ë¬¸ìì—´ ìœ„ì¹˜: {e.pos}")
                print(f"âŒ JSON ë¬¸ìì—´ (ì˜¤ë¥˜ ìœ„ì¹˜ ì£¼ë³€): {json_str[max(0, e.pos-100):e.pos+100]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°±
                return create_fallback_result(judge_model, llm_responses)
            
            result = {
                "ìµœì ì˜_ë‹µë³€": parsed_data.get("optimal_answer", ""),
                "llm_ê²€ì¦_ê²°ê³¼": {},
                "ì‹¬íŒëª¨ë¸": judge_model,
                "ìƒíƒœ": "ì„±ê³µ",
                "ì‹ ë¢°ë„": parsed_data.get("confidence_score", "50"),
                "ìƒí˜¸ëª¨ìˆœ": parsed_data.get("contradictions_detected", []),
                "ì‚¬ì‹¤ê²€ì¦": parsed_data.get("fact_verification", {}),
                "ë¶„ì„_ê·¼ê±°": parsed_data.get("analysis_rationale", "")
            }
            
            # ê²€ì¦ ê²°ê³¼ íŒŒì‹± (ìƒí˜¸ëª¨ìˆœ ìš°ì„  ì²˜ë¦¬)
            verification_results = parsed_data.get("verification_results", {})
            contradictions = parsed_data.get("contradictions_detected", [])
            
            # ì²˜ë¦¬ëœ ëª¨ë¸ ì¶”ì 
            processed_models = set()
            
            for model_name, verification in verification_results.items():
                processed_models.add(model_name)
                errors_text = verification.get("errors", "ì˜¤ë¥˜ ì—†ìŒ")
                
                # ìƒí˜¸ëª¨ìˆœì´ ê°ì§€ëœ ê²½ìš° ê°•ì œë¡œ ì˜¤ë¥˜ ì²˜ë¦¬
                has_contradiction = any(
                    model_name.lower() in str(contradiction).lower() or 
                    "ìƒì¶©" in errors_text or 
                    "ë¶ˆí™•ì‹¤" in errors_text or
                    "ë‹¤ë¥¸ AI" in errors_text
                    for contradiction in contradictions
                )
                
                # ê¸°ë³¸ ì •í™•ì„± íŒë‹¨
                is_accurate_by_default = (
                    verification.get("accuracy") == "ì •í™•" or
                    errors_text.lower() in ["ì—†ìŒ", "ì˜¤ë¥˜ ì—†ìŒ", "ì •í™•í•œ ì •ë³´ ì œê³µ", "ì •í™•í•œ ì •ë³´"] or
                    "ì •í™•í•œ ì •ë³´" in errors_text
                )
                
                # ìƒí˜¸ëª¨ìˆœì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
                is_accurate = is_accurate_by_default and not has_contradiction
                
                # adopted_infoì™€ rejected_info ì¶”ì¶œ
                adopted_info = verification.get("adopted_info", [])
                rejected_info = verification.get("rejected_info", [])
                
                # adopted_infoê°€ ë¹„ì–´ìˆê³  rejected_infoë„ ë¹„ì–´ìˆìœ¼ë©´, ì›ë³¸ LLM ì‘ë‹µì—ì„œ ì¶”ì¶œ
                if (not adopted_info or len(adopted_info) == 0) and (not rejected_info or len(rejected_info) == 0):
                    print(f"âš ï¸ {model_name}: adopted_infoì™€ rejected_infoê°€ ëª¨ë‘ ë¹„ì–´ìˆìŒ. ì›ë³¸ ì‘ë‹µì—ì„œ ì¶”ì¶œ ì‹œë„...")
                    if llm_responses and model_name in llm_responses:
                        original_response = llm_responses[model_name]
                        # ì›ë³¸ ì‘ë‹µì´ ìˆìœ¼ë©´ adopted_infoì— í¬í•¨ (ì¼ë‹¨ ì±„íƒ)
                        if original_response and len(original_response.strip()) > 0:
                            # ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ìµœëŒ€ 3ê°œ ë¬¸ì¥)
                            import re
                            sentences = re.split(r'[.!?]\s+', original_response.strip())
                            adopted_info = [s.strip() + '.' for s in sentences[:3] if len(s.strip()) > 10]
                            print(f"âœ… {model_name}: ì›ë³¸ ì‘ë‹µì—ì„œ {len(adopted_info)}ê°œ ë¬¸ì¥ ì¶”ì¶œ")
                
                # rejected_infoì—ì„œ "(Wikipedia ...ê³¼ ë¶ˆì¼ì¹˜)" í…ìŠ¤íŠ¸ ì œê±°
                cleaned_rejected_info = []
                for item in rejected_info:
                    # "(Wikipedia ...ê³¼ ë¶ˆì¼ì¹˜)" íŒ¨í„´ ì œê±°
                    import re
                    cleaned_item = re.sub(r'\s*\(Wikipedia[^)]*ë¶ˆì¼ì¹˜[^)]*\)', '', str(item))
                    cleaned_item = re.sub(r'\s*\(Wikipedia.*?\)', '', cleaned_item)  # ê¸°íƒ€ Wikipedia ê´„í˜¸ ì œê±°
                    cleaned_item = cleaned_item.strip()
                    if cleaned_item:
                        cleaned_rejected_info.append(cleaned_item)
                
                # adopted_infoë„ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”
                cleaned_adopted_info = []
                for item in adopted_info:
                    if isinstance(item, str) and item.strip():
                        cleaned_adopted_info.append(item.strip())
                
                print(f"ğŸ“Š {model_name}: adopted_info={len(cleaned_adopted_info)}ê°œ, rejected_info={len(cleaned_rejected_info)}ê°œ")

                result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                    "ì •í™•ì„±": "âœ…" if is_accurate else "âŒ",
                    "ì˜¤ë¥˜": errors_text if not is_accurate else "ì •í™•í•œ ì •ë³´ ì œê³µ",
                    "ì‹ ë¢°ë„": verification.get("confidence", "50"),
                    "ì±„íƒëœ_ì •ë³´": cleaned_adopted_info,
                    "ì œì™¸ëœ_ì •ë³´": cleaned_rejected_info
                }
            
            # Judge ëª¨ë¸ì´ ë°˜í™˜í•˜ì§€ ì•Šì€ ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œë„ ê¸°ë³¸ ê²€ì¦ ê²°ê³¼ ìƒì„±
            if llm_responses:
                print(f"ğŸ“‹ llm_responsesì˜ ëª¨ë“  ëª¨ë¸: {list(llm_responses.keys())}")
                print(f"ğŸ“‹ processed_models: {list(processed_models)}")
                for model_name in llm_responses.keys():
                    if model_name not in processed_models:
                        print(f"âš ï¸ {model_name}: Judge ëª¨ë¸ì´ ê²€ì¦ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ê²€ì¦ ê²°ê³¼ ìƒì„±...")
                        # ì›ë³¸ ì‘ë‹µì—ì„œ ì •ë³´ ì¶”ì¶œ
                        original_response = llm_responses[model_name]
                        adopted_info = []
                        if original_response and len(original_response.strip()) > 0:
                            import re
                            sentences = re.split(r'[.!?]\s+', original_response.strip())
                            adopted_info = [s.strip() + '.' for s in sentences[:3] if len(s.strip()) > 10]
                        
                        result["llm_ê²€ì¦_ê²°ê³¼"][model_name] = {
                            "ì •í™•ì„±": "âœ…",
                            "ì˜¤ë¥˜": "ì •í™•í•œ ì •ë³´ ì œê³µ",
                            "ì‹ ë¢°ë„": "50",
                            "ì±„íƒëœ_ì •ë³´": adopted_info,
                            "ì œì™¸ëœ_ì •ë³´": []
                        }
                        print(f"âœ… {model_name}: ê¸°ë³¸ ê²€ì¦ ê²°ê³¼ ìƒì„± ì™„ë£Œ (adopted_info={len(adopted_info)}ê°œ)")
                    else:
                        print(f"âœ… {model_name}: Judge ëª¨ë¸ì´ ê²€ì¦ ê²°ê³¼ë¥¼ ë°˜í™˜í•¨ (ì´ë¯¸ ì²˜ë¦¬ë¨)")
                
                print(f"ğŸ“Š ìµœì¢… llm_ê²€ì¦_ê²°ê³¼ í‚¤: {list(result['llm_ê²€ì¦_ê²°ê³¼'].keys())}")
                result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
            
            return result
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°±
            return create_fallback_result(judge_model, llm_responses)
            
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return create_fallback_result(judge_model, llm_responses)



def create_fallback_result(judge_model, llm_responses=None):
    """í´ë°± ê²°ê³¼ ìƒì„±"""
    if llm_responses:
        actual_models = list(llm_responses.keys())
    else:
        actual_models = ["GPT-4-Turbo", "GPT-4o", "GPT-3.5-Turbo", "GPT-4o-mini", 
                        "Gemini-Pro-1.5", "Gemini-Pro-1.0",
                        "Claude-3-Opus", "Claude-3-Sonnet", "Claude-3-Haiku",
                        "Clova-HCX-003", "Clova-HCX-DASH-001"]
    
    result = {
        "ìµœì ì˜_ë‹µë³€": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "llm_ê²€ì¦_ê²°ê³¼": {},
        "ì‹¬íŒëª¨ë¸": judge_model,
        "ìƒíƒœ": "íŒŒì‹± ì‹¤íŒ¨",
        "ì‹ ë¢°ë„": "0",
        "ìƒí˜¸ëª¨ìˆœ": [],
        "ì‚¬ì‹¤ê²€ì¦": {}
    }
    
    for model in actual_models:
        # ì›ë³¸ LLM ì‘ë‹µì—ì„œ ì§ì ‘ ì¶”ì¶œ
        adopted_info = []
        if llm_responses and model in llm_responses:
            original_response = llm_responses[model]
            if original_response and len(original_response.strip()) > 0:
                # ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ìµœëŒ€ 3ê°œ ë¬¸ì¥)
                import re
                sentences = re.split(r'[.!?]\s+', original_response.strip())
                adopted_info = [s.strip() + '.' for s in sentences[:3] if len(s.strip()) > 10]
        
        result["llm_ê²€ì¦_ê²°ê³¼"][model] = {
            "ì •í™•ì„±": "âŒ", 
            "ì˜¤ë¥˜": "ê²€ì¦ ì‹¤íŒ¨ - Judge ëª¨ë¸ ì˜¤ë¥˜", 
            "ì‹ ë¢°ë„": "0",
            "ì±„íƒëœ_ì •ë³´": adopted_info,
            "ì œì™¸ëœ_ì •ë³´": []
        }
    
    if llm_responses:
        result["ì›ë³¸_ì‘ë‹µ"] = llm_responses
    
    return result



def format_optimal_response(final_result):
    """ìµœì  ë‹µë³€ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    try:
        print(f"ğŸ” format_optimal_response ì‹œì‘...")
        print(f"ğŸ” final_result íƒ€ì…: {type(final_result)}")
        print(f"ğŸ” final_result í‚¤: {list(final_result.keys()) if isinstance(final_result, dict) else 'N/A'}")
        
        optimal_answer = final_result.get("ìµœì ì˜_ë‹µë³€", "")
        print(f"ğŸ” optimal_answer ê¸¸ì´: {len(optimal_answer) if optimal_answer else 0}ì")
        print(f"ğŸ” optimal_answer ë‚´ìš©: {optimal_answer[:200] if optimal_answer else 'None'}...")
        
        verification_results = final_result.get("llm_ê²€ì¦_ê²°ê³¼", {})
        print(f"ğŸ” verification_results í‚¤ ê°œìˆ˜: {len(verification_results)}ê°œ")
        print(f"ğŸ” verification_results ëª¨ë¸: {list(verification_results.keys())}")
        
        judge_model = final_result.get("ì‹¬íŒëª¨ë¸", "GPT-5")
        status = final_result.get("ìƒíƒœ", "ì„±ê³µ")
        
        # ìƒˆë¡œìš´ JSON í˜•ì‹ ì§€ì›
        confidence = final_result.get("ì‹ ë¢°ë„", "50")
        contradictions = final_result.get("ìƒí˜¸ëª¨ìˆœ", [])
        
        # ë¶„ì„ ê·¼ê±° ì¶”ì¶œ
        analysis_rationale = final_result.get("ë¶„ì„_ê·¼ê±°", "")
        print(f"ğŸ” analysis_rationale ê¸¸ì´: {len(analysis_rationale) if analysis_rationale else 0}ì")

        original_responses = final_result.get("ì›ë³¸_ì‘ë‹µ", {})

        def normalize_spaces(text):
            return re.sub(r'\s+', ' ', text or '').strip()

        def contains_text(container, snippet):
            if not container or not snippet:
                return False
            normalized_container = normalize_spaces(container).lower()
            normalized_snippet = normalize_spaces(snippet).lower()
            return bool(normalized_snippet) and normalized_snippet in normalized_container

        def find_original_text(model_key):
            if not original_responses:
                return ""
            if model_key in original_responses:
                return original_responses[model_key]
            lower_key = model_key.lower()
            for candidate_key, value in original_responses.items():
                if candidate_key.lower() == lower_key:
                    return value
            return ""
        
        # ìµœì  ë‹µë³€ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²´í¬
        if not optimal_answer or len(optimal_answer.strip()) == 0:
            print(f"âš ï¸ ìµœì  ë‹µë³€ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! í´ë°± ë©”ì‹œì§€ ìƒì„±...")
            optimal_answer = "ìµœì  ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê° AI ëª¨ë¸ì˜ ê°œë³„ ì‘ë‹µì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ë©”ì¸ ë‹µë³€ êµ¬ì„± (ì±„íŒ… ì°½ì—ëŠ” ìµœì  ë‹µë³€ ë³¸ë¬¸ë§Œ í‘œì‹œ)
        formatted_response = f"""## ìµœì ì˜ ë‹µë³€

{optimal_answer}
"""
        
        # ë¶„ì„ ê·¼ê±°ì™€ ê° LLM ê²€ì¦ ê²°ê³¼ëŠ” ëª¨ë‹¬ì—ì„œë§Œ í‘œì‹œë˜ë„ë¡ ì œê±°
        # (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ analysisDataë¥¼ í†µí•´ ëª¨ë‹¬ì— í‘œì‹œ)
        # í•˜ì§€ë§Œ verification_results í•„í„°ë§ì€ ëª¨ë‹¬ ë°ì´í„°ë¥¼ ìœ„í•´ ìœ ì§€
        model_names = {
            # GPT ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
            "GPT-5": "GPT-5",
            "GPT-5-Mini": "GPT-5 Mini",
            "GPT-4.1": "GPT-4.1",
            "GPT-4.1-Mini": "GPT-4.1 Mini",
            "GPT-4o": "GPT-4o",
            "GPT-4o-Mini": "GPT-4o Mini",
            "GPT-4-Turbo": "GPT-4 Turbo",
            "GPT-3.5-Turbo": "GPT-3.5 Turbo",
            
            # Gemini ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
            "Gemini-2.5-Pro": "Gemini 2.5 Pro",
            "Gemini-2.5-Flash": "Gemini 2.5 Flash",
            "Gemini-2.0-Flash-Exp": "Gemini 2.0 Flash Exp",
            "Gemini-2.0-Flash-Lite": "Gemini 2.0 Flash Lite",
            
            # Claude ëª¨ë¸ë“¤ (ìµœì‹  ì¶”ê°€)
            "Claude-4-Opus": "Claude 4 Opus",
            "Claude-3.7-Sonnet": "Claude 3.7 Sonnet",
            "Claude-3.5-Sonnet": "Claude 3.5 Sonnet",
            "Claude-3.5-Haiku": "Claude 3.5 Haiku",
            "Claude-3-Opus": "Claude 3 Opus",
            
            # HyperCLOVA X ëª¨ë¸ë“¤
            "HCX-003": "HyperCLOVA X HCX-003",
            "HCX-DASH-001": "HyperCLOVA X HCX-DASH-001",
        }
        
        # verification_results í•„í„°ë§ (ëª¨ë‹¬ ë°ì´í„°ë¥¼ ìœ„í•´ ìœ ì§€, ë§ˆí¬ë‹¤ìš´ì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
        # í•„í„°ë§ ë¡œì§ ì™„í™”: adopted_infoê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        print(f"ğŸ” format_optimal_response - verification_results í‚¤: {list(verification_results.keys())}")
        print(f"ğŸ” format_optimal_response - model_names í‚¤: {list(model_names.keys())}")
        
        # verification_resultsì˜ ëª¨ë“  í‚¤ë¥¼ ìˆœíšŒ (model_namesì— ì—†ëŠ” ëª¨ë¸ë„ í¬í•¨)
        for model_key in verification_results.keys():
            verification = verification_results[model_key]
            adopted = verification.get("ì±„íƒëœ_ì •ë³´", []) or []
            rejected = verification.get("ì œì™¸ëœ_ì •ë³´", []) or []

            original_text = find_original_text(model_key)
            print(f"ğŸ” format_optimal_response - ì²˜ë¦¬ ì¤‘ì¸ ëª¨ë¸: {model_key}, original_text ê¸¸ì´: {len(original_text) if original_text else 0}")

            # adopted_info í•„í„°ë§ (ì™„í™”ëœ ì¡°ê±´)
            if optimal_answer:
                # ì›ë³¸ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆê³ , ìµœì  ë‹µë³€ê³¼ ê´€ë ¨ì´ ìˆëŠ” ì •ë³´ë§Œ í•„í„°ë§
                adopted_filtered = [
                    item.strip() for item in adopted
                    if isinstance(item, str) and item.strip()
                    and contains_text(original_text, item)
                ]
                # í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ adopted_info ì‚¬ìš©
                if not adopted_filtered and adopted:
                    adopted_filtered = [item.strip() for item in adopted if isinstance(item, str) and item.strip()]
            else:
                adopted_filtered = [
                    item.strip() for item in adopted
                    if isinstance(item, str) and item.strip() and contains_text(original_text, item)
                ]
                # í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ adopted_info ì‚¬ìš©
                if not adopted_filtered and adopted:
                    adopted_filtered = [item.strip() for item in adopted if isinstance(item, str) and item.strip()]

            # rejected_info í•„í„°ë§ (ì›ë³¸ ë‹µë³€ì— í¬í•¨ëœ ì •ë³´ë§Œ)
            rejected_filtered = [
                item.strip() for item in rejected
                if isinstance(item, str) and item.strip()
                and contains_text(original_text, item)
            ]

            # verification_results ì—…ë°ì´íŠ¸ (ëª¨ë‹¬ì—ì„œ ì‚¬ìš©)
            verification["ì±„íƒëœ_ì •ë³´"] = adopted_filtered if adopted_filtered else adopted
            verification["ì œì™¸ëœ_ì •ë³´"] = rejected_filtered
        
        # ìƒí˜¸ëª¨ìˆœ ì •ë³´ë„ ì±„íŒ… ì°½ì—ì„œëŠ” ì œì™¸ (í•„ìš”ì‹œ ëª¨ë‹¬ì—ì„œ í‘œì‹œ ê°€ëŠ¥)
        
        return formatted_response
        
    except Exception as e:
        print(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
        return f"""**ìµœì ì˜ ë‹µë³€:**

{final_result.get('ìµœì ì˜_ë‹µë³€', 'ë‹µë³€ ìƒì„± ì‹¤íŒ¨')}

*í¬ë§·íŒ… ì˜¤ë¥˜ ë°œìƒ*
"""

