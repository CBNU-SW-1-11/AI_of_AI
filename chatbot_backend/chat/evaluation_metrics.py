"""
AI í†µí•© ë‹µë³€ ìµœì í™” í”Œë«í¼ìš© ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
í”„ë¡œì íŠ¸ ëª©í‘œ: AI í†µí•© ê¸°ë°˜ ë‹µë³€ ìµœì í™” í”Œë«í¼
"""

import re
import json
import numpy as np
from typing import Dict, List, Any, Tuple
from collections import Counter
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class EvaluationMetrics:
    """AI í†µí•© ë‹µë³€ ìµœì í™” í”Œë«í¼ìš© í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics_cache = {}
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        print("ğŸ” AI í†µí•© ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def evaluate_ensemble_quality(self, ai_responses: Dict[str, str], ensemble_answer: str, query: str) -> Dict[str, Any]:
        """ì•™ìƒë¸” í•™ìŠµ ê²°ê³¼ì˜ í’ˆì§ˆ í‰ê°€"""
        try:
            print(f"ğŸ” ì•™ìƒë¸” í’ˆì§ˆ í‰ê°€ ì‹œì‘: {len(ai_responses)}ê°œ ì‘ë‹µ")
            
            results = {}
            
            # 1. ê°œë³„ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€
            individual_scores = {}
            for ai_name, response in ai_responses.items():
                individual_scores[ai_name] = self._evaluate_individual_response(response, query)
            
            # 2. ì•™ìƒë¸” ë‹µë³€ í’ˆì§ˆ í‰ê°€
            ensemble_score = self._evaluate_ensemble_response(ensemble_answer, query)
            
            # 3. ì•™ìƒë¸” íš¨ê³¼ì„± í‰ê°€
            ensemble_effectiveness = self._evaluate_ensemble_effectiveness(ai_responses, ensemble_answer)
            
            # 4. ì‹ ë¢°ë„ ë° ì¼ê´€ì„± í‰ê°€
            reliability_metrics = self._evaluate_reliability_and_consistency(ai_responses, ensemble_answer)
            
            # 5. ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡
            user_satisfaction_score = self._predict_user_satisfaction(ensemble_answer, query)
            
            results = {
                'individual_scores': individual_scores,
                'ensemble_score': ensemble_score,
                'ensemble_effectiveness': ensemble_effectiveness,
                'reliability_metrics': reliability_metrics,
                'user_satisfaction_score': user_satisfaction_score,
                'overall_quality': self._calculate_overall_ensemble_quality(
                    individual_scores, ensemble_score, ensemble_effectiveness, reliability_metrics, user_satisfaction_score
                )
            }
            
            print(f"âœ… ì•™ìƒë¸” í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: ì „ì²´ í’ˆì§ˆ {results['overall_quality']:.2f}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì•™ìƒë¸” í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return self._create_fallback_evaluation(ai_responses, ensemble_answer)
    
    def _evaluate_individual_response(self, response: str, query: str) -> Dict[str, float]:
        """ê°œë³„ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            basic_metrics = self._calculate_basic_metrics(response)
            
            # ì§ˆë¬¸ ê´€ë ¨ì„±
            relevance_score = self._calculate_relevance_score(response, query)
            
            # ì‘ë‹µ ì™„ì„±ë„
            completeness_score = self._calculate_completeness_score(response, query)
            
            # ëª…í™•ì„± ë° ê°€ë…ì„±
            clarity_score = self._calculate_clarity_score(response)
            
            # ì‚¬ì‹¤ ì •í™•ì„± (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            factual_accuracy = self._calculate_factual_accuracy(response)
            
            # ìœ ìš©ì„±
            usefulness_score = self._calculate_usefulness_score(response, query)
            
            return {
                'basic_metrics': basic_metrics,
                'relevance_score': relevance_score,
                'completeness_score': completeness_score,
                'clarity_score': clarity_score,
                'factual_accuracy': factual_accuracy,
                'usefulness_score': usefulness_score,
                'overall_score': np.mean([
                    relevance_score, completeness_score, clarity_score, 
                    factual_accuracy, usefulness_score
                ])
            }
            
        except Exception as e:
            logger.warning(f"ê°œë³„ ì‘ë‹µ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'basic_metrics': {'word_count': len(response.split()), 'sentence_count': len(re.split(r'[.!?]+', response))},
                'relevance_score': 0.5,
                'completeness_score': 0.5,
                'clarity_score': 0.5,
                'factual_accuracy': 0.5,
                'usefulness_score': 0.5,
                'overall_score': 0.5
            }
    
    def _evaluate_ensemble_response(self, ensemble_answer: str, query: str) -> Dict[str, float]:
        """ì•™ìƒë¸” ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        try:
            # êµ¬ì¡°ì  ì™„ì„±ë„ (ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„)
            structure_score = self._evaluate_ensemble_structure(ensemble_answer)
            
            # ì •ë³´ ë°€ë„
            information_density = self._calculate_information_density(ensemble_answer)
            
            # ì•™ìƒë¸” íŠ¹ì„± í‰ê°€
            ensemble_characteristics = self._evaluate_ensemble_characteristics(ensemble_answer)
            
            # ì¢…í•© í’ˆì§ˆ
            overall_quality = np.mean([
                structure_score, information_density, ensemble_characteristics
            ])
            
            return {
                'structure_score': structure_score,
                'information_density': information_density,
                'ensemble_characteristics': ensemble_characteristics,
                'overall_quality': overall_quality
            }
            
        except Exception as e:
            logger.warning(f"ì•™ìƒë¸” ì‘ë‹µ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'structure_score': 0.5,
                'information_density': 0.5,
                'ensemble_characteristics': 0.5,
                'overall_quality': 0.5
            }
    
    def _evaluate_ensemble_effectiveness(self, ai_responses: Dict[str, str], ensemble_answer: str) -> Dict[str, float]:
        """ì•™ìƒë¸” íš¨ê³¼ì„± í‰ê°€"""
        try:
            # ê°œë³„ ì‘ë‹µê³¼ ì•™ìƒë¸” ë‹µë³€ì˜ ìœ ì‚¬ë„ ë¶„ì„
            similarities = []
            for ai_name, response in ai_responses.items():
                similarity = self._calculate_text_similarity(response, ensemble_answer)
                similarities.append(similarity)
            
            avg_similarity = np.mean(similarities)
            
            # ì•™ìƒë¸”ì´ ê°œë³„ ì‘ë‹µë³´ë‹¤ ì–¼ë§ˆë‚˜ ê°œì„ ë˜ì—ˆëŠ”ì§€ í‰ê°€
            improvement_score = self._calculate_improvement_score(ai_responses, ensemble_answer)
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ (ê°œë³„ ì‘ë‹µë“¤ ê°„ì˜ ì°¨ì´)
            diversity_score = self._calculate_diversity_score(list(ai_responses.values()))
            
            return {
                'avg_similarity': avg_similarity,
                'improvement_score': improvement_score,
                'diversity_score': diversity_score,
                'effectiveness_score': np.mean([improvement_score, diversity_score])
            }
            
        except Exception as e:
            logger.warning(f"ì•™ìƒë¸” íš¨ê³¼ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'avg_similarity': 0.5,
                'improvement_score': 0.5,
                'diversity_score': 0.5,
                'effectiveness_score': 0.5
            }
    
    def _evaluate_reliability_and_consistency(self, ai_responses: Dict[str, str], ensemble_answer: str) -> Dict[str, float]:
        """ì‹ ë¢°ë„ ë° ì¼ê´€ì„± í‰ê°€"""
        try:
            # ì‘ë‹µë“¤ ê°„ì˜ ì¼ê´€ì„±
            consistency_score = self._calculate_consistency_score(list(ai_responses.values()))
            
            # ì‹ ë¢°ë„ ì§€í‘œ (êµ¬ì²´ì  ì •ë³´ í¬í•¨ë„)
            reliability_score = self._calculate_reliability_score(ensemble_answer)
            
            # ë¶ˆí™•ì‹¤ì„± í‘œì‹œ
            uncertainty_score = self._calculate_uncertainty_score(ensemble_answer)
            
            return {
                'consistency_score': consistency_score,
                'reliability_score': reliability_score,
                'uncertainty_score': uncertainty_score,
                'overall_reliability': np.mean([consistency_score, reliability_score, 1 - uncertainty_score])
            }
            
        except Exception as e:
            logger.warning(f"ì‹ ë¢°ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'consistency_score': 0.5,
                'reliability_score': 0.5,
                'uncertainty_score': 0.5,
                'overall_reliability': 0.5
            }
    
    def _predict_user_satisfaction(self, ensemble_answer: str, query: str) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡"""
        try:
            # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„±
            length_score = self._calculate_length_appropriateness(ensemble_answer, query)
            
            # ì§ˆë¬¸ í•´ê²°ë„
            resolution_score = self._calculate_query_resolution_score(ensemble_answer, query)
            
            # ì‚¬ìš©ì ì¹œí™”ì„±
            user_friendliness = self._calculate_user_friendliness(ensemble_answer)
            
            # ì•¡ì…˜ ê°€ëŠ¥ì„± (ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ ì œê³µ)
            actionability = self._calculate_actionability(ensemble_answer)
            
            return np.mean([length_score, resolution_score, user_friendliness, actionability])
            
        except Exception as e:
            logger.warning(f"ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_basic_metrics(self, text: str) -> Dict[str, int]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            words = text.split()
            sentences = re.split(r'[.!?]+', text)
            
            return {
                'word_count': len(words),
                'sentence_count': len([s for s in sentences if s.strip()]),
                'character_count': len(text),
                'avg_words_per_sentence': len(words) / max(len(sentences), 1)
            }
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'word_count': 0, 'sentence_count': 0, 'character_count': 0, 'avg_words_per_sentence': 0}
    
    def _calculate_relevance_score(self, response: str, query: str) -> float:
        """ì§ˆë¬¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            query_words = set(query.lower().split())
            response_words = set(response.lower().split())
            
            # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨
            common_words = query_words.intersection(response_words)
            relevance = len(common_words) / max(len(query_words), 1)
            
            return min(relevance, 1.0)
        except Exception as e:
            logger.warning(f"ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_completeness_score(self, response: str, query: str) -> float:
        """ì‘ë‹µ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ì‘ë‹µì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            query_keywords = re.findall(r'\b\w+\b', query.lower())
            response_lower = response.lower()
            
            covered_keywords = sum(1 for keyword in query_keywords if keyword in response_lower)
            completeness = covered_keywords / max(len(query_keywords), 1)
            
            # ì‘ë‹µ ê¸¸ì´ë„ ê³ ë ¤
            length_factor = min(len(response.split()) / max(len(query.split()) * 3, 50), 1.0)
            
            return np.mean([completeness, length_factor])
        except Exception as e:
            logger.warning(f"ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_clarity_score(self, response: str) -> float:
        """ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            sentences = re.split(r'[.!?]+', response)
            if not sentences:
                return 0.0
            
            # í‰ê·  ë¬¸ì¥ ê¸¸ì´ (15-20ë‹¨ì–´ê°€ ì´ìƒì )
            avg_length = sum(len(s.split()) for s in sentences) / len(sentences)
            length_score = max(0, 1 - abs(avg_length - 17.5) / 17.5)
            
            # ë¬¸ì¥ êµ¬ì¡° ë‹¤ì–‘ì„±
            structure_variety = len(set(len(s.split()) for s in sentences)) / max(len(sentences), 1)
            
            return np.mean([length_score, structure_variety])
        except Exception as e:
            logger.warning(f"ëª…í™•ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_factual_accuracy(self, response: str) -> float:
        """ì‚¬ì‹¤ ì •í™•ì„± ì ìˆ˜ ê³„ì‚° (íœ´ë¦¬ìŠ¤í‹±)"""
        try:
            # êµ¬ì²´ì  ì •ë³´ í¬í•¨ë„
            numbers = len(re.findall(r'\d+', response))
            dates = len(re.findall(r'\d{4}', response))
            specific_terms = len(re.findall(r'\b[A-Z][a-z]+\b', response))  # ê³ ìœ ëª…ì‚¬
            
            factual_elements = numbers + dates + specific_terms
            return min(factual_elements / 10, 1.0)  # ìµœëŒ€ 10ê°œ ìš”ì†Œ
        except Exception as e:
            logger.warning(f"ì‚¬ì‹¤ ì •í™•ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_usefulness_score(self, response: str, query: str) -> float:
        """ìœ ìš©ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì•¡ì…˜ ê°€ëŠ¥í•œ ì •ë³´ í¬í•¨ë„
            action_words = ['ë°©ë²•', 'ë‹¨ê³„', 'ê³¼ì •', 'ì ˆì°¨', 'ê°€ì´ë“œ', 'íŒ', 'ì¡°ì–¸']
            action_count = sum(1 for word in action_words if word in response)
            
            # êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨ë„
            example_indicators = ['ì˜ˆë¥¼ ë“¤ì–´', 'ì˜ˆì‹œ', 'ì˜ˆ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì‹¤ì œë¡œ']
            example_count = sum(1 for indicator in example_indicators if indicator in response)
            
            usefulness = (action_count + example_count) / 5  # ìµœëŒ€ 5ê°œ
            return min(usefulness, 1.0)
        except Exception as e:
            logger.warning(f"ìœ ìš©ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_ensemble_structure(self, ensemble_answer: str) -> float:
        """ì•™ìƒë¸” ë‹µë³€ êµ¬ì¡° í‰ê°€"""
        try:
            # ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„
            sections = re.findall(r'## .+', ensemble_answer)
            subsections = re.findall(r'### .+', ensemble_answer)
            
            # êµ¬ì¡°ì  ì™„ì„±ë„
            structure_score = 0.0
            if 'ğŸ¯ í†µí•© ë‹µë³€' in ensemble_answer:
                structure_score += 0.3
            if 'ğŸ“Š ê° AI ë¶„ì„' in ensemble_answer:
                structure_score += 0.3
            if 'ğŸ” ë¶„ì„ ê·¼ê±°' in ensemble_answer:
                structure_score += 0.2
            if 'ğŸ† ìµœì¢… ì¶”ì²œ' in ensemble_answer:
                structure_score += 0.2
            
            return structure_score
        except Exception as e:
            logger.warning(f"ì•™ìƒë¸” êµ¬ì¡° í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_information_density(self, text: str) -> float:
        """ì •ë³´ ë°€ë„ ê³„ì‚°"""
        try:
            # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨
            words = text.split()
            meaningful_words = [w for w in words if len(w) > 3 and not w.isdigit()]
            density = len(meaningful_words) / max(len(words), 1)
            
            return min(density, 1.0)
        except Exception as e:
            logger.warning(f"ì •ë³´ ë°€ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _evaluate_ensemble_characteristics(self, ensemble_answer: str) -> float:
        """ì•™ìƒë¸” íŠ¹ì„± í‰ê°€"""
        try:
            # ì•™ìƒë¸” íŠ¹ì„± í‚¤ì›Œë“œ í¬í•¨ë„
            ensemble_keywords = ['í†µí•©', 'ì•™ìƒë¸”', 'ì¢…í•©', 'ë¶„ì„', 'ë¹„êµ', 'ì‹ ë¢°ë„', 'ê°€ì¤‘ì¹˜']
            keyword_count = sum(1 for keyword in ensemble_keywords if keyword in ensemble_answer)
            
            return min(keyword_count / len(ensemble_keywords), 1.0)
        except Exception as e:
            logger.warning(f"ì•™ìƒë¸” íŠ¹ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not text1 or not text2:
                return 0.0
            
            # TF-IDF ë²¡í„°í™”
            tfidf_matrix = self.vectorizer.fit_transform([text1, text2])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            
            return similarity
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_improvement_score(self, ai_responses: Dict[str, str], ensemble_answer: str) -> float:
        """ê°œì„  ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°œë³„ ì‘ë‹µë“¤ì˜ í‰ê·  í’ˆì§ˆ
            individual_qualities = []
            for response in ai_responses.values():
                quality = self._calculate_basic_quality_score(response)
                individual_qualities.append(quality)
            
            avg_individual_quality = np.mean(individual_qualities)
            ensemble_quality = self._calculate_basic_quality_score(ensemble_answer)
            
            # ê°œì„  ì •ë„
            improvement = max(0, ensemble_quality - avg_individual_quality)
            return min(improvement, 1.0)
        except Exception as e:
            logger.warning(f"ê°œì„  ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_diversity_score(self, responses: List[str]) -> float:
        """ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(responses) < 2:
                return 0.0
            
            # ì‘ë‹µë“¤ ê°„ì˜ í‰ê·  ìœ ì‚¬ë„
            similarities = []
            for i in range(len(responses)):
                for j in range(i + 1, len(responses)):
                    similarity = self._calculate_text_similarity(responses[i], responses[j])
                    similarities.append(similarity)
            
            avg_similarity = np.mean(similarities)
            diversity = 1 - avg_similarity  # ìœ ì‚¬ë„ê°€ ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ë†’ìŒ
            
            return max(diversity, 0.0)
        except Exception as e:
            logger.warning(f"ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_consistency_score(self, responses: List[str]) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            if len(responses) < 2:
                return 1.0
            
            # ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨
            all_words = set()
            for response in responses:
                words = set(response.lower().split())
                all_words.update(words)
            
            common_words = set(responses[0].lower().split())
            for response in responses[1:]:
                common_words = common_words.intersection(set(response.lower().split()))
            
            consistency = len(common_words) / max(len(all_words), 1)
            return min(consistency, 1.0)
        except Exception as e:
            logger.warning(f"ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_reliability_score(self, text: str) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # êµ¬ì²´ì  ì •ë³´ í¬í•¨ë„
            numbers = len(re.findall(r'\d+', text))
            dates = len(re.findall(r'\d{4}', text))
            citations = len(re.findall(r'ì¶œì²˜|ì°¸ê³ |ë§í¬|ì°¸ì¡°', text))
            
            reliability_indicators = numbers + dates + citations
            return min(reliability_indicators / 5, 1.0)
        except Exception as e:
            logger.warning(f"ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_uncertainty_score(self, text: str) -> float:
        """ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë¶ˆí™•ì‹¤ì„± í‘œí˜„ í¬í•¨ë„
            uncertainty_words = ['ì•„ë§ˆë„', 'ì¶”ì •', 'ê°€ëŠ¥ì„±', 'ë¶ˆí™•ì‹¤', 'ëª¨í˜¸', 'ì •í™•í•˜ì§€ ì•ŠìŒ']
            uncertainty_count = sum(1 for word in uncertainty_words if word in text)
            
            return min(uncertainty_count / 3, 1.0)  # ìµœëŒ€ 3ê°œ
        except Exception as e:
            logger.warning(f"ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_length_appropriateness(self, response: str, query: str) -> float:
        """ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± ê³„ì‚°"""
        try:
            query_length = len(query.split())
            response_length = len(response.split())
            
            # ì´ìƒì ì¸ ì‘ë‹µ ê¸¸ì´ (ì§ˆë¬¸ ê¸¸ì´ì˜ 3-10ë°°)
            ideal_min = query_length * 3
            ideal_max = query_length * 10
            
            if ideal_min <= response_length <= ideal_max:
                return 1.0
            elif response_length < ideal_min:
                return response_length / ideal_min
            else:
                return ideal_max / response_length
        except Exception as e:
            logger.warning(f"ê¸¸ì´ ì ì ˆì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_query_resolution_score(self, response: str, query: str) -> float:
        """ì§ˆë¬¸ í•´ê²°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ì‘ë‹µì—ì„œ í•´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
            query_words = set(query.lower().split())
            response_words = set(response.lower().split())
            
            resolved_words = query_words.intersection(response_words)
            resolution_score = len(resolved_words) / max(len(query_words), 1)
            
            return min(resolution_score, 1.0)
        except Exception as e:
            logger.warning(f"ì§ˆë¬¸ í•´ê²°ë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_user_friendliness(self, text: str) -> float:
        """ì‚¬ìš©ì ì¹œí™”ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì¹œí™”ì  í‘œí˜„ í¬í•¨ë„
            friendly_words = ['ë„ì›€', 'ë„ì™€ë“œë¦¬', 'ì•ˆë‚´', 'ì„¤ëª…', 'ì´í•´', 'ì‰½ê²Œ', 'ê°„ë‹¨íˆ']
            friendly_count = sum(1 for word in friendly_words if word in text)
            
            # êµ¬ì¡°í™”ëœ ì •ë³´ ì œê³µ
            structured_indicators = ['1.', '2.', '3.', 'â€¢', '-', 'ë‹¨ê³„', 'ê³¼ì •']
            structured_count = sum(1 for indicator in structured_indicators if indicator in text)
            
            friendliness = (friendly_count + structured_count) / 10
            return min(friendliness, 1.0)
        except Exception as e:
            logger.warning(f"ì‚¬ìš©ì ì¹œí™”ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_actionability(self, text: str) -> float:
        """ì•¡ì…˜ ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ í¬í•¨ë„
            action_words = ['ë°©ë²•', 'ë‹¨ê³„', 'ê³¼ì •', 'ì ˆì°¨', 'ê°€ì´ë“œ', 'íŒ', 'ì¡°ì–¸', 'ì‹¤í–‰', 'ì ìš©']
            action_count = sum(1 for word in action_words if word in text)
            
            # êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨ë„
            example_indicators = ['ì˜ˆë¥¼ ë“¤ì–´', 'ì˜ˆì‹œ', 'ì˜ˆ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì‹¤ì œë¡œ', 'ê²½ìš°']
            example_count = sum(1 for indicator in example_indicators if indicator in text)
            
            actionability = (action_count + example_count) / 8
            return min(actionability, 1.0)
        except Exception as e:
            logger.warning(f"ì•¡ì…˜ ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_basic_quality_score(self, text: str) -> float:
        """ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
            length_score = min(len(text.split()) / 100, 1.0)
            
            # êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜
            structure_score = len(re.findall(r'[.!?]', text)) / max(len(text.split()), 1)
            
            # ë‹¤ì–‘ì„± ê¸°ë°˜ ì ìˆ˜
            words = text.split()
            unique_words = len(set(words))
            diversity_score = unique_words / max(len(words), 1)
            
            return np.mean([length_score, structure_score, diversity_score])
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_overall_ensemble_quality(self, individual_scores: Dict[str, Dict], ensemble_score: Dict, 
                                          ensemble_effectiveness: Dict, reliability_metrics: Dict, 
                                          user_satisfaction_score: float) -> float:
        """ì „ì²´ ì•™ìƒë¸” í’ˆì§ˆ ê³„ì‚°"""
        try:
            # ê°œë³„ ì ìˆ˜ë“¤ì˜ í‰ê· 
            avg_individual_score = np.mean([score['overall_score'] for score in individual_scores.values()])
            
            # ì•™ìƒë¸” ì ìˆ˜
            ensemble_quality = ensemble_score['overall_quality']
            
            # íš¨ê³¼ì„± ì ìˆ˜
            effectiveness_score = ensemble_effectiveness['effectiveness_score']
            
            # ì‹ ë¢°ë„ ì ìˆ˜
            reliability_score = reliability_metrics['overall_reliability']
            
            # ì‚¬ìš©ì ë§Œì¡±ë„
            satisfaction_score = user_satisfaction_score
            
            # ê°€ì¤‘ í‰ê·  (ì•™ìƒë¸” í’ˆì§ˆì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            overall_quality = np.average([
                avg_individual_score,
                ensemble_quality,
                effectiveness_score,
                reliability_score,
                satisfaction_score
            ], weights=[0.2, 0.3, 0.2, 0.15, 0.15])
            
            return overall_quality
        except Exception as e:
            logger.warning(f"ì „ì²´ ì•™ìƒë¸” í’ˆì§ˆ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _create_fallback_evaluation(self, ai_responses: Dict[str, str], ensemble_answer: str) -> Dict[str, Any]:
        """í´ë°± í‰ê°€ ê²°ê³¼ ìƒì„±"""
        try:
            individual_scores = {}
            for ai_name in ai_responses.keys():
                individual_scores[ai_name] = {
                    'overall_score': 0.5,
                    'relevance_score': 0.5,
                    'completeness_score': 0.5,
                    'clarity_score': 0.5,
                    'factual_accuracy': 0.5,
                    'usefulness_score': 0.5
                }
            
            return {
                'individual_scores': individual_scores,
                'ensemble_score': {'overall_quality': 0.5},
                'ensemble_effectiveness': {'effectiveness_score': 0.5},
                'reliability_metrics': {'overall_reliability': 0.5},
                'user_satisfaction_score': 0.5,
                'overall_quality': 0.5
            }
        except Exception as e:
            logger.error(f"í´ë°± í‰ê°€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'overall_quality': 0.0}
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
    def evaluate_summary_quality(self, summaries: Dict[str, str], reference: str = None) -> Dict[str, Any]:
        """ê¸°ì¡´ ìš”ì•½ í’ˆì§ˆ í‰ê°€ ë©”ì„œë“œ (í˜¸í™˜ì„± ìœ ì§€)"""
        return self.evaluate_ensemble_quality(summaries, reference or "", "")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
evaluation_metrics = EvaluationMetrics()