"""
LangChain RAGë¥¼ í™œìš©í•œ ì‹ ë¢°ë„ ê²€ì¦ ì‹œìŠ¤í…œ
- ë‚´ë¶€ ìë£Œì™€ AI ì‘ë‹µ ë¹„êµ ê²€ì¦
- ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ì§€ì‹ ê²€ìƒ‰
- ì‘ë‹µì˜ ì‚¬ì‹¤ì„± ë° ì¼ê´€ì„± ê²€ì¦
- ìë™ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
"""

import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib
import re

# LangChain ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langchain.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
    from langchain.vectorstores import FAISS, Chroma
    from langchain.chains import RetrievalQA
    from langchain.llms import OpenAI
    from langchain.schema import Document
    from langchain.embeddings.base import Embeddings
    LANGCHAIN_AVAILABLE = True
    print("âœ… LangChain RAG ì‹œìŠ¤í…œ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ LangChain ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ê²€ì¦ ê¸°ëŠ¥ë§Œ ì‚¬ìš©")

# HuggingFace ì„í¬íŠ¸ (ì„ íƒì )
try:
    import torch
    HF_AVAILABLE = True
    print("âœ… HuggingFace ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸ HuggingFace ì‚¬ìš© ë¶ˆê°€")

logger = logging.getLogger(__name__)

@dataclass
class VerificationResult:
    """ê²€ì¦ ê²°ê³¼"""
    overall_score: float
    factual_accuracy: float
    consistency_score: float
    source_verification: float
    contradictions: List[str]
    verified_facts: List[str]
    missing_information: List[str]
    confidence_level: str

@dataclass
class KnowledgeSource:
    """ì§€ì‹ ì†ŒìŠ¤"""
    source_id: str
    source_type: str  # 'document', 'database', 'api', 'knowledge_base'
    content: str
    metadata: Dict[str, Any]
    embedding_vector: Optional[List[float]] = None
    last_updated: str = None

class RAGVerificationSystem:
    """LangChain RAGë¥¼ í™œìš©í•œ ì‹ ë¢°ë„ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.knowledge_base = []
        self.vector_store = None
        self.embeddings = None
        self.retrieval_qa = None
        
        # ê²€ì¦ ì„¤ì •
        self.verification_config = {
            'similarity_threshold': 0.7,
            'factual_accuracy_weight': 0.4,
            'consistency_weight': 0.3,
            'source_verification_weight': 0.3,
            'max_retrieved_docs': 5
        }
        
        # ì§€ì‹ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
        self.knowledge_sources_dir = os.path.join(os.path.dirname(__file__), 'knowledge_sources')
        os.makedirs(self.knowledge_sources_dir, exist_ok=True)
        
        if LANGCHAIN_AVAILABLE:
            self._initialize_rag_system()
        
        print("ğŸ” RAG ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_rag_system(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            if os.getenv('OPENAI_API_KEY'):
                self.embeddings = OpenAIEmbeddings()
                print("âœ… OpenAI ì„ë² ë”© ì‚¬ìš©")
            elif HF_AVAILABLE:
                # ë¬´ë£Œ HuggingFace ì„ë² ë”© ì‚¬ìš©
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                )
                print("âœ… HuggingFace ì„ë² ë”© ì‚¬ìš©")
            else:
                print("âš ï¸ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
            self._load_or_create_vector_store()
            
            # ê²€ì¦ ì²´ì¸ ì´ˆê¸°í™”
            self._initialize_verification_chain()
            
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_or_create_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            vector_store_path = os.path.join(self.knowledge_sources_dir, 'vector_store')
            
            if os.path.exists(vector_store_path):
                # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
                self.vector_store = FAISS.load_local(vector_store_path, self.embeddings)
                print("âœ… ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
            else:
                # ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                self._create_initial_knowledge_base()
                
        except Exception as e:
            logger.warning(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._create_initial_knowledge_base()
    
    def _create_initial_knowledge_base(self):
        """ì´ˆê¸° ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±"""
        try:
            # ê¸°ë³¸ ì§€ì‹ ë¬¸ì„œë“¤ ë¡œë“œ
            documents = []
            
            # í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì„œë“¤ ë¡œë“œ
            documents.extend(self._load_project_documents())
            
            # ì¼ë°˜ì ì¸ ì§€ì‹ ë¬¸ì„œë“¤ ë¡œë“œ
            documents.extend(self._load_general_knowledge())
            
            if documents:
                # í…ìŠ¤íŠ¸ ë¶„í• 
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    separators=["\n\n", "\n", " ", ""]
                )
                
                split_documents = text_splitter.split_documents(documents)
                
                # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                if split_documents:
                    self.vector_store = FAISS.from_documents(split_documents, self.embeddings)
                    
                    # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
                    vector_store_path = os.path.join(self.knowledge_sources_dir, 'vector_store')
                    self.vector_store.save_local(vector_store_path)
                    
                    print(f"âœ… ì´ˆê¸° ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ: {len(split_documents)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            logger.error(f"ì´ˆê¸° ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _load_project_documents(self) -> List[Document]:
        """í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        
        try:
            # README íŒŒì¼ë“¤ ë¡œë“œ
            readme_files = [
                '/Users/seon/AIOFAI_F/AI_of_AI/README.md',
                '/Users/seon/AIOFAI_F/AI_of_AI/LLM_VIDEO_SEARCH_IMPLEMENTATION_COMPLETE.md',
                '/Users/seon/AIOFAI_F/AI_of_AI/enhanced_video_search_proposal.md'
            ]
            
            for readme_path in readme_files:
                if os.path.exists(readme_path):
                    loader = TextLoader(readme_path, encoding='utf-8')
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata['source_type'] = 'project_document'
                        doc.metadata['file_path'] = readme_path
                    documents.extend(docs)
            
            # ì„¤ì • íŒŒì¼ë“¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
            settings_files = [
                '/Users/seon/AIOFAI_F/AI_of_AI/chatbot_backend/chatbot_backend/settings.py',
                '/Users/seon/AIOFAI_F/AI_of_AI/frontend/package.json'
            ]
            
            for settings_path in settings_files:
                if os.path.exists(settings_path):
                    loader = TextLoader(settings_path, encoding='utf-8')
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata['source_type'] = 'configuration'
                        doc.metadata['file_path'] = settings_path
                    documents.extend(docs)
            
        except Exception as e:
            logger.warning(f"í”„ë¡œì íŠ¸ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return documents
    
    def _load_general_knowledge(self) -> List[Document]:
        """ì¼ë°˜ì ì¸ ì§€ì‹ ë¬¸ì„œ ë¡œë“œ"""
        documents = []
        
        try:
            # ì¼ë°˜ì ì¸ ê¸°ìˆ  ì§€ì‹ ì¶”ê°€
            general_knowledge = [
                {
                    'content': '''
                    AI ëª¨ë¸ ë¹„êµ ì •ë³´:
                    - GPT-4: OpenAIì˜ ìµœì‹  ëª¨ë¸, ì°½ì˜ì  ê¸€ì“°ê¸°ì™€ ë³µì¡í•œ ì¶”ë¡ ì— ë›°ì–´ë‚¨
                    - Claude-3.5-Sonnet: Anthropicì˜ ëª¨ë¸, ë¶„ì„ì  ì‚¬ê³ ì™€ ì‚¬ì‹¤ ê²€ì¦ì— ê°•í•¨
                    - Mixtral-8x7B: Mistralì˜ ëª¨ë¸, ë¹ ë¥¸ ì‘ë‹µê³¼ íš¨ìœ¨ì„±ì´ íŠ¹ì§•
                    ''',
                    'metadata': {'source_type': 'ai_knowledge', 'topic': 'ai_models'}
                },
                {
                    'content': '''
                    ë¹„ë””ì˜¤ ë¶„ì„ ê¸°ìˆ :
                    - YOLO: ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
                    - OpenCV: ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬
                    - TensorFlow/PyTorch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
                    - FFmpeg: ë¹„ë””ì˜¤ ì²˜ë¦¬ ë„êµ¬
                    ''',
                    'metadata': {'source_type': 'tech_knowledge', 'topic': 'video_analysis'}
                },
                {
                    'content': '''
                    ì›¹ ê°œë°œ ê¸°ìˆ :
                    - Django: Python ì›¹ í”„ë ˆì„ì›Œí¬
                    - React: JavaScript UI ë¼ì´ë¸ŒëŸ¬ë¦¬
                    - REST API: ì›¹ ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜
                    - PostgreSQL: ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤
                    ''',
                    'metadata': {'source_type': 'tech_knowledge', 'topic': 'web_development'}
                }
            ]
            
            for knowledge in general_knowledge:
                doc = Document(
                    page_content=knowledge['content'],
                    metadata=knowledge['metadata']
                )
                documents.append(doc)
            
        except Exception as e:
            logger.warning(f"ì¼ë°˜ ì§€ì‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return documents
    
    def _initialize_verification_chain(self):
        """ê²€ì¦ ì²´ì¸ ì´ˆê¸°í™”"""
        try:
            if self.vector_store and os.getenv('OPENAI_API_KEY'):
                # ê²€ì¦ìš© LLM ì´ˆê¸°í™”
                llm = OpenAI(temperature=0)
                
                # ê²€ì¦ ì²´ì¸ ìƒì„±
                self.retrieval_qa = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=self.vector_store.as_retriever(
                        search_kwargs={"k": self.verification_config['max_retrieved_docs']}
                    ),
                    return_source_documents=True
                )
                
                print("âœ… ê²€ì¦ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ê²€ì¦ ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def verify_ai_response(
        self, 
        ai_response: str, 
        query: str, 
        model_name: str = None
    ) -> VerificationResult:
        """AI ì‘ë‹µ ê²€ì¦"""
        try:
            print(f"ğŸ” AI ì‘ë‹µ ê²€ì¦ ì‹œì‘: {model_name or 'Unknown'}")
            
            # 1. ì‚¬ì‹¤ ì •í™•ì„± ê²€ì¦
            factual_accuracy = await self._verify_factual_accuracy(ai_response, query)
            
            # 2. ì¼ê´€ì„± ê²€ì¦
            consistency_score = await self._verify_consistency(ai_response, query)
            
            # 3. ì†ŒìŠ¤ ê²€ì¦
            source_verification = await self._verify_against_sources(ai_response, query)
            
            # 4. ëª¨ìˆœì  ì‹ë³„
            contradictions = await self._identify_contradictions(ai_response, query)
            
            # 5. ê²€ì¦ëœ ì‚¬ì‹¤ ì¶”ì¶œ
            verified_facts = await self._extract_verified_facts(ai_response, query)
            
            # 6. ëˆ„ë½ ì •ë³´ ì‹ë³„
            missing_info = await self._identify_missing_information(ai_response, query)
            
            # 7. ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = self._calculate_overall_score(
                factual_accuracy, consistency_score, source_verification
            )
            
            # 8. ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •
            confidence_level = self._determine_confidence_level(overall_score)
            
            result = VerificationResult(
                overall_score=overall_score,
                factual_accuracy=factual_accuracy,
                consistency_score=consistency_score,
                source_verification=source_verification,
                contradictions=contradictions,
                verified_facts=verified_facts,
                missing_information=missing_info,
                confidence_level=confidence_level
            )
            
            print(f"âœ… ê²€ì¦ ì™„ë£Œ: ì‹ ë¢°ë„ {confidence_level} ({overall_score:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return self._create_fallback_verification_result()
    
    async def _verify_factual_accuracy(self, response: str, query: str) -> float:
        """ì‚¬ì‹¤ ì •í™•ì„± ê²€ì¦"""
        try:
            if not self.vector_store:
                return 0.5  # ê¸°ë³¸ê°’
            
            # ì‘ë‹µì—ì„œ ì‚¬ì‹¤ì  ì •ë³´ ì¶”ì¶œ
            factual_claims = self._extract_factual_claims(response)
            
            if not factual_claims:
                return 0.5
            
            verified_claims = 0
            
            for claim in factual_claims:
                # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
                similar_docs = self.vector_store.similarity_search(claim, k=3)
                
                # ìœ ì‚¬ë„ ê²€ì‚¬
                for doc in similar_docs:
                    if self._check_claim_accuracy(claim, doc.page_content):
                        verified_claims += 1
                        break
            
            accuracy = verified_claims / len(factual_claims)
            return min(accuracy, 1.0)
            
        except Exception as e:
            logger.warning(f"ì‚¬ì‹¤ ì •í™•ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _extract_factual_claims(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ì  ì£¼ì¥ ì¶”ì¶œ"""
        claims = []
        
        # ìˆ«ìë‚˜ ë‚ ì§œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                # ìˆ«ì, ë‚ ì§œ, êµ¬ì²´ì  ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ì¥
                if (re.search(r'\d+', sentence) or 
                    re.search(r'\d{4}', sentence) or
                    re.search(r'(ë…„|ì›”|ì¼|ì‹œê°„|ë¶„|ì´ˆ)', sentence)):
                    claims.append(sentence)
        
        return claims[:5]  # ìµœëŒ€ 5ê°œ
    
    def _check_claim_accuracy(self, claim: str, source_text: str) -> float:
        """ì£¼ì¥ì˜ ì •í™•ì„± ê²€ì‚¬"""
        try:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            claim_words = set(claim.lower().split())
            source_words = set(source_text.lower().split())
            
            intersection = len(claim_words.intersection(source_words))
            union = len(claim_words.union(source_words))
            
            if union == 0:
                return 0.0
            
            similarity = intersection / union
            
            # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ì •í™•í•˜ë‹¤ê³  íŒë‹¨
            return 1.0 if similarity > 0.5 else 0.0
            
        except Exception:
            return 0.0
    
    async def _verify_consistency(self, response: str, query: str) -> float:
        """ì¼ê´€ì„± ê²€ì¦"""
        try:
            # ì‘ë‹µ ë‚´ë¶€ ì¼ê´€ì„± ê²€ì‚¬
            internal_consistency = self._check_internal_consistency(response)
            
            # ì§ˆë¬¸ê³¼ì˜ ì¼ê´€ì„± ê²€ì‚¬
            query_consistency = self._check_query_consistency(response, query)
            
            # ê°€ì¤‘ í‰ê· 
            consistency = (internal_consistency * 0.6 + query_consistency * 0.4)
            
            return min(consistency, 1.0)
            
        except Exception as e:
            logger.warning(f"ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _check_internal_consistency(self, text: str) -> float:
        """ë‚´ë¶€ ì¼ê´€ì„± ê²€ì‚¬"""
        try:
            # ëª¨ìˆœì ì¸ í‘œí˜„ ì°¾ê¸°
            contradictions = [
                ('ë§ë‹¤', 'ì ë‹¤'), ('í¬ë‹¤', 'ì‘ë‹¤'), ('ë†’ë‹¤', 'ë‚®ë‹¤'),
                ('ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤'), ('ê°€ëŠ¥í•˜ë‹¤', 'ë¶ˆê°€ëŠ¥í•˜ë‹¤')
            ]
            
            contradiction_count = 0
            total_pairs = len(contradictions)
            
            text_lower = text.lower()
            
            for pos, neg in contradictions:
                if pos in text_lower and neg in text_lower:
                    contradiction_count += 1
            
            # ëª¨ìˆœì´ ì ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
            consistency = 1.0 - (contradiction_count / total_pairs)
            return max(consistency, 0.0)
            
        except Exception:
            return 0.5
    
    def _check_query_consistency(self, response: str, query: str) -> float:
        """ì§ˆë¬¸ê³¼ì˜ ì¼ê´€ì„± ê²€ì‚¬"""
        try:
            # ì§ˆë¬¸ì˜ í‚¤ì›Œë“œê°€ ì‘ë‹µì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
            query_words = set(query.lower().split())
            response_words = set(response.lower().split())
            
            keyword_overlap = len(query_words.intersection(response_words))
            total_keywords = len(query_words)
            
            if total_keywords == 0:
                return 0.5
            
            relevance = keyword_overlap / total_keywords
            return min(relevance, 1.0)
            
        except Exception:
            return 0.5
    
    async def _verify_against_sources(self, response: str, query: str) -> float:
        """ì†ŒìŠ¤ ëŒ€ë¹„ ê²€ì¦"""
        try:
            if not self.vector_store:
                return 0.5
            
            # ê´€ë ¨ ì†ŒìŠ¤ ë¬¸ì„œ ê²€ìƒ‰
            similar_docs = self.vector_store.similarity_search(query, k=3)
            
            if not similar_docs:
                return 0.5
            
            # ì‘ë‹µê³¼ ì†ŒìŠ¤ ë¬¸ì„œë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            total_similarity = 0
            valid_sources = 0
            
            for doc in similar_docs:
                similarity = self._calculate_text_similarity(response, doc.page_content)
                total_similarity += similarity
                valid_sources += 1
            
            if valid_sources == 0:
                return 0.5
            
            avg_similarity = total_similarity / valid_sources
            return min(avg_similarity, 1.0)
            
        except Exception as e:
            logger.warning(f"ì†ŒìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            if union == 0:
                return 0.0
            
            return intersection / union
            
        except Exception:
            return 0.0
    
    async def _identify_contradictions(self, response: str, query: str) -> List[str]:
        """ëª¨ìˆœì  ì‹ë³„"""
        contradictions = []
        
        try:
            # ì¼ë°˜ì ì¸ ëª¨ìˆœ íŒ¨í„´
            contradiction_patterns = [
                (r'(\d+)ê°œ', r'(\d+)ê°œ'),  # ìˆ«ì ë¶ˆì¼ì¹˜
                (r'(ì •í™•íˆ|ì •í™•í•œ)', r'(ëŒ€ëµ|ì•½|ì¶”ì •)'),  # ì •í™•ì„± vs ì¶”ì •
                (r'(ëª¨ë“ |ì „ì²´)', r'(ì¼ë¶€|ì¼ë¶€ë¶„)'),  # ì „ì²´ vs ë¶€ë¶„
            ]
            
            for pattern1, pattern2 in contradiction_patterns:
                matches1 = re.findall(pattern1, response)
                matches2 = re.findall(pattern2, response)
                
                if matches1 and matches2:
                    contradictions.append(f"ìˆ«ìë‚˜ í‘œí˜„ì˜ ë¶ˆì¼ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # êµ¬ì²´ì ì¸ ëª¨ìˆœ ê²€ì‚¬
            if 'ê°€ëŠ¥í•˜ë‹¤' in response and 'ë¶ˆê°€ëŠ¥í•˜ë‹¤' in response:
                contradictions.append("ê°€ëŠ¥ì„±ì— ëŒ€í•œ ëª¨ìˆœëœ í‘œí˜„ì´ ìˆìŠµë‹ˆë‹¤.")
            
            if 'ë§ë‹¤' in response and 'ì ë‹¤' in response:
                contradictions.append("ì–‘ì— ëŒ€í•œ ëª¨ìˆœëœ í‘œí˜„ì´ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.warning(f"ëª¨ìˆœì  ì‹ë³„ ì‹¤íŒ¨: {e}")
        
        return contradictions
    
    async def _extract_verified_facts(self, response: str, query: str) -> List[str]:
        """ê²€ì¦ëœ ì‚¬ì‹¤ ì¶”ì¶œ"""
        verified_facts = []
        
        try:
            if not self.vector_store:
                return verified_facts
            
            # ì‘ë‹µì—ì„œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤ë“¤ ì¶”ì¶œ
            factual_sentences = self._extract_factual_claims(response)
            
            for fact in factual_sentences:
                # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê²€ì¦
                similar_docs = self.vector_store.similarity_search(fact, k=2)
                
                for doc in similar_docs:
                    if self._check_claim_accuracy(fact, doc.page_content) > 0.7:
                        verified_facts.append(fact)
                        break
            
        except Exception as e:
            logger.warning(f"ê²€ì¦ëœ ì‚¬ì‹¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return verified_facts
    
    async def _identify_missing_information(self, response: str, query: str) -> List[str]:
        """ëˆ„ë½ ì •ë³´ ì‹ë³„"""
        missing_info = []
        
        try:
            if not self.vector_store:
                return missing_info
            
            # ê´€ë ¨ ì†ŒìŠ¤ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ
            relevant_docs = self.vector_store.similarity_search(query, k=3)
            
            for doc in relevant_docs:
                # ì†ŒìŠ¤ì— ìˆì§€ë§Œ ì‘ë‹µì— ì—†ëŠ” ì •ë³´ ì°¾ê¸°
                source_keywords = set(doc.page_content.lower().split())
                response_keywords = set(response.lower().split())
                
                missing_keywords = source_keywords - response_keywords
                
                # ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                important_keywords = [
                    'ë°ì´í„°', 'ë¶„ì„', 'ê²°ê³¼', 'í†µê³„', 'ë¹„ìœ¨', 'ì¦ê°€', 'ê°ì†Œ',
                    'ê°œì„ ', 'ë¬¸ì œ', 'í•´ê²°', 'ë°©ë²•', 'ê¸°ìˆ ', 'ì•Œê³ ë¦¬ì¦˜'
                ]
                
                for keyword in important_keywords:
                    if keyword in missing_keywords and keyword in doc.page_content.lower():
                        missing_info.append(f"'{keyword}' ê´€ë ¨ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.warning(f"ëˆ„ë½ ì •ë³´ ì‹ë³„ ì‹¤íŒ¨: {e}")
        
        return missing_info[:3]  # ìµœëŒ€ 3ê°œ
    
    def _calculate_overall_score(
        self, 
        factual_accuracy: float, 
        consistency: float, 
        source_verification: float
    ) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        config = self.verification_config
        
        overall_score = (
            factual_accuracy * config['factual_accuracy_weight'] +
            consistency * config['consistency_weight'] +
            source_verification * config['source_verification_weight']
        )
        
        return min(overall_score, 1.0)
    
    def _determine_confidence_level(self, score: float) -> str:
        """ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •"""
        if score >= 0.8:
            return 'high'
        elif score >= 0.6:
            return 'medium'
        elif score >= 0.4:
            return 'low'
        else:
            return 'very_low'
    
    def _create_fallback_verification_result(self) -> VerificationResult:
        """í´ë°± ê²€ì¦ ê²°ê³¼ ìƒì„±"""
        return VerificationResult(
            overall_score=0.5,
            factual_accuracy=0.5,
            consistency_score=0.5,
            source_verification=0.5,
            contradictions=[],
            verified_facts=[],
            missing_information=[],
            confidence_level='medium'
        )
    
    async def add_knowledge_source(
        self, 
        content: str, 
        source_type: str, 
        metadata: Dict[str, Any] = None
    ) -> bool:
        """ìƒˆë¡œìš´ ì§€ì‹ ì†ŒìŠ¤ ì¶”ê°€"""
        try:
            if not self.vector_store or not self.embeddings:
                return False
            
            # ìƒˆ ë¬¸ì„œ ìƒì„±
            doc = Document(
                page_content=content,
                metadata={
                    'source_type': source_type,
                    'added_at': datetime.now().isoformat(),
                    **(metadata or {})
                }
            )
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            self.vector_store.add_documents([doc])
            
            # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
            vector_store_path = os.path.join(self.knowledge_sources_dir, 'vector_store')
            self.vector_store.save_local(vector_store_path)
            
            print(f"âœ… ì§€ì‹ ì†ŒìŠ¤ ì¶”ê°€ ì™„ë£Œ: {source_type}")
            return True
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ì†ŒìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def get_verification_summary(self, verification_result: VerificationResult) -> str:
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary_parts = []
        
        # ì „ì²´ ì‹ ë¢°ë„
        summary_parts.append(f"## ğŸ” ì‹ ë¢°ë„ ê²€ì¦ ê²°ê³¼")
        summary_parts.append(f"- ì „ì²´ ì‹ ë¢°ë„: {verification_result.overall_score:.1%} ({verification_result.confidence_level})")
        
        # ì„¸ë¶€ ì ìˆ˜
        summary_parts.append(f"- ì‚¬ì‹¤ ì •í™•ì„±: {verification_result.factual_accuracy:.1%}")
        summary_parts.append(f"- ì¼ê´€ì„±: {verification_result.consistency_score:.1%}")
        summary_parts.append(f"- ì†ŒìŠ¤ ê²€ì¦: {verification_result.source_verification:.1%}")
        
        # ê²€ì¦ëœ ì‚¬ì‹¤
        if verification_result.verified_facts:
            summary_parts.append(f"\n## âœ… ê²€ì¦ëœ ì‚¬ì‹¤")
            for fact in verification_result.verified_facts[:3]:
                summary_parts.append(f"- {fact}")
        
        # ëª¨ìˆœì 
        if verification_result.contradictions:
            summary_parts.append(f"\n## âš ï¸ ëª¨ìˆœì ")
            for contradiction in verification_result.contradictions:
                summary_parts.append(f"- {contradiction}")
        
        # ëˆ„ë½ ì •ë³´
        if verification_result.missing_information:
            summary_parts.append(f"\n## ğŸ“ ëˆ„ë½ëœ ì •ë³´")
            for missing in verification_result.missing_information:
                summary_parts.append(f"- {missing}")
        
        return "\n".join(summary_parts)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rag_verification_system = RAGVerificationSystem()
